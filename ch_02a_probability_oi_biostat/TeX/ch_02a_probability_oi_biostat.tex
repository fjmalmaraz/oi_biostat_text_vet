%!TEX root=../../main.tex
\begin{chapterpage}{Probability and Distributions of Random Variables}
  \chaptertitle{Probability and Distributions \\[4mm] of Random Variables}
  \label{probability}
  \label{modeling}
  \chaptersection{basicsOfProbability}
  %\chaptersection{conditionalProbabilitySection}
  %\chaptersection{sectionExtendedExampleCatGenetics}
  %\chaptersection{notesChapterProbability}
  %\begin{chapterpage}{Distributions of random variables}
  %\chaptertitle{Distributions of\\[4mm]random variables}
  %\label{modeling}
  \chaptersection{randomVariablesSection}
  %\chaptersection{binomialModel}
  \chaptersection{normalDist}
  %\chaptersection{poisson}
  %\chaptersection{distRelatedToBernoulli}
  %\chaptersection{correlatedRVs}
  %\chaptersection{sectionDistOfRVNotes}
  % \chaptersection{sectionDistOfRVExercises}
  \chaptersection{exercisesChapterProbability}
\end{chapterpage}
%\renewcommand{\chapterfolder}{ch_distributions_oi_biostat}

%\end{chapterpage}
\renewcommand{\chapterfolder}{ch_02a_probability_oi_biostat}

\index{probability|(}

\chapterintro{{\large What are the chances that a woman with an abnormal mammogram has breast cancer? What is the probability that a woman with an abnormal mammogram has breast cancer, given that she is in her 40's? What is the likelihood that out of 100 women who undergo a mammogram and test positive for breast cancer, at least one of the women has received a false positive result? \\

\noindent%
These questions use the language of probability to express statements about outcomes that may or may not occur. More specifically, probability is used to quantify the level of uncertainty about each outcome.  Like all mathematical tools, probability becomes easier to understand and work with once important concepts and terminology have been formalized. \\

%% CAMBIO 
\noindent%
We use probability to build tools to describe and understand apparent randombess and uncertainty. We often frame probability in terms of \term{random process} given rise to an \term{outcome} also known as an \term{event}.

\noindent%
This chapter introduces that formalization, using two types of examples. One set of examples uses settings familiar to most people -- rolling dice or picking cards from a deck. The other set of examples draws from medicine, biology, and public health, reflecting the contexts and language specific to those fields. The approaches to solving these two types of problems are surprisingly similar, and in both cases, seemingly difficult problems can be solved in a series of reliable steps.}}

\section{Defining probability}
\label{basicsOfProbability}

\subsection{Some examples}

The rules of probability can easily be modeled with classic scenarios, such as flipping coins or rolling dice. When a coin is flipped, there are only two possible outcomes, heads or tails. With a fair coin, each outcome is equally likely; thus, the chance of flipping heads is 1/2, and likewise for tails. The following examples deal with rolling a die or multiple dice; a die is a cube with six faces numbered \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, and \resp{6}. 

\begin{examplewrap}
\begin{nexample}{What is the chance of getting \resp{1} when rolling a die?}\label{probOf1}
If the die is fair, then there must be an equal chance of rolling a \resp{1} as any other possible number. Since there are six outcomes, the chance must be 1-in-6 or, equivalently, $1/6$.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}{What is the chance of not rolling a \resp{2}?}\label{probNot2}
Not rolling a \resp{2} is the same as getting a \resp{1}, \resp{3}, \resp{4}, \resp{5}, or \resp{6}, which makes up five of the six equally likely outcomes and has probability $5/6$.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}{Consider rolling two fair dice. What is the chance of getting two \resp{1}s?}\label{probOf2Ones}
If $1/6^{th}$ of the time the first die is a \resp{1} and $1/6^{th}$ of \emph{those} times the second die is also a \resp{1}, then the chance that both dice are \resp{1} is $(1/6) (1/6)$ or $1/36$.
\end{nexample}
\end{examplewrap}

Probability can also be used to model less artificial contexts, such as to predict the inheritance of genetic disease. Cystic fibrosis (CF) is a life-threatening genetic disorder caused by mutations in the \textit{CFTR} gene located on chromosome 7. Defective copies of \textit{CFTR} can result in the reduced quantity and function of the CFTR protein, which leads to the buildup of thick mucus in the lungs and pancreas.\footnote{The CFTR protein is responsible for transporting sodium and chloride ions across cell membranes.} CF is an autosomal recessive disorder; an individual only develops CF if they have inherited two affected copies of \textit{CFTR}. Individuals with one normal (wild-type) copy and one defective (mutated) copy are known as carriers; they do not develop CF, but may pass the disease-causing mutation onto their offspring.

\textD{\newpage}

\begin{examplewrap}
\begin{nexample}{Suppose that both members of a couple are CF carriers. What is the probability that a child of this couple will be affected by CF? Assume that a parent has an equal chance of passing either gene copy (i.e., allele) to a child.}\label{CFInheritanceExample}

\textit{Solution 1: Enumerate all of the possible outcomes and exploit the fact that the outcomes are equally likely, as in Example~\ref{probOf1}.}  Figure~\ref{fig:cfInheritance} shows the four possible genotypes for a child of these parents. The paternal chromosome is in blue and the maternal chromosome in green, while chromosomes with the wild-type and mutated versions of \textit{CFTR} are marked with $+$ and $-$, respectively. The child is only affected if they have genotype ($-$/$-$), with two mutated copies of \textit{CFTR}. Each of the four outcomes occurs with equal likelihood, so the child will be affected with probability 1-in-4, or $1/4$.  It is important to recognize that the child being an unaffected carrier ($+$/$-$) consists of two distinct outcomes, not one. 

\textit{Solution 2:  Calculate the proportion of outcomes that produce an affected child, as in Example~\ref{probOf2Ones}.}  During reproduction, one parent will pass along an affected copy half of the time.  When the child receives an affected allele from one parent, half of the those times, they will also receive an affected allele from the other parent. Thus, the proportion of times the child will have two affected copies is $(1/2) \times (1/2) = 1/4$.
\end{nexample}
\end{examplewrap}

\begin{figure}[h]
	\centering
	\includegraphics[width= 0.775\textwidth]{ch_02a_probability_oi_biostat/figures/cfInheritance/cfInheritance.png}
	\caption{Pattern of CF inheritance for a child of two unaffected carriers}
	\label{fig:cfInheritance}
\end{figure}

\textD{\newpage}

\begin{exercisewrap}
\begin{nexercise}
Suppose the father has CF and the mother is an unaffected carrier. What is the probability that their child will be affected by the disease?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{Since the father has CF, he must have two affected copies; he will always pass along a defective copy of the gene.  Since the mother will pass along a defective copy half of the time, the child will be affected half of the time, or with probability $(1) \times (1/2) = 1/2$.}

\subsection{Probability}

% \index{random phenomena |(}

Probability is used to assign a level of uncertainty to the outcomes of phenomena that either happen randomly (e.g. rolling dice, inheriting of disease alleles), or appear random because of a lack of understanding about exactly how the phenomenon occurs (e.g. a woman in her 40's developing breast cancer). Modeling these complex phenomena as random can be useful, and in either case, the interpretation of probability is the same: the chance that some event will occur.

Mathematicians and philosophers have struggled for centuries to arrive at a clear statement of how probability is defined, or what it means. The most common definition is used in this text.

\begin{onebox}{Probability}
The \term{probability} of an outcome is the proportion of times the outcome would occur if the random phenomenon could be observed an infinite number of times.
\end{onebox}

%\index{Law of Large Numbers |(}

This definition of probability can be illustrated by simulation. Suppose a die is rolled many times. Let $\hat{p}_n$ be the proportion of outcomes that are \resp{1} after the first $n$ rolls. As the number of rolls increases, $\hat{p}_n$ will converge to the probability of rolling a \resp{1}, $p = 1/6$. Figure~\ref{fig:dieProp} shows this convergence for 100,000 die rolls. The tendency of $\hat{p}_n$ to stabilize around $p$ is described by the \term{Law of Large Numbers}. The behavior shown in Figure~\ref{fig:dieProp} matches most people's intuition about probability, but proving mathematically that the behavior is always true is surprisingly difficult and beyond the level of this text.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{ch_02a_probability_oi_biostat/figures/dieProp/dieProp}
	\caption{The fraction of die rolls that are \resp{1} at each stage in a simulation. The proportion tends to get closer to the probability $1/6 \approx 0.167$ as the number of rolls increases.}
	\label{fig:dieProp}
\end{figure}

Occasionally the proportion veers off from the probability and appear to defy the Law of Large Numbers, as $\hat{p}_n$ does many times in Figure~\ref{fig:dieProp}. However, the likelihood of these large deviations becomes smaller as the number of rolls increases.

\textD{\newpage}

\begin{onebox}{Law of Large Numbers}
As more observations are collected, the proportion $\hat{p}_n$ of occurrences with a particular outcome converges to the probability $p$ of that outcome.
\end{onebox}

%\index{Law of Large Numbers |)}

Probability is defined as a proportion, and it always takes values between 0~and~1 (inclusively). It may also be expressed as a percentage between 0\% and 100\%. The probability of rolling a \resp{1}, $p$, can also be written as $P$(rolling a \resp{1}).

This notation can be further abbreviated. For instance, if it is clear that the process is ``rolling a die'', $P($rolling a \resp{1}$)$ can be written as~$P($\resp{1}$)$.  There also exists a notation for an event itself; the event $A$ of rolling a 1 can be written as $A = \{\text{rolling a \resp{1}}\}$, with associated probability $P(A)$.\marginpar[\raggedright\vspace{-13mm}

$P(A)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$]{\raggedright\vspace{-13mm}
	
	$P(A)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$} 

\index{random phenomena|)}

\subsection{Disjoint or mutually exclusive outcomes}

%\index{disjoint|(}
%\index{mutually exclusive|(}

Two outcomes are \termsub{disjoint}{disjoint events} or \termsub{mutually exclusive}{mutually exclusive events} if they cannot both happen at the same time. When rolling a die, the outcomes \resp{1} and \resp{2} are disjoint since they cannot both occur.  However, the outcomes \resp{1} and ``rolling an odd number'' are not disjoint since both occur if the outcome of the roll is a \resp{1}.\footnote{The terms \emph{disjoint} and \emph{mutually exclusive} are equivalent and interchangeable.} 

What is the probability of rolling a \resp{1} or a \resp{2}? When rolling a die, the outcomes \resp{1} and \resp{2} are disjoint. The probability that one of these outcomes will occur is computed by adding their separate probabilities:
\begin{eqnarray*}
P(\text{\resp{1} or \resp{2}}) = P(\text{\resp{1}})+P(\text{\resp{2}}) = 1/6 + 1/6 = 1/3.
\end{eqnarray*}
What about  the probability of rolling a \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, or \resp{6}? Here again, all of the outcomes are disjoint, so add the individual probabilities:
\begin{eqnarray*}
&&P(\text{\resp{1} or \resp{2} or \resp{3} or \resp{4} or \resp{5} or \resp{6}}) \\
	&&\quad= P(\text{\resp{1}})+P(\text{\resp{2}})+P(\text{\resp{3}})+P(\text{\resp{4}})+P(\text{\resp{5}})+P(\text{\resp{6}}) \\
	&&\quad= 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1.
\end{eqnarray*}

\begin{onebox}{Addition Rule of disjoint outcomes} If $A_1$ and $A_2$ represent two disjoint outcomes, then the probability that either one of them occurs is given by
\begin{eqnarray*}
P(A_1\text{ or } A_2) = P(A_1) + P(A_2).
\end{eqnarray*}
If there are $k$ disjoint outcomes $A_1$, ..., $A_k$, then the probability that either one of these outcomes will occur is
\begin{eqnarray}
P(A_1) + P(A_2) + \cdots + P(A_k).
\end{eqnarray}
\end{onebox}

\textD{\newpage}

\index{event|(}

\begin{exercisewrap}
\begin{nexercise}
Consider the CF example. Is the event that two carriers of CF have a child that is also a carrier represented by mutually exclusive outcomes? Calculate the probability of this event.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{Yes, there are two mutually exclusive outcomes for which a child of two carriers can also be a carrier - a child can either receive an affected copy of $CFTR$ from the mother and a normal copy from the father, or vice versa (since each parent can only contribute one allele). Thus, the probability that a child will be a carrier is 1/4 + 1/4 = 1/2.}

Probability problems often deal with \indexthis{\emph{sets}}{sets} or \indexthis{\emph{collections}}{collections} of outcomes. Let $A$ represent the event in which a die roll results in \resp{1} or \resp{2} and $B$~represent the event that the die roll is a \resp{4} or a \resp{6}. We write $A$ as the set of outcomes $\{$\resp{1},~\resp{2}$\}$ and $B=\{$\resp{4}, \resp{6}$\}$. These sets are commonly called \termsub{events}{event}. Because $A$ and $B$ have no elements in common, they are disjoint events. $A$ and $B$ are represented in Figure~\ref{fig:disjointEvents}.

\begin{figure}[hhh]
\centering
\includegraphics[width=0.55\textwidth]{ch_02a_probability_oi_biostat/figures/disjointEvents/disjointEvents.png}
\caption{Three events, $A$, $B$, and $D$, consist of outcomes from rolling a die. $A$ and $B$ are disjoint since they do not have any outcomes in common.}
\label{fig:disjointEvents}
\end{figure}

The Addition Rule applies to both disjoint outcomes and disjoint events. The probability that one of the disjoint events $A$ or $B$ occurs is the sum of the separate probabilities:
\begin{align*}
P(A\text{ or }B) = P(A) + P(B) = 1/3 + 1/3 = 2/3.
\end{align*}

\begin{exercisewrap}
\begin{nexercise}
(a) Verify the probability of event $A$, $P(A)$, is $1/3$ using the Addition Rule. (b) Do the same for event~$B$.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a) $P(A) = P($\resp{1} or \resp{2}$) = P($\resp{1}$) + P($\resp{2}$) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3}$. (b) Similarly, $P(B) = 1/3$.}

\begin{exercisewrap}
\begin{nexercise}\label{exerExaminingDisjointSetsABD}%
(a) Using Figure~\ref{fig:disjointEvents} as a reference, which outcomes are represented by event $D$? (b) Are events $B$ and $D$ disjoint? (c) Are events $A$ and $D$ disjoint?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a)~Outcomes \resp{2} and \resp{3}. (b)~Yes, events $B$ and $D$ are disjoint because they share no outcomes. (c)~The events $A$ and $D$ share an outcome in common, \resp{2}, and so are not disjoint.}

\textD{\newpage}

\begin{exercisewrap}
\begin{nexercise}
In Guided Practice~\ref{exerExaminingDisjointSetsABD}, you confirmed $B$ and $D$ from Figure~\ref{fig:disjointEvents} are disjoint. Compute the probability that event $B$ or event $D$~occurs.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{Since $B$ and $D$ are disjoint events, use the Addition Rule: $P(B$ or $D) = P(B) + P(D) = \frac{1}{3} + \frac{1}{3} = \frac{2}{3}$.}

\index{event|)}
%\index{disjoint|)}
%\index{mutually exclusive|)}


\subsection{Probabilities when events are not disjoint}

\term{Venn diagrams} are useful when outcomes can be categorized as ``in'' or ``out'' for two or three variables, attributes, or random processes. The Venn diagram in Figure~\ref{fig:cardsDiamondFaceVenn} uses one oval to represent diamonds and another to represent face cards (the cards labeled jacks, queens, and kings); if a card is both a diamond and a face card, it falls into the intersection of the ovals.

\begin{figure}[h]
\centering
\begin{tabular}{lll lll lll lll l}
\resp{2$\clubsuit$} & \resp{3$\clubsuit$} & \resp{4$\clubsuit$} & \resp{5$\clubsuit$} & \resp{6$\clubsuit$} & \resp{7$\clubsuit$} & \resp{8$\clubsuit$} & \resp{9$\clubsuit$} & \resp{10$\clubsuit$} & \resp{J$\clubsuit$} & \resp{Q$\clubsuit$} & \resp{K$\clubsuit$} & \resp{A$\clubsuit$}  \\
\color{redcards} \resp{2$\diamondsuit$} & \color{redcards}\resp{3$\diamondsuit$} & \color{redcards}\resp{4$\diamondsuit$} & \color{redcards}\resp{5$\diamondsuit$} & \color{redcards}\resp{6$\diamondsuit$} & \color{redcards}\resp{7$\diamondsuit$} & \color{redcards}\resp{8$\diamondsuit$} & \color{redcards}\resp{9$\diamondsuit$} & \color{redcards}\resp{10$\diamondsuit$} & \color{redcards}\resp{J$\diamondsuit$} & \color{redcards}\resp{Q$\diamondsuit$} & \color{redcards}\resp{K$\diamondsuit$} & \color{redcards}\resp{A$\diamondsuit$} \\
\color{redcards}\resp{2$\heartsuit$} & \color{redcards}\resp{3$\heartsuit$} & \color{redcards}\resp{4$\heartsuit$} & \color{redcards}\resp{5$\heartsuit$} & \color{redcards}\resp{6$\heartsuit$} & \color{redcards}\resp{7$\heartsuit$} & \color{redcards}\resp{8$\heartsuit$} & \color{redcards}\resp{9$\heartsuit$} & \color{redcards}\resp{10$\heartsuit$} & \color{redcards}\resp{J$\heartsuit$} & \color{redcards}\resp{Q$\heartsuit$} & \color{redcards}\resp{K$\heartsuit$} & \color{redcards}\resp{A$\heartsuit$} \\
\resp{2$\spadesuit$} & \resp{3$\spadesuit$} & \resp{4$\spadesuit$} & \resp{5$\spadesuit$} & \resp{6$\spadesuit$} & \resp{7$\spadesuit$} & \resp{8$\spadesuit$} & \resp{9$\spadesuit$} & \resp{10$\spadesuit$} & \resp{J$\spadesuit$} & \resp{Q$\spadesuit$} & \resp{K$\spadesuit$} & \resp{A$\spadesuit$}
\end{tabular}
\caption{A \indexthis{regular deck of 52 cards}{deck of cards} is split into four suits: $\clubsuit$ (club), {\color{redcards}$\diamondsuit$} (diamond), {\color{redcards}$\heartsuit$} (heart), $\spadesuit$ (spade). Each suit has 13 labeled cards: \resp{2}, \resp{3}, ..., \resp{10}, \resp{J} (jack), \resp{Q} (queen), \resp{K} (king), and \resp{A} (ace). Thus, each card is a unique combination of a suit and a label, e.g. {\color{redcards}\resp{4$\heartsuit$}} and \resp{J$\clubsuit$}. %The cards that are {\color{redcards}$\diamondsuit$} or {\color{redcards}$\heartsuit$} are typically colored {\color{redcards}red} while the other two suits are typically colored black.
	}
\label{deckOfCards}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{ch_02a_probability_oi_biostat/figures/cardsDiamondFaceVenn/cardsDiamondFaceVenn.png}
	\caption{A Venn diagram for diamonds and face cards.}
	\label{fig:cardsDiamondFaceVenn}
\end{figure}

\begin{exercisewrap}
\begin{nexercise}
(a) What is the probability that a randomly selected card is a diamond? (b)~What is the probability that a randomly selected card is a face card?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a) There are 52 cards and 13 diamonds. If the cards are thoroughly shuffled, each card has an equal chance of being drawn, so the probability that a randomly selected card is a diamond is $P({\color{redcards}\diamondsuit}) = \frac{13}{52} = 0.250$. (b)~Likewise, there are 12 face cards, so $P($face card$) = \frac{12}{52} = \frac{3}{13} = 0.231$.}

\textD{\newpage}

Let $A$ represent the event that a randomly selected card is a diamond and $B$ represent the event that it is a face card. Events $A$ and $B$ are not disjoint -- the cards {\color{redcards}\resp{J}$\diamondsuit$}, {\color{redcards}\resp{Q}$\diamondsuit$}, and {\color{redcards}\resp{K}$\diamondsuit$} fall into both categories. 

As a result, adding the probabilities of the two events together is not sufficient to calculate $P(A \ \text{or} \ B)$:
\begin{eqnarray*}
	P(A) + P(B) = P({\color{redcards}\diamondsuit}) + P(\text{face card}) = 12/52 + 13/52.
	\label{overCountFaceDiamond}
\end{eqnarray*}
Instead, a small modification is necessary. The three cards that are in both events were counted twice. To correct the double counting, subtract the probability that both events occur:

\begin{eqnarray}
P(A\text{ or } B) &=&P(\text{face card or }{\color{redcards}\diamondsuit})  \notag \\
 &=& P(\text{face card}) + P({\color{redcards}\diamondsuit}) - P(\text{face card and }{\color{redcards}\diamondsuit}) \label{diamondFace} \\
 &=& 13/52 + 12/52 - 3/52 \notag \\
 &=& 22/52 = 11/26. \notag
\end{eqnarray}
Equation~(\ref{diamondFace}) is an example of the \term{General Addition Rule}. 

\begin{onebox}{General Addition Rule}
If $A$ and $B$ are any two events, disjoint or not, then the probability that at least one of them will occur is
\begin{eqnarray}
P(A\text{ or }B) = P(A) + P(B) - P(A\text{ and }B),
\label{generalAdditionRule}
\end{eqnarray}
where $P(A$ and $B)$ is the probability that both events occur.
\end{onebox}

Note that in the language of statistics, "or" is inclusive such that $A$ or $B$ occurs means $A$, $B$, or both $A$ and $B$ occur.

\begin{exercisewrap}
\begin{nexercise}
(a) If $A$ and $B$ are disjoint, describe why this implies $P(A$ and $B) = 0$. (b) Using part (a), verify that the General Addition Rule simplifies to the Addition Rule for disjoint events if $A$ and $B$ are disjoint.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a) If $A$ and $B$ are disjoint, $A$ and $B$ can never occur simultaneously. (b) If $A$ and $B$ are disjoint, then the last term of Equation~(\ref{generalAdditionRule}) is 0 (see part (a)) and we are left with the Addition Rule for disjoint events.}

\begin{exercisewrap}
\begin{nexercise}
Human immunodeficiency virus (HIV) and tuberculosis (TB) affect substantial proportions of the population in certain areas of the developing world. Individuals sometimes are co-infected (i.e., have both diseases). Children of HIV-infected mothers may have HIV and TB can spread from one family member to another.  In a mother-child pair, let $A = \{\text{the mother has HIV} \}$,  $B = \{\textrm{the mother has TB} \}$, $C = \{\text{the child has HIV} \}$,  $D = \{\text{the child has TB} \}$.  Write out the definitions of the events $A \text{ or } B$, $A \text{ and } B$, $A \text{ and } C$, $A \text{ or } D$.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{Events $A$ or $B$: the mother has HIV, the mother has TB, or the mother has both HIV and TB. Events $A$ and $B$: the mother has both HIV and TB. Events $A$ and $C$: The mother has HIV and the child has HIV. $A$ or $D$: The mother has HIV, the child has TB, or the mother has HIV and the child has TB.}

%%%% CAMBIO: This part is copied later 

% \textD{\newpage}


% \subsection{Probability distributions}
% \label{introProbDistributions}

% A \term{probability distribution} consists of all disjoint outcomes and their associated probabilities. Figure~\ref{diceProb} shows the probability distribution for the sum of two dice. 

% \begin{figure}[h] \small
% \centering
% \begin{tabular}{l ccc ccc ccc cc}
%   \hline
%   \ \vspace{-3mm} \\
% Dice sum\vspace{0.3mm} & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12  \\
% Probability & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$\vspace{1mm} \\
%    \hline
% \end{tabular}
% \caption{Probability distribution for the sum of two dice.}
% \label{diceProb}
% \end{figure}

% \begin{onebox}{Rules for a probability distribution}
% A probability distribution is a list of all possible outcomes and their associated probabilities that satisfies three rules: \vspace{-2mm}
% \begin{enumerate}
% \setlength{\itemsep}{0mm}
% \item The outcomes listed must be disjoint.
% \item Each probability must be between 0 and 1.
% \item The probabilities must total to 1. \vspace{1mm}
% \end{enumerate}
% \end{onebox}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.73\textwidth]{ch_02a_probability_oi_biostat/figures/diceSumDist/diceSumDist}
% \caption{The probability distribution of the sum of two dice.}
% \label{diceSumDist}
% \end{figure}

% Probability distributions can be summarized in a bar plot. The probability distribution for the sum of two dice is shown in Figure~\ref{diceSumDist}, with the bar heights representing the probabilities of outcomes.

% Figure~\ref{fig:birthwtMarginalDist} shows a bar plot of the birth weight data for 3,999,386 live births in the United States in 2010, for which total counts have been converted to proportions. Since birth weight trends do not change much between years,  it is valid to consider the plot as a representation of the probability distribution of birth weights for upcoming years, such as 2017. The data are available as part of the US CDC National Vital Statistics System.\footnote{\url{http://205.207.175.93/vitalstats/ReportFolders/reportFolders.aspx}} 

% The graph shows that while most babies born weighed between 2000 and 5000 grams (2 to 5 kg), there were both small (less than 1000 grams) and large (greater than 5000 grams) babies. Pediatricians consider birth weights between 2.5 and 5 kg as normal.\footnote{\url{https://www.nlm.nih.gov/medlineplus/birthweight.html}} A probability distribution gives a sense of which outcomes can be considered unusual (i.e., outcomes with low probability).


% \begin{figure}[h]
% 	\includegraphics[width=\textwidth]{ch_02a_probability_oi_biostat/figures/birthwtMarginalDist/birthwtMarginalDist.pdf}
% 	\caption{Distribution of birth weights (in grams) of babies born in the US in 2010.}
% 	\label{fig:birthwtMarginalDist}
% \end{figure}

% \subsubsection{Continuous probability distributions}
% \label{contDist}

% \index{data!FCID|(}

% Probability distributions for events that take on a finite number of possible outcomes, such as the sum of two dice rolls, are referred to as \term{discrete probability distributions}. 

% Consider how the probability distribution for adult heights in the US might best be represented. Unlike the sum of two dice rolls, height can occupy any value over a continuous range. Thus, height has a \term{continuous probability distribution}, which is specified by a \term{probability density function} rather than a table; Figure~\ref{fdicHeightContDist} shows a histogram of the height for 3 million US adults from the mid-1990's, with an overlaid density curve.\footnote{This sample can be considered a simple random sample from the US population. It relies on the USDA Food Commodity Intake Database.} 

% Just as in the discrete case, the probabilities of all possible outcomes must still sum to 1; the total area under a probability density function equals 1. 

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.7\textwidth]{ch_02a_probability_oi_biostat/figures/fdicHeightContDist/fdicHeightContDist}
% 	\caption{The continuous probability distribution of heights for US adults.}
% 	\label{fdicHeightContDist}	
% \end{figure}

% \textD{\newpage}

% \begin{examplewrap}
% \begin{nexample}{Estimate the probability that a randomly selected adult from the US population has height between 180 and 185 centimeters. In Figure~\ref{usHeightsHist180185}, the two bins between 180 and 185 centimeters have counts of 195,307 and 156,239 people.}\label{probabilityOfBetween180185}%

% Find the proportion of the histogram's area that falls in the range \resp{180} cm and \resp{185}: add the heights of the bins in the range and divide by the sample size:

% \begin{align*}                                                    
% \frac{195,307+156,239}{\text{3,000,000}} = 0.1172.               
% \end{align*}

% The probability can be calculated precisely with the use of computing software, by finding the area of the shaded region under the curve between \resp{180} and \resp{185}:
% \begin{align*}
% P(\text{\var{height} between \resp{180} and \resp{185}})
% = \text{area between \resp{180} and \resp{185}}
% = 0.1157.
% \end{align*}
% \end{nexample}
% \end{examplewrap}

% \begin{figure}[h]
% 	\centering
% 	\subfigure[]{
% 		\includegraphics[width=0.485\textwidth]{ch_02a_probability_oi_biostat/figures/usHeightsHist180185/usHeightsHist180185}
% 		\label{usHeightsHist180185}
% 	}
% 	\subfigure[]{
% 		\includegraphics[width=0.485\textwidth]{ch_02a_probability_oi_biostat/figures/fdicHeightContDistFilled/fdicHeightContDistFilled}
% 		\label{fdicHeightContDistFilled}		
% 	}
% 	\caption{\subref{usHeightsHist180185} A histogram with bin sizes of 2.5 cm, with bars between \resp{180} and \resp{185} cm shaded.  \subref{fdicHeightContDistFilled} Density for heights in the US adult population with the area between \resp{180} and \resp{185} cm shaded.}
% \end{figure}

% \begin{examplewrap}
% \begin{nexample}{What is the probability that a randomly selected person is \textbf{exactly} \resp{180}~cm? Assume that height can be measured perfectly.}
% 	\label{probabilityOfExactly180cm}
% 	This probability is zero. A person might be close to \resp{180} cm, but not exactly \resp{180} cm tall. This also coheres with the definition of probability as an area under the density curve; there is no area captured between \resp{180}~cm and \resp{180}~cm.
% \end{nexample}
% \end{examplewrap}

% \begin{exercisewrap}
% \begin{nexercise}
% Suppose a person's height is rounded to the nearest centimeter. Is there a chance that a random person's \textbf{measured} height will be \resp{180} cm?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{This has positive probability. Anyone between \resp{179.5} cm and \resp{180.5} cm will have a \emph{measured} height of \resp{180} cm. This a more realistic scenario to encounter in practice versus Example~\ref{probabilityOfExactly180cm}.}

% \index{data!FCID|)}


% \textD{\newpage}


\subsection{Complement of an event}

Rolling a die produces a value in the set $\{$\resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, \resp{6}$\}$. This set of all possible outcomes is called the \term{sample space} ($S$)\marginpar[\raggedright\vspace{-5mm}

$S$\\\footnotesize Sample space]{\raggedright\vspace{-5mm}

$S$\\\footnotesize Sample space}\index{S@$S$} for rolling a die. 

Let $D=\{$\resp{2}, \resp{3}$\}$ represent the event that the outcome of a die roll is \resp{2} or \resp{3}. The \term{complement}\marginpar[\raggedright\vspace{0.2mm}

$A^c$\\\footnotesize Complement\\of outcome $A$]{\raggedright\vspace{0.2mm}

$A^c$\\\footnotesize Complement\\of outcome $A$}\index{Ac@$A^c$} of $D$ represents all outcomes in the sample space that are not in $D$, which is denoted by $D^c = \{$\resp{1}, \resp{4}, \resp{5}, \resp{6}$\}$. That is, $D^c$ is the set of all possible outcomes not already included in $D$. Figure~\ref{fig:complementOfD} shows the relationship between $D$, $D^c$, and the sample space $S$. 

\begin{figure}[hht]
\centering
\includegraphics[width=0.50\textwidth]{ch_02a_probability_oi_biostat/figures/complementOfD/complementOfD}
\caption{Event $D=\{$\resp{2}, \resp{3}$\}$ and its complement, $D^c = \{$\resp{1}, \resp{4}, \resp{5}, \resp{6}$\}$. $S$~represents the sample space, which is the set of all possible events.}
\label{fig:complementOfD}
\end{figure}

\begin{exercisewrap}
\begin{nexercise}
(a) Compute $P(D^c) = P($rolling a \resp{1}, \resp{4}, \resp{5}, or \resp{6}$)$. (b) What is $P(D) + P(D^c)$?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a)~The outcomes are disjoint and each has probability $1/6$, so the total probability is $4/6=2/3$. (b)~We can also see that $P(D)=\frac{1}{6} + \frac{1}{6} = 1/3$. Since $D$ and $D^c$ are disjoint, $P(D) + P(D^c) = 1$.}

\begin{exercisewrap}
\begin{nexercise}
Events $A=\{$\resp{1}, \resp{2}$\}$ and $B=\{$\resp{4}, \resp{6}$\}$ are shown in Figure~\ref{fig:disjointEvents} on page~\pageref{fig:disjointEvents}. (a) Write out what $A^c$ and $B^c$ represent. (b)~Compute $P(A^c)$ and $P(B^c)$. (c)~Compute $P(A)+P(A^c)$ and $P(B)+P(B^c)$.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{Brief solutions: (a)~$A^c=\{$\resp{3}, \resp{4}, \resp{5}, \resp{6}$\}$ and $B^c=\{$\resp{1}, \resp{2}, \resp{3}, \resp{5}$\}$. (b)~Noting that each outcome is disjoint, add the individual outcome probabilities to get $P(A^c)=2/3$ and $P(B^c)=2/3$. (c)~$A$~and~$A^c$ are disjoint, and the same is true of $B$~and~$B^c$. Therefore, $P(A) + P(A^c) = 1$ and $P(B) + P(B^c) = 1$.}

A complement of an event $A$ is constructed to have two very important properties: every possible outcome not in $A$ is in $A^c$, and $A$ and $A^c$ are disjoint. If every possible outcome not in $A$ is in $A^c$, this implies that
\begin{eqnarray}
P(A\text{ or }A^c) = 1.
\label{complementSumTo1}
\end{eqnarray}
Then, by Addition Rule for disjoint events,
\begin{eqnarray}
P(A\text{ or }A^c) = P(A) + P(A^c).
\label{complementDisjointEquation}
\end{eqnarray}
Combining Equations~(\ref{complementSumTo1}) and~(\ref{complementDisjointEquation}) yields a useful relationship between the probability of an event and its complement.

\begin{onebox}{Complement}
The complement of event $A$ is denoted $A^c$, and $A^c$ represents all outcomes not in~$A$. $A$ and $A^c$ are mathematically related: \vspace{-2mm}
\begin{eqnarray}\label{complement}
P(A) + P(A^c) = 1, \quad\text{i.e.}\quad P(A) = 1-P(A^c).
\end{eqnarray}
\end{onebox}

In simple examples, computing either $A$ or $A^c$ is feasible in a few steps. However, as problems grow in complexity, using the relationship between an event and its complement can be a useful strategy.

\begin{exercisewrap}
\begin{nexercise}
Let $A$ represent the event of selecting an adult from the US population with height between 180 and 185 cm, as calculated in Example~\ref{probabilityOfBetween180185}. What is $P(A^c)$?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{$P(A^c) = 1 - P(A) = 1 - 0.1157 = 0.8843$.}

\begin{exercisewrap}
\begin{nexercise}
Let $A$ represent the event in which two dice are rolled and their total is less than \resp{12}. (a) What does the event $A^c$ represent? (b) Determine $P(A^c)$ from Figure~\ref{diceProb} on page~\pageref{diceProb}. (c) Determine $P(A)$.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a)~The complement of $A$: when the total is equal to \resp{12}. (b)~$P(A^c) = 1/36$. (c)~Use the probability of the complement from part (b), $P(A^c) = 1/36$, and Equation~(\ref{complement}): $P($less than \resp{12}$) = 1 - P($\resp{12}$) = 1 - 1/36 = 35/36$.}

\begin{exercisewrap}
\begin{nexercise}
Consider again the probabilities from Figure~\ref{diceProb} and rolling two dice. Find the following probabilities: (a)~The sum of the dice is \emph{not} \resp{6}. (b)~The sum is at least \resp{4}. That is, determine the probability of the event $B=\{$\resp{4}, \resp{5}, ..., \resp{12}$\}$. (c) The sum is no more than \resp{10}. That is, determine the probability of the event $D=\{$\resp{2}, \resp{3}, ..., \resp{10}$\}$.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a)~First find $P($\resp{6}$)=5/36$, then use the complement: $P($not \resp{6}$) = 1 - P($\resp{6}$) = 31/36$.

(b)~First find  the complement, which requires much less effort: $P($\resp{2} or \resp{3}$)=1/36+2/36=1/12$. Then calculate $P(B) = 1-P(B^c) = 1-1/12 = 11/12$.

(c)~As before, finding the complement is the more direct way to determine $P(D)$. First find $P(D^c) = P($\resp{11} or \resp{12}$)=2/36 + 1/36=1/12$. Then calculate $P(D) = 1 - P(D^c) = 11/12$.}


\subsection{Independence}
\label{probabilityIndependence}

Just as variables and observations can be independent, random phenomena can also be independent. Two processes are \term{independent} if knowing the outcome of one provides no information about the outcome of the other. For instance, flipping a coin and rolling a die are two independent processes -- knowing that the coin lands heads up does not help determine the outcome of the die roll. On the other hand, stock prices usually move up or down together, so they are not independent.

\textD{\newpage}

Example~\ref{probOf2Ones} provides a basic example of two independent processes: rolling two dice. What is the probability that both will be \resp{1}? Suppose one of the dice is blue and the other green. If the outcome of the blue die is a \resp{1}, it provides no information about the outcome of the green die. This question was first encountered in Example~\ref{probOf2Ones}: $1/6^{th}$ of the time the blue die is a \resp{1}, and $1/6^{th}$ of \emph{those} times the green die will also be \resp{1}. This is illustrated in Figure~\ref{fig:indepForRollingTwo1s}. Because the rolls are independent, the probabilities of the corresponding outcomes can be multiplied to obtain the final answer: $(1/6)(1/6)=1/36$. This can be generalized to many independent processes. 

\begin{figure}[hht]
\centering
\includegraphics[width=0.475\textwidth]{ch_02a_probability_oi_biostat/figures/indepForRollingTwo1s/indepForRollingTwo1s.png}
\caption{$1/6^{th}$ of the time, the first roll is a \resp{1}. Then $1/6^{th}$ of \emph{those} times, the second roll will also be a \resp{1}.}
\label{fig:indepForRollingTwo1s}
\end{figure}

Complicated probability problems, such as those that arise in biology or medicine, are often solved with the simple ideas used in the dice example. For instance, independence was used implicitly in the second solution to Example~\ref{CFInheritanceExample}, when calculating the probability that two carriers will have an affected child with cystic fibrosis. Genes are typically passed along from the mother and father independently. This allows for the assumption that, on average, half of the offspring who receive a mutated gene copy from the mother will also receive a mutated copy from the father.


\begin{exercisewrap}
\begin{nexercise}\label{threeDice}%
What if there were also a red die independent of the other two? What is the probability of rolling the three dice and getting all \resp{1}s?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{The same logic applies from Example~\ref{probOf2Ones}. If $1/36^{th}$ of the time the blue and green dice are both \resp{1}, then $1/6^{th}$ of \emph{those} times the red die will also be \resp{1}, so multiply:
{\begin{align*}
P(blue=\text{\small\resp{1} and } green=\text{\small\resp{1} and } red=\text{\small\resp{1}})
	&= P(blue=\text{\small\resp{1}}) P(green=\text{\small\resp{1}}) P(red=\text{\small\resp{1}}) \\
	&= (1/6) (1/6) (1/6)
	= 1/216.
\end{align*}}}

\begin{exercisewrap}
\begin{nexercise}
Three US adults are randomly selected. The probability the height of a single adult is between 180 and 185 cm is 0.1157.\footnotemark{}\vspace{-1.5mm}
	\begin{enumerate}
		\setlength{\itemsep}{0mm}
		\item[(a)] What is the probability that all three are between 180 and 185 cm tall?
		\item[(b)] What is the probability that none are between 180 and 185 cm tall?
	\end{enumerate}
\end{nexercise}
\end{exercisewrap}
\footnotetext{Brief answers: (a) $0.1157 \times 0.1157 \times 0.1157 = 0.0015$. (b) $(1-0.1157)^3 = 0.692$.}

\textD{\newpage}

\begin{onebox}{Multiplication Rule for independent processes}
\index{Multiplication Rule|textbf}%
If $A$ and $B$ represent events from two different and independent processes, then the probability that both $A$ and $B$ occur is given by: \vspace{-1.5mm}
\begin{eqnarray}\label{eqForIndependentEvents}
P(A \text{ and }B) = P(A)  P(B).
\end{eqnarray}
Similarly, if there are $k$ events $A_1$, ..., $A_k$ from $k$ independent processes, then the probability they all occur is\vspace{-1.5mm}
\begin{eqnarray*}
P(A_1) P(A_2) \cdots P(A_k).
\end{eqnarray*}
\end{onebox}

\begin{examplewrap}
\begin{nexample}{\textbf{Mandatory drug testing.} Mandatory drug testing in the workplace is common practice for certain professions, such as air traffic controllers and transportation workers.  A false positive in a drug screening test occurs when the test incorrectly indicates that a screened person is an illegal drug user. Suppose a mandatory drug test has a false positive rate of 1.2\% (i.e., has probability  0.012 of indicating that an employee is using illegal drugs when that is not the case).  Given 150 employees who are in reality drug free, what is the probability that at least one will (falsely) test positive? Assume that the outcome of one drug test has no effect on the others.}

First, note that the complement of at least 1 person testing positive is that no one tests positive (i.e., all employees test negative). The multiplication rule can then be used to calculate the probability of 150 negative tests.

   \begin{align*} 
   P(\text{At least 1 "+"}) &= P(\text{1 or 2 or 3 \ldots or 150 are "+"}) \\
           &= 1 - P(\text{None are "+"}) \\
           & = 1 - P(\text{150 are "-"}) \\
		   &= 1 - P(\text{"-"})^{150} \\
           &= 1 - (0.988)^{150} = 1 - 0.16 = 0.84.
    \end{align*}
 
Even when using a test with a small probability of a false positive, the company is more than 80\% likely to incorrectly claim at least one employee is an illegal drug user!
\end{nexample}
\end{examplewrap}

\begin{exercisewrap}
\begin{nexercise}
Because of the high likelihood of at least one false positive in company wide drug screening programs, an individual with a positive test is almost always re-tested with a different screening test: one that is more expensive than the first, but has a lower false positive probability. Suppose the second test has a false positive rate of 0.8\%.  What is the probability that an employee who is not using illegal drugs will test positive on both tests?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{The outcomes of the two tests are independent of one another; $P (A\text{ and } B) = P(A) \times P(B)$, where events $A$ and $B$ are the results of the two tests. The probability of a false positive with the first test is 0.012 and 0.008 with the second. Thus, the probability of an employee who is not using illegal drugs testing positive on both tests is $0.012 \times 0.008 = 9.6 \times 10^{-5}$}

\textD{\newpage}

\begin{figure}[h]
	\centering
	\includegraphics[height=0.5\textwidth]{ch_02a_probability_oi_biostat/figures/aboInheritance/aboInheritance.png}
	\caption{Inheritance of ABO blood groups.}
	\label{fig:aboInheritance}
\end{figure}

\begin{examplewrap}
\begin{nexample}{\textbf{ABO blood groups.} There are four different common blood types (A, B, AB, and O), which are determined by the presence of certain antigens located on cell surfaces. Antigens are substances used by the immune system to recognize self versus non-self; if the immune system encounters antigens not normally found on the body's own cells, it will attack the foreign cells. When patients receive blood transfusions, it is critical that the antigens of transfused cells match those of the patient's, or else an immune system response will be triggered.
		
The ABO blood group system consists of four different blood groups, which describe whether an individual's red blood cells carry the A antigen, B antigen, both, or neither. The ABO gene has three alleles: ${I}^{A}$, ${I}^{B}$, and \textit{i}. The \textit{i} allele is recessive to both ${I}^{A}$ and ${I}^{B}$, and does not produce antigens; thus, an individual with genotype ${I}^{A}i$ is blood group A and an individual with genotype ${I}^{B}i$ is blood group B. The ${I}^{A}$ and ${I}^{B}$ alleles are codominant, such that individuals of ${I}^{A}$${I}^{B}$ genotype are AB. Individuals homozygous for the \textit{i} allele are known as blood group O, with neither A nor B antigens.

Suppose that both members of a couple have Group AB blood.	
\begin{enumerate}[a)]
	\item What is the probability that a child of this couple will have Group A blood?
	\item What is the probability that they have two children with Group A blood?
\end{enumerate}}
\begin{enumerate}[a)]
	\item An individual with Group AB blood is genotype ${I}^{A}$${I}^{B}$. Two ${I}^{A}$${I}^{B}$ parents can produce children with genotypes ${I}^{A}$${I}^{B}$, ${I}^{A}$${I}^{A}$, or ${I}^{B}$${I}^{B}$. Of these possibilities, only children with genotype ${I}^{A}$${I}^{A}$ have Group A blood. Each parent has 0.5 probability of passing down their ${I}^{A}$ allele. Thus, the probability that a child of this couple will have Group A blood is P(parent 1 passes down ${I}^{A}$ allele) $\times$ P(parent 2 passes down ${I}^{A}$ allele) = $0.5 \times 0.5 = 0.25$.
	
	\item Inheritance of alleles is independent between children. Thus, the probability of two children having Group A blood equals P(child 1 has Group A blood) $\times$ P(child 2 has group A blood). The probability of a child of this couple having Group A blood was previously calculated as 0.25. The answer is given by $0.25 \times 0.25 = 0.0625$.
\end{enumerate}
\end{nexample}
\end{examplewrap}

The previous examples in this section have used independence to solve probability problems. The definition of independence can also be used to check whether two events are independent -- two events $A$ and $B$ are independent if they satisfy Equation~\eqref{eqForIndependentEvents}.

\begin{examplewrap}
\begin{nexample}{Is the event of drawing a heart from a deck of cards independent of drawing an ace?}
The probability the card is a heart is $1/4$ ($13/52=1/4$) and the probability that it is an ace is $1/13$ ($4/52=1/13$). The probability that the card is the ace of hearts (\resp{A}${\color{redcards}\heartsuit}$) is $1/52$. Check whether Equation~\ref{eqForIndependentEvents} is satisfied:

\begin{align*}
P({\color{redcards}\heartsuit}) P(\text{\resp{A}}) = \left(\frac{1}{4} \right) \left( \frac{1}{13} \right) = \frac{1}{52} 
= P({\color{redcards}\heartsuit}\text{ and \resp{A}}).
\end{align*}
Since the equation holds, the event that the card is a heart and the event that the card is an ace are independent events.
\end{nexample}
\end{examplewrap}

\begin{examplewrap}
\begin{nexample}
 {In the general population, about 15\% of adults between 25 and 40  years of age are hypertensive.  Suppose that among males of this age, hypertension occurs about 18\% of the time.  Is hypertension independent of sex?}\label{hypertensionIndEx}%

Assume that the population is 50\% male, 50\% female; it is given in the problem that hypertension occurs about 15\% of the time in adults between ages 25 and 40. 

\[P(\text{hypertension}) \times P(\text{male}) = (0.15)(0.50) = 0.075 \neq 0.18.\] 

Equation~\ref{eqForIndependentEvents} is not satisfied, therefore hypertension is not independent of sex. In other words, knowing whether an individual is male or female is informative as to whether they are hypertensive. If hypertension and sex were independent, then we would expect hypertension to occur at an equal rate in males as in females.
\end{nexample}
\end{examplewrap}
%%% CAMBIO: This part is not included since we are not studying conditional probability on this course. 

%_________________
% \section{Conditional probability}
% \label{conditionalProbabilitySection}

% While it is difficult to obtain precise estimates, the US CDC estimated that in 2012, approximately 29.1 million Americans had type 2 diabetes -- about 9.3\% of the population.\footnote{21 million of these cases are diagnosed, while the CDC predicts that 8.1 million cases are undiagnosed; that is, approximately 8.1 million people are living with diabetes, but they (and their physicians) are unaware that they have the condition.} A health care practitioner seeing a new patient would expect a 9.3\% chance that the patient might have diabetes. 

% However, this is only the case if nothing is known about the patient. The prevalence of type 2 diabetes varies with age. Between the ages of 20 and 44, only about 4\% of the population have diabetes, but almost 27\% of people age 65 and older have the disease. Knowing the age of a patient provides information about the chance of diabetes; age and diabetes status are not independent. While the probability of diabetes in a randomly chosen member of the population is 0.093, the \textit{conditional} probability of diabetes in a person known to be 65 or older is 0.27.

% Conditional probability is used to  characterize how the probability of an outcome varies with the knowledge of another factor or condition, and is closely related to the concepts of marginal and joint probabilities.

% \subsection{Marginal and joint probabilities}
% \label{marginalAndJointProbabilities}

% \index{marginal probability|(}
% \index{joint probability|(}


% Figures~\ref{DiabetesAgeContTable} and \ref{DiabetesAgeProbTable} provide additional information about the relationship between diabetes prevalence and age.\footnote{Because the CDC provides only approximate numbers for diabetes prevalence, the numbers in the table are approximations of actual population counts.} Figure~\ref{DiabetesAgeContTable} is a contingency table for the entire US population in 2012; the values in the table are in thousands (to make the table more readable).  

% % latex table generated in R 3.0.1 by xtable 1.7-1 package
% % Thu Sep 10 11:36:48 2015
% \begin{figure}[ht]
% 	\centering
% 	\begin{tabular}{rrrr}
% 		\hline
% 		& Diabetes & No Diabetes & Sum \\ 
% 		\hline
% 		Less than 20 years & 200 & 86,664 & 86,864 \\ 
% 		20 to 44 years & 4,300 & 98,724 & 103,024 \\ 
% 		45 to 64 years & 13,400 & 68,526 & 81,926 \\ 
% 		Greater than 64 years & 11,200 & 30,306 & 41,506 \\ 
% 		Sum & 29,100 & 284,220 & 313,320 \\ 
% 		\hline
% 	\end{tabular}
% 	\caption{Contingency table showing type 2 diabetes status and age group, in thousands.}
% 	\label{DiabetesAgeContTable}
% \end{figure}
% % can xtable insert comma separators automatically?  If not, I will add them by hand. JV: apparently R can do this, but I didn't manage to get it working http://stackoverflow.com/questions/1581232/adding-commas-into-number-for-output

% In the first row, for instance, Figure~\ref{DiabetesAgeContTable} shows that in the entire population of approximately 313,320,000 people, approximately 200,000 individuals were in the less than 20 years age group and diagnosed with diabetes -- about 0.1\%. The table also indicates that among the approximately 86,864,000 individuals less than 20 years of age, only 200,000 suffered from type 2 diabetes, approximately 0.2\%.  The distinction between these two statements is small but important. The first provides information about the size of the group with type 2 diabetes population that is less than 20 years of age, relative to the entire population. In contrast, the second statement is about the size of the diabetes population within the less than 20 years of age group, relative to the size of that age group.

% \textD{\newpage}

% \begin{exercisewrap}
% \begin{nexercise}\label{DiabetesAge20to44}%
% What fraction of the US population are 45 to 64 years of age and have diabetes?  What fraction of the population age 45 to 64 have diabetes?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{The first value is given by the intersection of "45 - 64 years of age" and "diabetes", divided by the total population number: $13,400,000/313,320,000 = 0.043$. The second value is given by dividing 13,400,000 by 81,926,000, the number of individuals in that age group: $13,400,000/81,926,000 = 0.164$.}

% The entries in Figure~\ref{DiabetesAgeProbTable} show the proportions of the population in each of the eight categories defined by diabetes status and age, obtained by dividing each value in the cells of Figure~\ref{DiabetesAgeContTable} by the total population size.

% %  xtable(diabetes.age.table.thousands, digits = 0)
% %  this table uses census counts; still not perfect, but closer

% % latex table generated in R 3.0.1 by xtable 1.7-1 package
% % Thu Sep 10 11:44:44 2015
% \begin{figure}[ht]
% 	\centering
% 	\begin{tabular}{rrrr}
% 		\hline
% 		& Diabetes & No Diabetes & Sum \\ 
% 		\hline
% 		Less than 20 years & 0.001 & 0.277 & 0.277 \\ 
% 		20 to 44 years & 0.014 & 0.315 & 0.329 \\ 
% 		45 to 64 years & 0.043 & 0.219 & 0.261 \\ 
% 		Greater than 64 years & 0.036 & 0.097 & 0.132 \\ 
% 		Sum & 0.093 & 0.907 & 1.000 \\ 
% 		\hline
% 	\end{tabular}
% 	\caption{Probability table summarizing diabetes status and age group.}
% 	\label{DiabetesAgeProbTable}
% \end{figure}
% %   xtable(diabetes.age.table.prop, digits = 3)

% If these proportions are interpreted as probabilities for randomly chosen individuals from the population, the value 0.014 in the first column of the second row implies that the probability of selecting someone at random who has diabetes and whose age is between 20 and 44 is 0.014, or 1.4\%. The entries in the eight main table cells (i.e., excluding the values in the margins) are \term{joint probabilities}, which specify the probability of two events happening at the same time -- in this case, diabetes and a particular age group. In probability notation, this joint probability can be expressed as $0.014 = P(\text{diabetes and age 20 to 44})$.\footnote{Alternatively, this is commonly written as as $P(\text{diabetes, age 20 to 44})$, with a comma replacing ``and''.}

% The values in the last row and column of the table are the sums of the corresponding rows or columns. The sum of the of the probabilities of the disjoint events (diabetes, age 20 to 44) and (no diabetes, age 20 to 44), 0.329, is the probability of being in the age group 20 to 44. The row and column sums are \term{marginal probabilities}; they are probabilities about only one type of event, such as age. For example, the sum of the first column (0.093) is the marginal probability of a member of the population having diabetes.

% \begin{onebox}{Marginal and joint probabilities}
% A \emph{\hiddenterm{marginal probability}} is a probability only related to a single event or process, such as $P(A)$. A \emph{\hiddenterm{joint probability}} is the probability that two or more events or processes occur jointly, such as $P(A \textrm{ and } B)$.
% \end{onebox}

% \begin{exercisewrap}
% \begin{nexercise}\label{MarginalJointProbDiabetes}%
% What is the interpretation of the value 0.907 in the last row of the table?  And of the value 0.097 directly above it?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{The value 0.907 in the last row indicates the total proportion of individuals in the population who do not have diabetes. The value 0.097 indicates the joint probability of not having diabetes and being in the greater than 64 years age group.}


% \textD{\newpage}


% \subsection{Defining conditional probability}

% \index{conditional probability|(}

% The probability that a randomly selected individual from the US has diabetes is 0.093, the sum of the first column in Figure~\ref{DiabetesAgeProbTable}. How does that probability change if it is known that the individual's age is 64 or greater?  

% The conditional probability can be calculated from Figure~\ref{DiabetesAgeContTable}, which shows that 11,200,000 of the 41,506,000 people in that age group have diabetes, so the likelihood that someone from that age group has diabetes is:
% \[  
% \frac{11,200,000}{41,506,000} = 0.27,
% \]
% or 27\%. The additional information about a patient's age allows for a more accurate estimate of the probability of diabetes.

% Similarly, the conditional probability can be calculated from the joint and marginal proportions in Figure~\ref{DiabetesAgeProbTable}. Consider the main difference between the conditional probability versus the joint and marginal probabilities. Both the joint probability and marginal probabilities are probabilities relative to the entire population. However, the conditional probability is the probability of having diabetes, \textit{relative only to} the segment of the population greater than the age of 64.

% Intuitively, the denominator in the calculation of a conditional probability must account for the fact that only a segment of the population is being considered, rather than the entire population. The conditional probability of diabetes given age 64 or older is simply the joint probability of having diabetes and being greater than 64 years of age divided by the marginal probability of being in that age group:
% \begin{align*}
% \frac{\text{prop. of population with diabetes, age 64 or greater}}{\text{prop. of population greater than age 64}} &= \frac{11,200,000/313,320,000}{41,506,000/313,320,000}\\
% &= \frac{0.036}{0.132} \\
% &= 0.270.
% \end{align*}
% This leads to the mathematical definition of conditional probability.

% \begin{onebox}{Conditional probability}
% The conditional probability of an event $A$ given an event or condition $B$ is:
% \begin{align}
% P(A | B) = \frac{P(A\text{ and }B)}{P(B)}.
% \label{condProbEq}
% \end{align}
% \end{onebox}

% \begin{exercisewrap}
% \begin{nexercise}
% Calculate the probability that a randomly selected person has diabetes, given that their age is between 45 and 64.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{Let $A$ be the event a person has diabetes, and $B$ the event that their age is between 45 and 64. Use the information in Figure~\ref{DiabetesAgeProbTable} to calculate $P(A|B)$. $P(A|B) = \frac{P(A\text{ and }B)}{P(B)} = \frac{0.043}{0.261} = 0.165.$}\textD{\vspace{-2mm}}

% \textD{\newpage}

% \begin{exercisewrap}
% \begin{nexercise}
% Calculate the probability that a randomly selected person is between 45 and 64 years old, given that the person has diabetes.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{Again, let $A$ be the event a person has diabetes, and $B$ the event that their age is between 45 and 64. Find $P(B|A)$. $P(B|A) = \frac{P(A\text{ and }B)}{P(A)} = \frac{0.043}{0.093} = 0.462.$}\textD{\vspace{-2mm}}

% Conditional probabilities have similar properties to regular (unconditional) probabilities.

% \begin{onebox}{Sum of conditional probabilities}
% Let $A_1$, ..., $A_k$ represent all the disjoint outcomes for a variable or process. Then if $B$ is an event, possibly for another variable or process, we have: \vspace{-1mm}
% \begin{align*}
% P(A_1|B)+\cdots+P(A_k|B) = 1.
% \end{align*}\vspace{-5.5mm} \par
% The rule for complements also holds when an event and its complement are conditioned on the same information: \vspace{-1.5mm}
% \begin{align*}
% P(A | B) = 1 - P(A^c | B).
% \end{align*}
% \end{onebox}

% \begin{exercisewrap}
% \begin{nexercise} 
% Calculate the probability a randomly selected person is older than 20 years of age, given that the person has diabetes.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{Let $A$ be the event that a person has diabetes, and $B$ be the event that their age is less than 20 years. The desired probability is $P(B^c|A) = 1 - P(B|A) = 1 - \frac{0.001}{0.093} = 0.989$.}


% \textD{\newpage}


% \subsection{General multiplication rule}

% Section~\ref{probabilityIndependence} introduced the Multiplication Rule for independent processes. Here, the \term{General Multiplication Rule} is introduced for events that might not be independent.

% \begin{onebox}{General Multiplication Rule}
% If $A$ and $B$ represent two outcomes or events, then \vspace{-1.5mm}
% \begin{eqnarray*}
% P(A\text{ and }B) = P(A | B) P(B).
% \end{eqnarray*} \vspace{-6.5mm} \par
% It is useful to think of $A$ as the outcome of interest and $B$ as the condition.
% \end{onebox}

% This General Multiplication Rule is simply a rearrangement of the definition for conditional probability in Equation~(\ref{condProbEq}) on page~\pageref{condProbEq}.

% \begin{examplewrap}
% \begin{nexample}{Suppose that among male adults between 25 and 40 years of age, hypertension occurs about 18\% of the time. Assume that the population is 50\% male, 50\% female. What is the probability of randomly selecting a male with hypertension from the population of individuals 25-40 years of age?} 
% Let $A$ be the event that a person has hypertension, and $B$ the event that they are a male adult between 25 and 40 years of age. $P(A|B)$, the probability of hypertension given male sex, is 0.18. Thus, $P(A \text{ and } B) = (0.18)(0.50) = 0.09$.
% \end{nexample}
% \end{examplewrap}


% \subsection{Independence and conditional probability}

% If two events are independent, knowing the outcome of one should provide no information about the other.

% \begin{examplewrap}
% \begin{nexample}{Let $X$ and $Y$ represent the outcomes of rolling two dice. Use the formula for conditional probability to compute $P(Y =$ \resp{1}$\ |\ X = $ \resp{1}$)$. What is $P(Y=1)$? Is this different from $P(Y =$ \resp{1}$\ |\ X = $ \resp{1}$)$?}\label{condProbOfRollingA1AfterOne1}%
% \[\frac{P(Y = \text{ \resp{1} and }X=\text{ \resp{1}})}{P(X=\text{ \resp{1}})} = \frac{1/36}{1/6} = 1/6.\]

% The probability $P(Y=1) = 1/6$ is the same as the conditional probability. The probability that $Y=1$ was unchanged by knowledge about $X$, since the events $X$ and $Y$ are independent.
% \end{nexample}
% \end{examplewrap}

% \textD{\newpage}

% Using the Multiplication Rule for independent events allows for a mathematical illustration of why the condition information has no influence in Example~\ref{condProbOfRollingA1AfterOne1}:

% \begin{eqnarray*}
% P(Y=\text{\resp{1}}\ |\ X=\text{\resp{1}})
% 	&=& \frac{P(Y=\text{\resp{1} and }X=\text{\resp{1}})}{P(X=\text{\resp{1}})} \\
% 	&=& \frac{P(Y=\text{\resp{1}}) \color{oiGB}P(X=\text{\resp{1}})}{\color{oiGB}P(X=\text{\resp{1}})} \\
% 	&=& P(Y=\text{\resp{1}}). \\
% \end{eqnarray*}

% This is a specific instance of the more general result that if two events $A$ and $B$ are independent, $P(A |B) = P(A)$ as long as $P(B) > 0$:
% \begin{eqnarray*}
% 	P(A | B) &= \dfrac{P(A \text{ and } B)}{P(B)} \\
% 	     &= \dfrac{P(A) P(B)}{P(B)} \\
% 		 &= P(A).
% \end{eqnarray*} 

% \begin{exercisewrap}
% \begin{nexercise}
% In the US population, about 45\% of people are blood group O. Suppose that 40\% of Asian people living in the US are blood group O, and that the Asian population in the United States is approximately 4\%. Do these data suggest that blood group is independent of ethnicity?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{Let $A$ represent blood group O, and $B$ represent Asian ethnicity. Since $P(A|B) = 0.40$ does not equal $P(A) = 0.45$, the two events are not independent. Blood group does not seem to be independent of ethnicity.}


% \subsection{Bayes' Theorem}
% \label{bayesTheoremSubsection}

% \index{Bayes' Theorem|(}

% This chapter began with a straightforward question -- what are the chances that a woman with an abnormal (i.e., positive) mammogram has breast cancer?  For a clinician, this question can be rephrased as the conditional probability  that a woman has breast cancer, given that her mammogram is abnormal. This conditional probability is called the \term{positive predictive value} (PPV) of a mammogram. More concisely, if $A = \text{\{a woman has breast cancer\}}$, and $B = \text{\{a mammogram is  positive\}}$, the PPV of a mammogram is $P(A|B)$.

% The characteristics of a mammogram (and other diagnostic tests) are given with the reverse conditional probabilities\textemdash  the probability that the mammogram correctly returns a positive result if a woman has breast cancer, as well as the probability that the mammogram correctly returns a negative result if a woman does not have breast cancer. These are the probabilities $P(B|A)$ and $P(B^c|A^c)$, respectively.

% Given the probabilities $P(B|A)$ and $P(B^c|A^c)$, as well as the marginal probability of disease $P(A)$, how can the positive predictive value $P(A|B)$ be calculated?

% There are several possible strategies for approaching this type of problem\textemdash 1) constructing tree diagrams, 2) using a purely algebraic approach using Bayes' Theorem, and 3) creating contingency tables based on calculating conditional probabilities from a large, hypothetical population.

% \textD{\newpage}

% \begin{examplewrap}
% \begin{nexample}{In Canada, about 0.35\% of women over 40 will develop breast cancer in any given year. A common screening test for cancer is the mammogram, but it is not perfect. In about 11\% of patients with breast cancer, the test gives a \term{false negative}: it indicates a woman does not have breast cancer when she does have breast cancer. Similarly, the test gives a \term{false positive} in 7\% of patients who do not have breast cancer: it indicates these patients have breast cancer when they actually do not.\footnotemark{}\label{probabilityOfBreastCancerGivenPositiveTestExample}%
% If a randomly selected woman over 40 is tested for breast cancer using a mammogram and the test is positive -- that is, the test suggests the woman has cancer -- what is the probability she has breast cancer?}
% Read on in the text for three solutions to this example.
% \end{nexample}
% \end{examplewrap}
% \footnotetext{The probabilities reported here were obtained using studies reported at \oiRedirect{textbook-breastCancerDotOrg_20090831b}{www.breastcancer.org} and \oiRedirect{textbook-ncbi_nih_breast_cancer}{www.ncbi.nlm.nih.gov/pmc/articles/PMC1173421}.}


% \subsubsection{Example~\ref{probabilityOfBreastCancerGivenPositiveTestExample} Solution 1. Tree Diagram.}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.93\textwidth]{ch_02a_probability_oi_biostat/figures/BreastCancerTreeDiagram/BreastCancerTreeDiagram}
% 	\caption{A tree diagram for breast cancer screening.}
% 	\label{BreastCancerTreeDiagram}
% \end{figure}

% A \term{tree diagram} is a tool to organize outcomes and probabilities around the structure of data, and is especially useful when two or more processes occur in a sequence, with each process  conditioned on its predecessors. 

% In Figure~\ref{BreastCancerTreeDiagram}, the primary branches split the population by cancer status, and show the marginal probabilities 0.0035 and 0.9965 of having cancer or not, respectively. The secondary branches are conditioned on the primary branch and show conditional probabilities; for example, the top branch is the probability that a mammogram is positive given that an individual has cancer. The problem provides enough information to compute the probability of testing positive if breast cancer is present, since this probability is the complement of the probability of a false negative: $1 - 0.11 = 0.89$.

% Joint probabilities can be constructed at the end of each branch by multiplying the numbers from right to left, such as the probability that a woman tests positive given that she has breast cancer (abbreviated as BC):

% \begin{align*}
% P(\text{BC and mammogram$^+$}) &= P(\text{mammogram$^+$ } |\ \text{BC}) \times  P(\text{BC}) \\
% 	&= (0.89) (0.0035) = 0.00312.
% \end{align*}

% \textD{\newpage}

% Using the tree diagram allows for the information in the problem to be mapped out in a way that makes it easier to calculate the desired conditional probability. In this case, the diagram makes it clear that there are two scenarios in which someone can test positive: either testing positive when having breast cancer or by testing positive in the absence of breast cancer. To find the probability that a woman has breast cancer given that she tests positive, apply the conditional probability formula: divide the probability of testing positive when having breast cancer by the probability of testing positive.

% The probability of a positive test result is the sum of the two corresponding scenarios:
% \begin{align*}
% P(\text{mammogram$^+$}) =&  P(\text{mammogram$^+$ and has BC}) + P(\text{mammogram$^+$ and no BC}) \\
% =&[P(\text{mammogram$^+$ } | \text{ has BC}) \times P(\text{has BC})] + [P(\text{mammogram$^+$ } | \text{ no BC}) \times P(\text{no BC})] \\
% =& (0.0035)(0.89) + (0.9965)(0.07) = 0.07288.
% \end{align*}
% Thus, if the mammogram screening is positive for a patient, the probability that the patient has breast cancer is given by: 
% \begin{align*}
% P(\text{has BC } | \text{ mammogram$^+$})
% 	&= \frac{P(\text{has BC and mammogram$^+$})}{P(\text{mammogram$^+$})}\\
% 	&= \frac{0.00312}{0.07288} \approx 0.0428.
% \end{align*}

% Even with a positive mammogram, there is still only a~4\%~chance of breast cancer! It may seem surprising that even when the false negative and false positive probabilities of the test are small (0.11 and 0.07, respectively), the conditional probability of disease given a positive test could also be so small. In this population, the probability that a woman does not have breast cancer is high (1 - 0.0035 = 0.9965), which results in a relatively high number of false positives in comparison to true positives.

% Calculating probabilities for diagnostic tests is done so often in medicine that the topic has some specialized terminology. The \term{sensitivity} of a test is the probability of a positive test result when disease is present, such as a positive mammogram when a patient has breast cancer. The \term{specificity} of a test is the probability of a negative test result when disease is absent.\footnote{The specificity and sensitivity are, respectively, the probability of a true positive test result and the probability of a true negative test result.} The probability of disease in a population is referred to as the \term{prevalence}. With specificity and sensitivity information for a particular test, along with disease prevalence, the \term{positive predictive value} (PPV) can be calculated: the probability that disease is present when a test result is positive. Similarly, the \term{negative predictive value} is the probability that disease is absent when test results are negative. These terms are used for nearly all diagnostic tests used to screen for diseases.

% \begin{exercisewrap}
% \begin{nexercise}
% 	Identify the prevalence, sensitivity, specificity, and PPV from the scenario in Example~\ref{probabilityOfBreastCancerGivenPositiveTestExample}.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{The prevalence of breast cancer is 0.0035. The sensitivity is the probability of a positive test result when disease is present, which is the complement of a false negative: $1 - 0.11 = 0.89$. The specificity is the probability of a negative test result when disease is absent, which is the complement of a false positive: $1 - 0.07 = 0.93$. The PPV is 0.04, the probability of breast cancer given a positive mammogram.}


% \textD{\newpage}


% \subsubsection{Example~\ref{probabilityOfBreastCancerGivenPositiveTestExample} Solution 2. Bayes' Rule.}

% The process used to solve the problem via the tree diagram can be condensed into a single algebraic expression by substituting the original probability expressions into the numerator and denominator:

% \begin{small}
% \begin{align*}
% P(\text{has BC } | \text{ mammogram$^+$})
% &= \frac{P(\text{has BC and mammogram$^+$})}{P(\text{mammogram$^+$})} \\
% &= \frac{P(\text{mammogram$^+$ } | \text{ has BC}) \times P(\text{has BC})}
% {[P(\text{mammogram$^+$ } | \text{ has BC}) \times P(\text{has BC})] + [P(\text{mammogram$^+$ } | \text{ no BC}) \times P(\text{no BC})]}.
% \end{align*}
% \end{small}

% The expression can also be written in terms of diagnostic testing language, where $D = \text{\{has disease\}}$, $D^c = \text{\{does not have disease\}}$, $T^{+} = \text{\{positive test result\}}$, and $T^{-} = \text{\{negative test result\}}$.

% \begin{align*}
% P(D|T^{+}) &= \dfrac{P(D \text{ and } T^{+})}{P(T^{+})} \\
% &= \dfrac{P(T^{+}|D) \times P(D)}{[P(T^{+}|D) \times P(D)] + [P(T^{+}|D^c) \times P(D^c)]} \\
% \text{PPV} &= \dfrac{\text{sensitivity } \times \text{ prevalence}}{[\text{sensitivity } \times \text{ prevalence}] + [\text{(1 - specificity) } \times \text{ (1 - prevalence)}]}.
% \end{align*}

% The generalization of this formula is known as Bayes' Theorem or Bayes' Rule.

% \begin{onebox}{Bayes' Theorem}
% Consider the following conditional probability for variable 1 and variable 2:\vspace{-1.5mm}
% \begin{align*}
% P(\text{outcome $A_1$ of variable 1 } | \text{ outcome $B$ of variable 2}).
% \end{align*}
% Bayes' Theorem states that this conditional probability can be identified as the following fraction:\vspace{-1.5mm}
% \begin{align}
% \frac{P(B | A_1) P(A_1)}
% 	{P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + \cdots + P(B | A_k) P(A_k)},
% 	\label{equationOfBayesTheorem}
% \end{align}
% where $A_2$, $A_3$, ..., and $A_k$ represent all other possible outcomes of the first variable.\index{Bayes' Theorem|textbf}
% \end{onebox}

% The numerator identifies the probability of getting both $A_1$ and $B$. The denominator is the marginal probability of getting $B$. This bottom component of the fraction describes the adding of probabilities from the different ways to get $B$. 

% To apply Bayes' Theorem correctly, there are two preparatory steps:
% \begin{enumerate}
% \setlength{\itemsep}{0mm}
% \item[(1)] First identify the marginal probabilities of each possible outcome of the first variable: $P(A_1)$, $P(A_2)$, ..., $P(A_k)$.
% \item[(2)] Then identify the probability of the outcome $B$, conditioned on each possible scenario for the first variable: $P(B | A_1)$, $P(B | A_2)$, ..., $P(B | A_k)$.
% \end{enumerate}
% Once these probabilities are identified, they can be applied directly within the formula.


% \textD{\newpage}


% \subsubsection{Example~\ref{probabilityOfBreastCancerGivenPositiveTestExample} Solution 3. Contingency Table.}

% The positive predictive value (PPV) of a diagnostic test can be calculated by constructing a two-way contingency table for a large, hypothetical population and calculating conditional probabilities by conditioning on rows or columns. Using a large enough hypothetical population results in an empirical estimate of PPV that is very close to the exact value obtained via using the previously discussed approaches.

% Begin by constructing an empty $2 \times 2$ table, with the possible outcomes of the diagnostic test as the rows, and the possible disease statuses as the columns (Figure~\ref{tableMammogramSetup}). Include cells for the row and column sums.

% Choose a large number $N$, for the hypothetical population size. Typically, $N$ of 100,000 is sufficient for an accurate estimate. 

% \begin{figure}[ht]
% 	\centering
% 	\begin{tabular}{rrrr}
% 		\hline
% 		& Breast Cancer Present & Breast Cancer Absent & Sum \\ 
% 		\hline
% 		Mammogram Positive & -- & -- & -- \\ 
% 		Mammogram Negative & -- & -- & -- \\ 
% 		Sum & -- & -- & 100,000 \\ 
% 		\hline
% 	\end{tabular}
% 	\caption{A $2 \times 2$ table for the mammogram example, with hypothetical population size $N$ of 100,000.}
% 	\label{tableMammogramSetup}
% \end{figure}

% Continue populating the table, using the provided information about the prevalence of breast cancer in this population (0.35\%), the chance of a false negative mammogram (11\%), and the chance of a false positive (7\%):

% \begin{enumerate}
% 	\item Calculate the two column totals (the number of women with and without breast cancer) from $P(\text{BC})$, the disease prevalence:
% 	\[N \times P(\text{BC}) = 100,000 \times .0035 = 350 \text{ women with BC}\]
% 	\[N \times [1 - P(\text{BC})] = 100,000 \times [1 - .0035] = 99,650 \text{ women without BC}\]
	
% 	Alternatively, the number of women without breast cancer can be calculated by subtracting the number of women with breast cancer from $N$.
	
% 	\item Calculate the two numbers in the first column: the number of women who have breast cancer and tested either negative (false negative) or positive (true positive).
% 	\[\text{ women with BC} \times P(\text{false "-"}) = 350 \times .11 = 38.5 \text{ false "-" results}\]
% 	\[\text{ women with BC} \times [1 - P(\text{true "+"})] = 350 \times [1 - .11] = 311.5 \text{ true "+" results}\]
	
% 	\item Calculate the two numbers in the second column: the number of women who do not have breast cancer and tested either positive (false positive) or negative (true negative). 
% 	\[\text{ women without BC} \times P(\text{false "+"}) = 99,650 \times .07 = 6,975.5 \text{ false "+" results}\]
% 	\[\text{ women without BC} \times [1 - P(\text{true "-"})] = 99,650 \times [1 - .07] = 92,674.5 \text{ true "-" results}\]
	
% 	\item Complete the table by calculating the two row totals: the number of positive and negative mammograms out of 100,000.
% 	\[(\text{true "+" results}) + (\text{false "+" results}) = 311.5 + 6,975.5 = 7,287 \text{ "+" mammograms}\]
% 	\[(\text{true "-" results}) + (\text{false "-" results}) = 38.5 + 92,674.5 = 92,713 \text{ "-" mammograms}\]
	
% 	\item Finally, calculate the PPV of the mammogram by using the ratio of the number of true positives to the total number of positive mammograms. This estimate is more than accurate enough, with the calculated value differing only in the third decimal place from the exact calculation, 
	
% 	\[\dfrac{\text{ true "+" results}}{\text{ "+" mammograms}} = \dfrac{311.5}{7,287} = 0.0427. \]
	
% \end{enumerate}

% \begin{figure}[ht]
% 	\centering
% 	\begin{tabular}{rrrr}
% 		\hline
% 		& Breast Cancer Present & Breast Cancer Absent & Sum \\ 
% 		\hline
% 		Mammogram Positive & 311.5 & 6,975.5 & 7,287 \\ 
% 		Mammogram Negative &  38.5 & 92,674.5 & 92,713 \\ 
% 		Sum & 350 & 99,650 & 100,000 \\ 
% 		\hline
% 	\end{tabular}
% 	\caption{Completed table for the mammogram example. The table shows again why the PPV of the mammogram is low: almost 7,300 women will have a positive mammogram result in this hypothetical population, but only \textasciitilde312 of those women actually have breast cancer.}
% 	\label{tableMammogramExample}
% \end{figure}

% \begin{exercisewrap}
% \begin{nexercise}
% Some congenital disorders are caused by errors that occur during cell division, resulting in the presence of additional chromosome copies. Trisomy 21 occurs in approximately 1 out of 800 births. Cell-free fetal DNA (cfDNA) testing is one commonly used way to screen fetuses for trisomy 21. The test sensitivity is 0.98 and the specificity is 0.995. Calculate the PPV and NPV of the test.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{PPV = $\dfrac{P(T^{+}|D) \times P(D)}{[P(T^{+}|D) \times P(D)] + [P(T^{+}|D^c) \times P(D^c)]} = \dfrac{(0.98)(1/800)}{(0.98)(1/800) + (1 - 0.995)(799/800)} = 0.197$. \\ NPV = $\dfrac{P(T^{-}|D^c) \times P(D^c)}{[P(T^{-}|D) \times P(D)] + [P(T^{-}|D^c) \times P(D^c)]} = \dfrac{(0.995)(799/800)}{(1-0.98)(1/800) + (0.995)(799/800)} = 0.999975$.}

% \index{Bayes' Theorem|)}
% \index{tree diagram|)}
% \index{conditional probability|)}
% \index{probability|)}


% %____________
% \section[Extended example]{Extended example: cat genetics}
% \label{sectionExtendedExampleCatGenetics}

% So far, the principles of probability have only been illustrated with short examples. In a more complex setting, it can be surprisingly difficult to accurately translate a problem scenario into the language of probability. This section demonstrates how the rules of probability can be applied to work through a relatively sophisticated conditioning problem.

% \subsubsection{Problem statement}

% The gene that controls white coat color in cats, $KIT$, is known to be responsible for multiple phenotypes such as deafness and blue eye color. A dominant allele $W$ at one location in the gene has complete penetrance for white coat color; all cats with the $W$ allele have white coats. There is incomplete penetrance for blue eyes and deafness; not all white cats will have blue eyes and not all white cats will be deaf. However, deafness and blue eye color are strongly linked, such that white cats with blue eyes are much more likely to be deaf. The variation in penetrance for eye color and deafness may be due to other genes as well as environmental factors.

% Suppose that 30\% of white cats have one blue eye, while 10\% of white cats have two blue eyes. About 73\% of white cats with two blue eyes are deaf and 40\% of white cats with one blue eye are deaf. Only 19\% of white cats with other eye colors are deaf.

% \begin{enumerate}[a)]
% 	\item Calculate the prevalence of deafness among white cats.
	
% 	\item Given that a white cat is deaf, what is the probability that it has two blue eyes?
	
% 	\item Suppose that deaf, white cats have an increased chance of being blind, but that the prevalence of blindness differs according to eye color. While deaf, white cats with two blue eyes or two non-blue eyes have probability 0.20 of developing blindness, deaf and white cats with one blue eye have probability 0.40 of developing blindness. White cats that are not deaf have probability 0.10 of developing blindness, regardless of their eye color.
	
% 	\begin{enumerate}[i.]
% 		\item What is the prevalence of blindness among deaf, white cats?
		
% 		\item What is the prevalence of blindness among white cats?
		
% 		\item Given that a cat is white and blind, what is the probability that it has two blue eyes?
% 	\end{enumerate}
% \end{enumerate}

% \subsubsection{Defining notation}

% Before beginning any calculations, it is essential to clearly define any notation that will be used. For this problem, there are several events of interest: deafness, number of blue eyes (either 0, 1, or 2), and blindness. 

% \begin{itemize}
% 	\item Let $D$ represent the event that a white cat is deaf.
% 	\item Let $B_0$ = \{zero blue eyes\}, $B_1$ = \{one blue eye\}, and $B_2$ = \{two blue eyes\}.
% 	\item Let $L$ represent the event that a white cat is blind.
% \end{itemize}

% Note that since all cats mentioned in the problem are white, it is not necessary to define whiteness as an event; white cats represent the sample space.

% \subsubsection{Part a) Deafness}

% The prevalence of deafness among white cats is the proportion of white cats that are deaf; i.e., the probability of deafness among white cats. In the notation of probability, this question asks for the value of $P(D)$.

% \begin{examplewrap}
% \begin{nexample}{The following information has been given in the problem. Re-write the information using the notation defined earlier.
% 	\begin{quote}
% 		Suppose that 30\% of white cats have one blue eye, while 10\% of white cats have two blue eyes. About 73\% of white cats with two blue eyes are deaf and 40\% of white cats with one blue eye are deaf. Only 19\% of white cats with other eye colors are deaf.
% 	\end{quote}}

% The first sentence provides information about the prevalence of white cats with one blue eye and white cats with two blue eyes: $P(B_1) = 0.30$ and $P(B_2) = 0.10$. The only other possible eye color combination is zero blue eyes (i.e., two non-blue eyes); i.e., since $P(B_0) + P(B_1) + P(B_2) = 1$, $P(B_0) = 1  - P(B_1) - P(B_2) = 0.60$. 60\% of white cats have two non-blue eyes.

% While it is not difficult to recognize that the second and third sentences provide information about deafness in relation to eye color, it can be easy to miss that these probabilities are conditional probabilities. A close reading should focus on the language\textemdash "About 73\% \textit{of} white cats with two blue eyes are deaf...": i.e., out of the white cats that have two blue eyes, 73\% are deaf. Thus, these are probabilities of deafness conditioned on eye color. From these sentences, $P(D|B_2) = 0.73$, $P(D|B_1) = 0.40$, and $P(D|B_0) = 0.19$. 
% \end{nexample}
% \end{examplewrap}

% Consider that there are three possible ways to partition the event $D$, that a white cat is deaf: a cat could be deaf and have two blue eyes, be deaf and have one blue eye (and one non-blue eye), or be deaf and have two non-blue eyes. Thus, by the addition rule of disjoint outcomes: 
% \[P(D) = P(D \textrm{ and } B_2 ) + P(D \textrm{ and } B_1 ) + P(D \textrm{ and } B_0 ).\]

% Although the joint probabilities of being deaf and having particular eye colors are not given in the problem, these can be solved for based on the given information. The definition of conditional probability $P(A|B)$ relates the joint probability $P(A \textrm{ and } B)$ with the marginal probability $P(B)$.\footnote{This rearrangement of the definition of conditional probability, $P(A \textrm{ and } B) = P(A|B)P(B)$, is also known as the general multiplication rule.}  
% \[P(A|B) = \dfrac{P(A \textrm{ and } B)}{P(B)} \qquad P(A \textrm{ and } B) = P(A|B)P(B). \]

% Thus, the probability $P(D)$ is given by:
% \begin{align*}
% P(D) =& P(D \textrm{ and } B_2 ) + P(D \textrm{ and } B_1 ) + P(D \textrm{ and } B_0 ) \\
% =& P(D|B_2)P(B_2) + P(D|B_1)P(B_1) + P(D|B_0)P(B_0) \\
% =& (0.73)(0.10) + (0.40)(0.30) + (0.19)(0.60) \\
% =& 0.307.
% \end{align*}

% The prevalence of deafness among white cats is 0.307.


% \textD{\newpage}


% \subsubsection{Part b) Deafness and eye color}

% The probability that a white cat has two blue eyes, given that it is deaf, can be expressed as $P(B_2|D)$. 

% \begin{examplewrap}
% \begin{nexample}{Using the definition of conditional probability, solve for $P(B_2|D)$.}
	
% \[P(B_2|D) = \dfrac{P(D \textrm{ and } B_2)}{P(D)} = \dfrac{P(D|B_2)P(B_2)}{P(D)} = \dfrac{(0.73)(0.10)}{0.307} = 0.238. \]

% The probability that a white cat has two blue eyes, given that it is deaf, is 0.238.
% \end{nexample}
% \end{examplewrap}

% It is also possible to think of this as a Bayes' Rule problem, where there are three possible partitions of the event of deafness, $D$. In this problem, it is possible to directly solve from the definition of conditional probability since $P(D)$  was solved for in part a); note that the expanded denominator below matches the earlier work to calculate $P(D)$.
% \[P(B_2|D) = \dfrac{P(D \textrm{ and } B_2)}{P(D)} = \dfrac{P(D|B_2)P(B_2)}{P(D|B_2)P(B_2) + P(D|B_1)P(B_1) + P(D|B_0)P(B_0)}. \]


% \subsubsection{Part c) Blindness, deafness, and eye color}

% \begin{examplewrap}
% \begin{nexample}{The following information has been given in the problem. Re-write the information using the notation defined earlier.
% 		\begin{quote}
% 			Suppose that deaf, white cats have an increased chance of being blind, but that the prevalence of blindness differs according to eye color. While deaf, white cats with two blue eyes or two non-blue eyes have probability 0.20 of developing blindness, deaf and white cats with one blue eye have probability 0.40 of developing blindness. White cats that are not deaf have probability 0.10 of developing blindness, regardless of their eye color.
% 		\end{quote}}

% The second sentence gives probabilities of blindness, conditional on eye color and being deaf: $P(L|B_2, D) = P(L|B_0, D) = 0.20$, and $P(L|B_1, D) = 0.40$. The third sentence gives the probability that a white cat is blind, given that it is not deaf: $P(L|D^C) = 0.10$.  
% \end{nexample}
% \end{examplewrap}

% Part i. asks for the prevalence of blindness among deaf, white cats: $P(L|D)$. As in part a), the event of blindness given deafness can be partitioned by eye color:
% \[P(L|D) = P(L \textrm{ and } B_0|D) + P(L \textrm{ and } B_1 | D) + P(L \textrm{ and } |D).\]

% \begin{examplewrap}
% \begin{nexample}{Expand the previous expression using the general multiplication rule, $P(A \textrm{ and } B) = P(A|B)P(B)$.}
	
% 	The general multiplication rule may seem difficult to apply when conditioning is present, but the principle remains the same. Think of the conditioning as a way to restrict the sample space; in this context, conditioning on deafness implies that for this part of the problem, all the cats being considered are deaf (and white). 
	
% 	For instance, consider the first term, $P(L \textrm{ and } B_0|D)$, the probability of being blind and having two non-blue eyes, given deafness. How could this be rewritten if the probability were simply $P(L \textrm{ and } B_0)$? 
% 	\[P(L \textrm{ and } B_0) = P(L|B_0)P(B_0) \]
	
% 	Now, recall that for this part of the problem, the sample space is restricted to deaf (and white) cats. Thus, all of the terms in the expansion should include conditioning on deafness:
% 	\[P(L \textrm{ and } B_0|\textcolor{red}{D}) = P(L|\textcolor{red}{D}, B_0)P(B_0|\textcolor{red}{D}). \]
	
% 	Thus, 
% 	\[P(L|D) = P(L|D, B_0)P(B_0|D) + P(L|D, B_1)P(B_1|D) + P(L|D, B_2)P(B_2|D).\]
% \end{nexample}
% \end{examplewrap}

% Although $P(L|D, B_0)$, $P(L|D, B_1)$, and $P(L|D, B_2)$ are given from the problem statement, $P(B_0|D$, $P(B_1|D)$, and $P(B_2|D)$ are not. However, note that the probability that a white cat has two blue eyes given that it is deaf, $P(B_2|D)$, was calculated in part b). 

% \begin{exercisewrap}
% \begin{nexercise}
% Calculate $P(B_0|D)$ and $P(B_1|D)$.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{\[P(B_0|D) = \dfrac{P(D \textrm{ and } B_0)}{P(D)} = \dfrac{P(D|B_0)P(B_0)}{P(D)} = \dfrac{(0.19)(0.60)}{0.307} = 0.371. \] \[P(B_1|D) = \dfrac{P(D \textrm{ and } B_1)}{P(D)} = \dfrac{P(D|B_1)P(B_1)}{P(D)} = \dfrac{(0.40)(0.30)}{0.307} = 0.391. \]}

% There is now sufficient information to calculate $P(L|D)$: 
% \begin{align*}
% P(L|D) =& P(L \textrm{ and } B_0|D) + P(L \textrm{ and } B_1 | D) + P(L \textrm{ and } |D) \\
% =& P(L|D, B_0)P(B_0|D) + P(L|D, B_1)P(B_1|D) + P(L|D, B_2)P(B_2|D) \\
% =& (0.20)(0.371) + (0.40)(0.391) + (0.20)(0.238) \\
% =& 0.278.
% \end{align*}
% The prevalence of blindness among deaf, white cats is 0.278.

% Part ii. asks for the prevalence of blindness among white cats, $P(L)$. Again, partitioning is an effective strategy. Instead of partitioning by eye color, however, partition by deafness.

% \textD{\newpage}

% \begin{examplewrap}
% \begin{nexample}{Calculate the prevalence of blindness among white cats, $P(L)$.}
% \begin{align*}
% P(L) =& P(L \textrm{ and } D) + P(L \textrm{ and } D^C) \\
% =& P(L|D)P(D) + P(L|D^C)P(D^C) \\
% =& (0.278)(0.307) + (0.10)(1 - 0.307) \\
% =& 0.155.
% \end{align*}	
	
% $P(D)$ was calculated in part a), while $P(L|D)$ was calculated in part c, i. The conditioning probability of blindness given a white cat is not deaf is 0.10, as given in the question statement. By the definition of the complement, $P(D^C) = 1 - P(D)$. 

% The prevalence of blindness among white cats is 0.155.
% \end{nexample}
% \end{examplewrap}

% Part iii. asks for the probability that a cat has two blue eyes, given that it is white and blind. This probability can be expressed as $P(B_2|L)$. Recall that since all cats being discussed in the problem are white, it is not necessary to condition on coat color.

% Start out with the definition of conditional probability:
% \[P(B_2|L) = \dfrac{P(B_2 \textrm{ and } L)}{P(L)}. \]

% The key to calculating $P(B_2|L)$ relies on recognizing that the event a cat is blind and has two blue eyes can be partitioned by whether or not the cat is also deaf:
% \begin{align}
% P(B_2|L) &= \dfrac{P(B_2 \textrm{ and } L \textrm{ and } D) + P(B_2 \textrm{ and } L \textrm{ and } D^C)}{P(L)}.
% \label{pB2GivenL}
% \end{align}

% \begin{examplewrap}
% \begin{nexample}{Draw a tree diagram to organize the events involved in this problem. Identify the branches that represent the possible paths for a white cat to both have two blue eyes and be blind. }

% When drawing a tree diagram, remember that each branch is conditioned on the previous branches. While there are various possible trees, the goal is to construct a tree for which as many of the branches as possible have known probabilities. 

% The tree for this problem will have three branch points, corresponding to either deafness, blindness, or eye color. The first set of branches contain unconditional probabilities, the second set contains conditional probabilities given one event, and the third set contains conditional probabilities given two events.

% Recall that the probabilities $P(L|D, B_0)$, $P(L|D, B_1)$, and $P(L|D, B_2)$ were provided in the problem statement. These are the only probabilities conditioned on two events that have previously appeared in the problem, so blindness is the most convenient choice of third branch point. 

% It is not immediately obvious whether it will be more efficient to start with deafness or eye color, since unconditional and conditional probabilities related to both have appeared in the problem. Figure~\ref{catGeneticsTrees} shows two trees, one starting with deafness and the other starting with eye color. The two possible paths for a white cat to both have two blue eyes and be blind are shown in green.
% \end{nexample}
% \end{examplewrap}

% \begin{figure}[h]
% \centering
% \subfigure[]{\includegraphics[width=0.485\textwidth]{ch_02a_probability_oi_biostat/figures/catGeneticsTrees/catGeneticsTreesA}
% 	\label{catGeneticsTreesA}
% 	}
% \subfigure[]{\includegraphics[width=0.485\textwidth]{ch_02a_probability_oi_biostat/figures/catGeneticsTrees/catGeneticsTreesB}
% 	\label{catGeneticsTreesB}
% 	}
% \caption{In \subref{catGeneticsTreesA}, the first branch is based on deafness, while in \subref{catGeneticsTreesB}, the first branch is based on eye color. }
% \label{catGeneticsTrees}
% \end{figure}

% \begin{examplewrap}
% \begin{nexample}{Expand Equation~\ref{pB2GivenL} according to the tree shown in Figure~\ref{catGeneticsTreesA}, and solve for $P(B_2|L)$.}
	
% \begin{align*}
% P(B_2|L) &= \dfrac{P(B_2 \textrm{ and } L \textrm{ and } D) + P(B_2 \textrm{ and } L \textrm{ and } D^C)}{P(L)} \\
% &= \dfrac{P(L|B_2, D)P(B_2|D)P(D) + P(L|B_2, D^C)P(B_2|D^C)P(D^C)}{P(L)} \\
% &= \dfrac{(0.20)(0.238)(0.307) + (0.10)P(B_2|D^C)P(D^C)}{0.155}.
% \end{align*}

% Two of the probabilities have not been calculated previously: $P(B_2|D^C)$ and $P(D^C)$. From the definition of the complement, $P(D^C) = 1 - P(D) = 0.693$; $P(D)$ was calculated in part a). To calculate $P(B_2|D^C)$, apply the definition of conditional probability as in part b), where $P(B_2|D)$ was calculated:
% \[P(B_2|D^C) = \dfrac{P(D^C \textrm{ and } B_2)}{P(D^C)} = \dfrac{P(D^C|B_2)(P(B_2)}{P(D^C)} = \dfrac{(1-0.73)(0.10)}{0.693} = 0.0390.\]

% \begin{align*}
% P(B_2|L) &= \dfrac{(0.20)(0.238)(0.307) + (0.10)(0.0390)(0.693)}{0.155} \\
% &= 0.112
% \end{align*}
	
% The probability that a white cat has two blue eyes, given that it is blind, is 0.112.
% \end{nexample}
% \end{examplewrap}

% \begin{exercisewrap}
% \begin{nexercise}
% Expand Equation~\ref{pB2GivenL} according to the tree shown in Figure~\ref{catGeneticsTreesB}, and solve for $P(B_2|L)$.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{\[P(B_2|L) = \frac{P(L|B_2, D)P(D|B_2)P(B_2) + P(L|B_2, D^C)P(D^C|B_2)P(B_2)}{P(L)} = \frac{(0.20)(0.73)(0.10) + (0.10)(1-0.73)(0.10)}{0.155} = 0.112.\]}

% A tree diagram is useful for visualizing the different possible ways that a certain set of outcomes can occur. Although conditional probabilities can certainly be calculated without the help of tree diagrams, it is often easy to make errors with a strictly algebraic approach. Once a tree is constructed, it can be used to solve for several probabilities of interest. The following example shows how one of the previous trees can be applied to answer a different question than the one posed in part c), iii.

% \textD{\newpage}

% \begin{examplewrap}
% \begin{nexample}{What is the probability that a white cat has one blue eye and one non-blue eye, given that it is not~blind?}

% Calculate $P(B_1|L^C)$. Start with the definition of conditional probability, then expand.

% \[P(B_1|L^C) = \dfrac{P(B_1 \textrm{ and } L^C)}{P(L^C)} = \frac{P(B_1 \textrm{ and } L^C \textrm{ and } D) + P(B_1 \textrm{ and } L^C \textrm{ and } D^C)}{P(L^C)}. \]


% Figure~\ref{catGeneticsTreesC} is a reproduction of the earlier tree diagram (Figure~\ref{catGeneticsTreesB}), with yellow arrows showing the two paths of interest.
	
% 	As before, expand the numerator and fill in the known values.
	
% 	\begin{align*}
% 	P(B_1|L^C) =& \frac{P(B_1 \textrm{ and } L^C \textrm{ and } D) + P(B_1 \textrm{ and } L^C \textrm{ and } D^C)}{P(L^C)}\\
% 	=& \frac{P(L^C|D, B_1)P(D|B_1)P(B_1) + P(L^C|D^C, B_1)P(D^C|B_1)P(B_1)}{P(L^C)} \\
% 	=& \frac{\textcolor{red}{\mathbf{P(L^C|D, B_1)}}(0.40)(0.30) + \textcolor{red}{\mathbf{P(L^C|D^C, B_1)P(D^C|B_1)}}(0.30)}{\textcolor{red}{\mathbf{P(L^C)}}}.
% 	\end{align*} 
	
% 	The probabilities in \textbf{\color{red}bold} are not known. Apply the definition of the complement; recall that the rule for complements holds when an event and its complement are conditioned on the same information: $P(A|B) = 1 - P(A^C|B)$.
	
% 	\begin{itemize}
% 		\item $P(L^C) = 1 - P(L) = 1 - 0.155 = 0.845$
% 		\item $P(D^C|B_1) = 1 - P(D|B_1) = 1 - 0.40 = 0.60$
% 		\item $P(L^C|D, B_1) = 1 - P(L|D, B_1) = 1 - 0.40 = 0.60$
% 	\end{itemize}
	
% 	The definition of the complement can also be applied to calculate $P(L^C|D^C, B_1)$. The problem statement originally specified that white cats that are not deaf have probability 0.10 of developing blindness regardless of eye color: $P(L|D^C) = 0.10$. Thus, $P(L^C|D^C, B_1) = P(L^C|D^C)$. By the definition of the complement, $P(L^C|D^C) = 1 - P(L|D^C) = 1 - 0.10 = 0.90$.
	
% 	\begin{align*}
% 	P(B_1|L^C) =& \frac{P(B_1 \textrm{ and } L^C \textrm{ and } D) + P(B_1 \textrm{ and } L^C \textrm{ and } D^C)}{P(L^C)}\\
% 	=& \frac{P(L^C|D, B_1)P(D|B_1)P(B_1) + P(L^C|D^C, B_1)P(D^C|B_1)P(B_1)}{P(L^C)} \\
% 	=& \frac{(0.60)(0.40)(0.30) + (0.90)(0.60)(0.30)}{0.845} \\
% 	=& 0.277.
% 	\end{align*} 
	
% 	The probability that a white cat has one blue eye and one non-blue eye, given that it is not blind, is~0.277.
% \end{nexample}
% \end{examplewrap}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.65\textwidth]{ch_02a_probability_oi_biostat/figures/catGeneticsTrees/catGeneticsTreesC}
% \caption{The two possible paths for a white cat to both have one blue eye (and one non-blue eye) and to not be blind are shown in yellow.}
% \label{catGeneticsTreesC}
% \end{figure}
	

% %___________
% \section{Notes}
% \label{notesChapterProbability}

% Probability is a powerful framework for quantifying uncertainty and randomness. In particular, conditional probability represents a way to update the uncertainty associated with an event given that specific information has been observed. For example, the probability that a person has a particular disease can be adjusted based on observed information, such as age, sex, or the results of a diagnostic test.

% As discussed in the text, there are several possible approaches to solving conditional probability problems, including the use of tree diagrams or contingency tables. It can also be intuitive to use a simulation approach in computing software; refer to the labs for details about this method. Regardless of the specific approach that will be used for calculation, it is always advisable to start any problem by understanding the problem context (i.e., the sample space, given information, probabilities of interest) and reading the problem carefully, in order to avoid mistakes when translating between words and probability notation. A common mistake is to confuse joint and conditional probabilities.

% Probability distributions were briefly introduced in Section~\ref{introProbDistributions}. This topic will be discussed in greater detail in the next chapter.

% Probability forms the foundation for data analysis and statistical inference, since nearly every conclusion to a study should be accompanied by a measure of uncertainty. For example, the publication reporting the results of the LEAP study discussed in Chapter~\ref{introductionToData} included the probability that the observed results could have been due to chance variation. This aspect of probability will be discussed in later chapters.

% The four labs for Chapter 2 cover basic principles of probability, conditional probability, positive predictive value of a diagnostic test (via Bayes' Theorem), and the calculation of probabilities conditional on several events in the context of genetic inheritance. Probabilities can be calculated algebraically, using formulas given in this and other texts, but can also be calculated with simple simulations, since a probability represents a proportion of times an event happens when an experiment is repeated many times.  Computers are particularly good at keeping track of events during many replications of an experiment.  The labs for this chapter use both algebraic and simulation methods, and are particularly useful for building programming skills with the \textsf{R} language.

% In medicine, the positive predictive value of a diagnostic test may be one of the most important applications of probability theory. It is certainly the most common. The positive predictive value of a test is the conditional probability of the presence of a disease or condition, given a positive test for the condition, and is often used when counseling patients about their risk for being diagnosed with a disease in the future.  The lab on positive predictive value examines the conditional probability of a trisomy 21 genetic mutation (Down syndrome) given that a test based on cell-free DNA suggests its presence.


%%%% CAMBIO: ADDED CHAPTER 3

%!TEX root=../../main.tex

%\index{random variable|(}

%%% CAMBIO:  Introductory section removed

%\chapterintro{{When planning clinical research studies, investigators try to anticipate the results they might see under certain hypotheses. The treatments for some forms of cancer, such as advanced lung cancer, are only effective in a small percentage of patients: typically 20\% or less. Suppose that a study testing a new treatment will be conducted on 20 participants, where the working assumption is that 20\% of the patients will respond to the treatment. How might the possible outcomes of the study be represented, along with their probabilities? It is possible to express various outcomes using the probability notation in the previous chapter, e.g. if $A$ were the event that one patient responds to treatment, but this would quickly become unwieldy. \\

%\noindent%
%Instead, the anticipated outcome in the study can be represented as a \term{random variable}, which numerically summarizes the possible outcomes of a random experiment. For example, let $X$ represent the number of patients who respond to treatment; a numerical value $x$ can be assigned to each possible outcome, and the probabilities of $1, 2, \dots, x$ patients having a good response can be expressed as $P(X = 1), P(X = 2), \dots , P(X = x)$. The distribution of a random variable specifies the probability of each possible outcome associated with the random variable. \\

%\noindent%
%This chapter will begin by outlining general properties of random variables and their distributions. The rest of the chapter discusses specific named distributions that are commonly used throughout probability and statistics.}}

%_________________
\section{Random variables}
\label{randomVariablesSection}


\subsection{Distributions of random variables}

\index{random variable|(}
Formally, a \term{random variable} assigns numerical values to the outcome of a random phenomenon, and is usually written with a capital letter such as $X$, $Y$, or $Z$. 

If a coin is tossed three times, the outcome is the sequence of observed heads and tails. One such outcome might be TTH: tails on the first two tosses, heads on the third. If the random variable $X$ is the number of heads for the three tosses, $X=1$; if $Y$ is the number of tails, then $Y=2$. For the sequence THT, only the order has changed, but the values of $X$ and $Y$ remain the same. For the sequence HHH, however, $X=3$ and $Y=0$. Even in this simple setting, is possible to define other random variables; for example, if $Z$ is the toss when the first H occurs, then $Z=3$ for the first set of tosses (TTH) and 1 for the third set (HHH).  

\begin{figure}[h]
	\centering
	\includegraphics[width=0.70\textwidth]
	{ch_02a_probability_oi_biostat/figures/coinToss/coinToss.png}
	\caption{Possible outcomes for number of heads in three tosses of a coin.}
	\label{coinToss}
\end{figure}

If probabilities can be assigned to the outcomes in a random phenomenon or study, then those can be used to assign probabilities to values of a random variable.  Using independence, $P(\text{HHH}) = (1/2)^3 = 1/8$.  Since $X$ in the above example can only be three if the three tosses are all heads, $P(X=3) = 1/8$.  The distribution of a random variable is the collection of probabilities for all of the variable's unique values. Figure~\ref{coinToss} shows the eight possible outcomes when a coin is cossed three times: TTT, HTT, THT, TTH, HHT, HTH, THH, HHH. For the first set of tosses, $X = 0$; for the next three, $X=1$, then $X=2$ for the following three tosses and $X=3$ for the last set (HHH).  

Using independence again, each of the 8 outcomes have probability 1/8, so $P(X = 0) = P(X = 3) = 1/8$ and $P(X = 1) = P(X = 2) = 3/8$. Figure~\ref{distCoinTossing} shows the probability distribution for $X$.  Probability distributions for random variables follow the rules for probability; for instance, the sum of the probabilities must be 1.00.  The possible outcomes of $X$ are labeled with a corresponding lower case letter $x$ and subscripts.  The values of X are $x_1=0$, $x_2=1$,  $x_3 = 2$, and $x_4 = 3$; these occur with probabilities $1/8$, $3/8$, $3/8$ and $1/8$.

\begin{figure}[h]
	\centering 
	\begin{tabular}{l rrrr l}
		\hline 
		$i$ & 1 & 2 & 3 & 4 & Total\\
		\hline
		$x_i$ & 0 & 1 & 2 & 3 & --\\
		$P(X = x_i)$ & 1/8 & 3/8 & 3/8 & 1/8 & 8/8 = 1.00\\
	\end{tabular}
	\caption{Tabular form for the distribution of the number of heads in three coin tosses.}
	\label{distCoinTossing}
\end{figure}

%\textD{\newpage}
Another example of  probability distribution consists of all disjoint outcomes and their associated probabilities. Figure~\ref{diceProb} shows the probability distribution for the sum of two dice. 

\begin{figure}[h] \small
\centering
\begin{tabular}{l ccc ccc ccc cc}
  \hline
  \ \vspace{-3mm} \\
Dice sum\vspace{0.3mm} & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12  \\
Probability & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$\vspace{1mm} \\
   \hline
\end{tabular}
\caption{Probability distribution for the sum of two dice.}
\label{diceProb}
\end{figure}

\begin{onebox}{Rules for a probability distribution}
A probability distribution is a list of all possible outcomes and their associated probabilities that satisfies three rules: \vspace{-2mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item The outcomes listed must be disjoint.
\item Each probability must be between 0 and 1.
\item The probabilities must total to 1. \vspace{1mm}
\end{enumerate}
\end{onebox}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.56\textwidth]
	{ch_02a_probability_oi_biostat/figures/barPlotCoinTossing/barPlotCoinTossing.pdf}
	\caption{Bar plot of the distribution of the number of heads in three coin tosses.}
	\label{barPlotCoinTossing}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=0.73\textwidth]{ch_02a_probability_oi_biostat/figures/diceSumDist/diceSumDist}
\caption{The probability distribution of the sum of two dice.}
\label{diceSumDist}
\end{figure}

Bar graphs can be used to show the distribution of a random variable.  Figure~\ref{barPlotCoinTossing} is a bar graph of the distribution of $X$ in the coin tossing example and Figure~\ref{diceSumDist} is a bar graph of the distribution in the sum of two dice. When bar graphs are used to show the distribution of a dataset, the heights of the bars show the frequency of observations; in contrast, bar heights for a probability distribution show the probabilities of possible values of a random variable.

$X$ is an example of a \term{discrete random variable} since it takes on a finite number of values.\footnote{Some discrete random variables have an infinite number of possible values, such as all the non-negative integers.}

A \term{continuous random variable} can take on any real value in an interval. 

% In the hypothetical clinical study described at the beginning of this section, how unlikely would it be for 12 or more patients to respond to the treatment, given that only 20\% of patients are expected to respond? Suppose $X$ is a random variable that will denote the possible number of responding patients, out of a total of 20. $X$ will have the same probability distribution as the number of heads in a 20 tosses of a weighted coin, where the probability of landing heads is 0.20. The graph of the probability distribution for $X$ in Figure~\ref{distRespClinStudy} can be used to approximate this probability. The event of 12 or more consists of nine values (12, 13, \dots, 20); the graph shows that the probabilities for each value is extremely small, so the chance of 12 or more responses must be less than 0.01.\footnote{Formulas in Section~\ref{binomialModel} can be used to show that the exact probability is slightly larger than 0.0001.}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.72\textwidth]
% 	{ch_02a_probability_oi_biostat/figures/distRespClinStudy/distRespClinStudy.pdf}
% 	\caption{Bar plot of the distribution of the number of responses in a study with 20 participants and response probability 0.20}
% 	\label{distRespClinStudy}
% \end{figure}

%%% CAMBIO: sction added from section 2.1.5 from the original textbook 

Consider how the probability distribution for adult heights in the US might best be represented. Unlike the sum of two dice rolls, height can occupy any value over a continuous range. Thus, height is a random variable which has a continuous probability distribution, which is specified by a \term{probability density function} rather than a table; Figure~\ref{fdicHeightContDist} shows a histogram of the height for 3 million US adults from the mid-1990's, with an overlaid density curve.\footnote{This sample can be considered a simple random sample from the US population. It relies on the USDA Food Commodity Intake Database.} 

Just as in the discrete case, the probabilities of all possible outcomes must still sum to 1; the total area under a probability density function equals 1. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{ch_02a_probability_oi_biostat/figures/fdicHeightContDist/fdicHeightContDist}
	\caption{The continuous probability distribution of heights for US adults.}
	\label{fdicHeightContDist}	
\end{figure}

%\textD{\newpage}

\begin{examplewrap}
\begin{nexample}{Estimate the probability that a randomly selected adult from the US population has height between 180 and 185 centimeters. In Figure~\ref{usHeightsHist180185}, the two bins between 180 and 185 centimeters have counts of 195,307 and 156,239 people.}\label{probabilityOfBetween180185}%

Find the proportion of the histogram's area that falls in the range \resp{180} cm and \resp{185}: add the heights of the bins in the range and divide by the sample size:

\begin{align*}                                                    
\frac{195,307+156,239}{\text{3,000,000}} = 0.1172.               
\end{align*}

The probability can be calculated precisely with the use of computing software, by finding the area of the shaded region under the curve between \resp{180} and \resp{185}:
\begin{align*}
P(\text{\var{height} between \resp{180} and \resp{185}})
= \text{area between \resp{180} and \resp{185}}
= 0.1157.
\end{align*}
\end{nexample}
\end{examplewrap}

\begin{figure}[h]
	\centering
	\subfigure[]{
		\includegraphics[width=0.485\textwidth]{ch_02a_probability_oi_biostat/figures/usHeightsHist180185/usHeightsHist180185}
		\label{usHeightsHist180185}
	}
	\subfigure[]{
		\includegraphics[width=0.485\textwidth]{ch_02a_probability_oi_biostat/figures/fdicHeightContDistFilled/fdicHeightContDistFilled}
		\label{fdicHeightContDistFilled}		
	}
	\caption{\subref{usHeightsHist180185} A histogram with bin sizes of 2.5 cm, with bars between \resp{180} and \resp{185} cm shaded.  \subref{fdicHeightContDistFilled} Density for heights in the US adult population with the area between \resp{180} and \resp{185} cm shaded.}
\end{figure}

\begin{examplewrap}
\begin{nexample}{What is the probability that a randomly selected person is \textbf{exactly} \resp{180}~cm? Assume that height can be measured perfectly.}
	\label{probabilityOfExactly180cm}
	This probability is zero. A person might be close to \resp{180} cm, but not exactly \resp{180} cm tall. This also coheres with the definition of probability as an area under the density curve; there is no area captured between \resp{180}~cm and \resp{180}~cm.
\end{nexample}
\end{examplewrap}

\begin{exercisewrap}
\begin{nexercise}
Suppose a person's height is rounded to the nearest centimeter. Is there a chance that a random person's \textbf{measured} height will be \resp{180} cm?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{This has positive probability. Anyone between \resp{179.5} cm and \resp{180.5} cm will have a \emph{measured} height of \resp{180} cm. This a more realistic scenario to encounter in practice versus Example~\ref{probabilityOfExactly180cm}.}

\index{data!FCID|)}


\begin{onebox}{Some properties of continuous distributions}
Let $X$ be a continuous random variable, $a$ and $b$ real numbers. 
\begin{multicols}{2}
  \begin{enumerate}
\item $P(X=a)=0$ 
\item $P(X\leq a)=P(X<a)$
\item $P(-\infty<X<+\infty)=1$  
\item $P(X>a)=1-P(X<a)$ 
\item $P(a<X<b)=P(X<b)-P(X<a)$
\end{enumerate}
\end{multicols}

\end{onebox}

%%%% CAMBIO: End of added section

\subsection{Expectation} 
\label{section:expectationRandomVariable}

\index{expectation|(}

Just like distributions of data, distributions of random variables also have means, variances, standard deviations, medians, etc.; these characteristics are computed a bit differently for random variables. The mean of a random variable is called its \term{expected value} and written $E(X)$. To calculate the mean of a random variable, multiply each possible value by its corresponding probability and add these products.

\begin{onebox}{Expected value of a discrete random variable}
If $X$ takes on outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, ..., $P(X=x_k)$, the expected value of $X$ is the sum of each outcome multiplied by its corresponding probability:
\begin{align}
E(X) 	&= x_1 P(X=x_1) + \cdots + x_k P(X=x_k) \notag \\
&= \sum_{i=1}^{k}x_iP(X=x_i).
\end{align}
The Greek letter $\mu$\index{Greek!mu ($\mu$)} may be used in place of the notation $E(X)$.
\end{onebox}

\begin{examplewrap}
\begin{nexample}{Calculate the expected value of $X$, where $X$ represents the number of heads in three tosses of a fair coin.}
	
	$X$ can take on values 0, 1, 2, and 3. The probability of each $x_k$ is given in Figure~\ref{distCoinTossing}.

\begin{align*}
E(X) &= x_1 P(X = x_1) + \dots + x_k P(X = x_k)\\
&= (0)(P(X=0)) + (1)(P(X=1)) + (2)(P(X=2)) + (3)(P(X = 3)) \\
&= (0)(1/8) + (1)(3/8) + (2)(3/8) + (3)(1/8) = 12/8 \\
&= 1.5.
\end{align*}

The expected value of $X$ is 1.5.
\end{nexample}
\end{examplewrap}

The expected value for a random variable represents the average outcome. For example, $E(X)=1.5$ represents the average number of heads in three tosses of a coin, if the three tosses were repeated many times.\footnote{The expected value $E(X)$ can also be expressed as $\mu$, e.g. $\mu=1.5$}  It often happens with discrete random variables that the expected value is not precisely one of the possible outcomes of the variable.

\marginpar[\raggedright\vspace{-47mm}

$\text{E}(X)$\vspace{1mm}\\\footnotesize Expected Value\\of $X$]{\raggedright\vspace{-47mm}
	
	$\text{E}(X)$\vspace{1mm}\\\footnotesize Expected Value \\of $X$}
\vspace{0mm}

\begin{exercisewrap}
\begin{nexercise}
Calculate the expected value of $Y$, where $Y$ represents the number of heads in three tosses of an unfair coin, where the probability of heads is 0.70.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{First, calculate the probability distribution. $P(Y=0) = (1 - 0.70)^3 = 0.027$ and $P(Y=3) = (0.70)^3 = 0.343.$ Note that there are three ways to obtain 1 head (HTT, THT, TTH), thus, $P(Y=1) = (3)(0.70)(1 - 0.70)^2 = 0.189$. By the same logic, $P(Y = 2) = (3)(0.70)^2( 1- 0.70) = 0.441$. Thus, $E(Y) = (0)(0.027) + (1)(0.189) + (2)(0.441) + (3)(0.343) = 2.1$. The expected value of $Y$ is 2.1.}

\index{expectation|)}

% CAMBIO: Added comment about distribution


It is also possible to compute the expected value of a continuous
random variable. However, it requires a
little calculus and this is beyond the scope of this course.\footnote{$\mu = \int xf(x)dx$ where $f(x)$ represents the probability density function of the random variable.}

In Physics, the expectation holds the same meaning as the center of gravity. The distribution can be represented by a series of weights at each outcome, and the mean represents the balancing point. Figure~\ref{contBalance} shows a continuous probability distribution balanced atop a wedge placed at the mean.


\begin{figure}
\centering
\includegraphics[width=0.68\textwidth]{ch_02a_probability_oi_biostat/figures/contBalance/contBalance}
\caption{A continuous distribution can also be balanced at its mean.}
\label{contBalance}
\end{figure}

%\textD{\newpage}


\subsection{Variability of random variables} 
\label{section:varianceRandomVariable}

The variability of a random variable can be described with \indexthis{variance}{variance} and \indexthis{standard deviation}{standard deviation}. For data, the variance is computed by squaring deviations from the mean ($x_i - \mu$) and then averaging over the number of values in the dataset (Section~\ref{measuresOfSpread}). 

In the case of a random variable, the squared deviations from the mean of the random variable are used instead, and their sum is weighted by the corresponding probabilities. This weighted sum of squared deviations equals the variance; the standard deviation is the square root of the variance.

\begin{onebox}{Variance of a discrete random variable}
If $X$ takes on outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, \dots, $P(X=x_k)$ and expected value $\mu=E(X)$, then the variance of $X$, denoted by $\text{Var}(X)$ or $\sigma^2$, is
\begin{align}
Var(X) &= (x_1-\mu)^2 P(X=x_1) + \cdots \notag + (x_k-\mu)^2 P(X=x_k) \notag \\
&= \sum_{i=1}^{k} (x_i - \mu)^2 P(X=x_i).
\end{align}
The standard deviation of $X$, labeled $SD(X)$ or $\sigma$\index{Greek!sigma ($\sigma$)}, is the square root of the variance.
\end{onebox}%
\marginpar[\raggedright\vspace{-47mm}

$\text{Var}(X)$\vspace{1mm}\\\footnotesize Variance\\of $X$]{\raggedright\vspace{-47mm}
	
	$\text{Var}(X)$\vspace{1mm}\\\footnotesize Variance\\of $X$}

The variance of a random variable can be interpreted as the expectation of the terms $(x_i - \mu)^2$; i.e., $\sigma^2 = E(X - \mu)^2$. While this compact form is not useful for direct computation, it can be helpful for understanding the concept of variability in the context of a random variable; variance is simply the average of the deviations from the mean.


\begin{examplewrap}
\begin{nexample}{Compute the variance and standard deviation of $X$, the number of heads in three tosses of a fair~coin.}
	
	In the formula for the variance, $k = 4$ and $\mu_X = E(X) = 1.5$. 
	\begin{align*}
	\sigma_X^2 &= (x_1-\mu_X)^2P(X=x_1) + \cdots \notag + (x_4-\mu)^2 P(X=x_4) \notag \\
	&= (0- 1.5)^2(1/8) + (1 - 1.5)^2 (3/8) + 
	(2 -1.5)^2 (3/8) + (3-1.5)^2 (1/8) \notag \\
	&= 3/4.
	\end{align*}
	
	The variance is $3/4 = 0.75$ and the standard deviation is $\sqrt{3/4} = 0.866$.  
\end{nexample}
\end{examplewrap}

The coin tossing scenario provides a simple illustration of the mean and variance of a random variable.
% CAMBIO: removed example about health care insurance 
%For the rest of this section, a more realistic example will be discussed\textemdash calculating expected health care costs.

% \label{healthCareCostsEmployee}
% In most typical health insurance plans in the United States, members of the plan pay annually in three categories: a monthly premium, a deductible amount that members pay each year before the insurance covers service, and ``out-of-pocket'' costs which include co-payments for each physician visit or prescription.\footnote{The deductible also includes care and supplies that are not covered by insurance.} Picking a new health plan involves estimating costs for the next year based on a person's best guess at the type and number of services that will be needed.

% \textD{\newpage}

% In 2015, Harvard University offered several alternative plans to its employees. In the Health Maintenance Organization (HMO) plan for employees earning less than \$70,000 per year, the monthly premium was \$79, and the co-payment for each office visit or physical therapy session was \$20. After a new employee examined her health records for the last 10 years, she noticed that in three of the 10 years, she visited the office of her primary care physician only once, for one annual physical. In four of the 10 years, she visited her physician three times: once for a physical, and twice for cases of the flu. In two of the years, she had four visits. In one of the 10 years, she experienced a knee injury that required 3 office visits and 5 physical therapy sessions.

% \begin{examplewrap}
% \begin{nexample}{Ignoring the cost of prescription drugs, over-the-counter medications, and the annual deductible amount, calculate the expectation and the standard deviation of the expected annual health care cost for this employee.}  \label{example:healthCareCosts}
	
% 	Let the random variable $X$ denote annual health care costs, where $x_i$ represents the costs in a year for $i$ number of visits. If the last ten years are an accurate picture of annual costs for this employee, $X$ will have four possible values. 
	
% 	The total cost of the monthly premiums in a single year is $12 \times \$79 = \$948$. The cost of each visit is \$20, so the total visit cost for a year is \$20 times the number of visits. 
	
% 	For example, the first column in the table contains information about the years in which the employee had one office visit. Adding the \$948 for the annual premium and \$20 for one visit results in $x_{1}=\$968$; $P(X=x_{i}) = 3/10 = 0.30$.
	
% 	\begin{center}
% 		\begin{tabular}{l rrrr r}
% 			\hline
% 			$i$ & 1 & 2 & 3 & 4 & Sum \\
% 			\hline
% 			Number of visits & 1 & 3 & 4 & 8 &\\
% 			$x_i$ & 968 & 1008 & 1028 & 1108 &  \\
% 			$P(X=x_i)$ & 0.30 & 0.40 & 0.20 & 0.10 & 1.00 \\
% 			$x_i P(X=x_i)$ & 290.40 & 403.20 & 205.60 & 110.80 & 1010.00 \\
% 			\hline
% 		\end{tabular}
% 	\end{center}
% 	The expected cost of health care for a year, $\sum_i x_i P(X = x_i)$, is $\mu=\$1010.00$.
% 	\begin{center}
% 		\begin{tabular}{l rrrr r}
% 			\hline
% 			$i$ & 1 & 2 & 3 & 4 & Sum \\
% 			\hline
% 			Number of visits & 1 & 3 & 4 & 8 &\\
% 			$x_i$ & 968 & 1008 & 1028 & 1108 &  \\
% 			$P(X=x_i)$ & 0.30 & 0.40 & 0.20 & 0.10 & 1.00 \\
% 			$(x_i)P(X=x_i)$ & 290.40 & 403.20 & 205.60 & 110.80 & 1010.00 \\
% 			\hline
% 			$x_i - \mu$ & -42.00  & -2.00  & 18.00  & 98.00 & \\
% 			$(x_i - \mu)^2$ &  1764.00 & 4.00  & 324.00  & 9604 & \\
% 			$(x_i - \mu)^2 P(X=x_i)$ & 529.20  & 1.60  & 64.80  & 960.40 & 1556.00\\
% 			\hline
% 		\end{tabular}
% 	\end{center}
% 	The variance of $X$, $\sum_i (x_i - \mu)^2 P(X = x_i)$,  is $\sigma^2 = 1556.00$, and the standard deviation is $\sigma = \$39.45$.\footnotemark{}
% \end{nexample}
% \end{examplewrap}
% \footnotetext{Note that the standard deviation always has the same units as the original measurements.}


% \textD{\newpage}


\subsection{Linear combinations of random variables}

Sums of random variables arise naturally in many problems. In a health insurance, the amount spent by the employee during her next five years of employment can be represented as $X_1 + X_2 + X_3 + X_4 + X_5$, where $X_1$ is the cost of the first year, $X_2$ the second year, etc. If the employee's domestic partner has health insurance with another employer, the total annual cost to the couple would be the sum of the costs for the employee ($X$) and for her partner ($Y$), or $X + Y$. In~each of these examples, it is intuitively clear that the average cost would be the sum of the average of each~term.

Sums of random variables represent a special case of linear combinations of variables.  

\begin{onebox}{Linear combinations of random variables and their expected values}
If $X$ and $Y$ are random variables, then a linear combination of the random variables is given by
\[aX + bY,\] \label{linComboOfRandomVariablesXAndY}
where $a$ and $b$ are constants.  The mean of a linear combination of random variables is 
\[E(aX + bY) = aE(X) + bE(Y) = a\mu_X + b\mu_Y.\]
\end{onebox}

The formula easily generalizes to a sum of any number of random variables. For example, the average health care cost for 5 years, given that the cost for services remains the same, is
\[E(X_1 + X_2 + X_3 + X_4 + X_5) = E(5 X_1) = 5E(X_1) =(5)(1010) = \$5,050.\]

The formula implies that for a random variable $Z$, $E(a + Z) = a + E(Z)$.  This could have been used when calculating the average health costs for the employee by defining $a$ as the fixed cost of the premium ($a=\$948$) and $Z$ as the cost of the physician visits. Thus, the total annual cost for a year could be calculated as: $E(a + Z) = a + E(Z) = \$948 + E(Z) = \$948 + .30(1 \times \$20) + .40(3 \times \$20) + .20(4 \times \$20) + 0.10(8 \times \$20)= \$1,010.00$. 

\begin{exercisewrap}
\begin{nexercise}\label{healthCareCostsPartner}%
Suppose the employee will begin a domestic partnership in the next year. Although she and her companion will begin living together and sharing expenses, they will each keep their existing health insurance plans; both, in fact, have the same plan from the same employer. In the last five years, her partner visited a physician only once in four of the ten years, and twice in the other six years. Calculate the expected total cost of health insurance to the couple in the next year.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{Let $X$ represent the costs for the employee and $Y$ represent the costs for her partner. $E(X) = \$1,010.00$, as previously calculated. $E(Y) = 948 + 0.4(1 \times \$20) + 0.6(2 \times \$20) = \$980.00$. Thus, $E(X + Y) = E(X) + E(Y) = \$1,010.00 + \$980.00 = \$1,990.00$.}

Calculating the variance and standard deviation of a linear combination of random variables requires more care.  The formula given here requires that the random variables in the linear combination be independent, such that an observation on one of the variables provides no information about the value of the other variable. 

%\textD{\newpage}

\begin{onebox}{Variability of linear combinations of random variables}
\[\text{Var}(aX + bY) = a^2 \text{Var}(X) + b^2\text{Var}(Y).\]
This equation is valid only if the random variables are independent of each other.
\end{onebox}

For the transformation $a + bZ$, the variance is $b^{2} \text{Var}(Z)$, since a constant $a$ has variance 0.  When $b = 1$, variance of $a + Z$ is $\text{Var}(Z)$\textemdash adding a constant to a random variable has no effect on the variability of the random variable.

\begin{examplewrap}
\begin{nexample}{Calculate the variance and standard deviation for the combined cost of next year's health care for the two partners, assuming that the costs for each person are independent.}\label{sdHealthCostsPartners} 

Let $X$ represent the sum of costs for the employee and $Y$ the sum of costs for her partner.
	
First, calculate the variance of health care costs for the partner. The partner's costs are the sum of the annual fixed cost and the variable annual costs, so the variance will simply be the variance of the variable costs. If $Z$ represents the component of the variable costs, $E(Z) = 0.4(1 \times \$20) + 0.6(2 \times \$20) = \$8 + \$24 = \$32$. Thus, the variance of $Z$ equals
\[\textrm{Var}(Z) = 0.4(20 - 32)^2 + 0.6(40 - 32)^2 = 96. \]

Under the assumption of independence, Var$(X + Y)$ = $\text{Var}(X) + \text{Var}(Y) = 1556 + 96 = 1652$, and the standard deviation is $\sqrt{1652} = \$40.64$.
\end{nexample}
\end{examplewrap}

% The example of health insurance costs has been simplified to make the calculations clearer.  It ignores the fact that many plans have a deductible amount, and that plan members pay for services at different rates before and after the deductible has been reached. Often, insured individuals no longer need to pay for services at all once a maximum amount has been reached in a year. The example also assumes that the proportions of number of physician visits per year, estimated from the last 10 years, can be treated as probabilities measured without error. Had a different timespan been chosen, the proportions might well have been different.  

% It also relies on the assumption that health care costs for the two partners are independent.  Two individuals living together may pass on infectious diseases like the flu, or may participate together in activities that lead to similar injuries, such as skiing or long distance running.  Section~\ref{correlatedRVs} shows how to adjust a variance calculation when independence is unrealistic.


% \index{random variable|)}


% __________
% CAMBIO: Section removed 
% \section{Binomial distribution}
% \label{binomialModel}

% The hypothetical clinical study and coin tossing example discussed earlier in this chapter are both examples of experiments that can be modeled with a binomial distribution. The binomial distribution is a more general case of another named distribution, the Bernoulli distribution.

% \index{distribution!binomial|(}

% \subsection{Bernoulli distribution}
% \label{bernoulli}

% \index{distribution!Bernoulli|(}

% Psychologist Stanley Milgram\index{Milgram, Stanley} began a series of experiments in 1963 to study the effect of authority on obedience. In a typical experiment, a participant would be ordered by an authority figure to give a series of increasingly severe shocks to a stranger. Milgram found that only about 35\% of people would resist the authority and stop giving shocks before the maximum voltage was reached. Over the years, additional research suggested this number is approximately consistent across communities and time.\footnote{Find further information on Milgram's experiment at \par \ \ \hspace{0.2mm}\ \oiRedirect{textbook-milgram}{www.cnr.berkeley.edu/ucce50/ag-labor/7article/article35.htm}.}

% Each person in Milgram's experiment can be thought of as a \term{trial}. Suppose that a trial is labeled a \term{success} if the person refuses to administer the worst shock. If the person does administer the worst shock, the trial is a \term{failure}. The \term{probability of a success} can be written as $p=0.35$. The probability of a failure is sometimes denoted with $q=1-p$.

% When an individual trial only has two possible outcomes, it is called a \termsub{Bernoulli random variable}{distribution!Bernoulli}. It is arbitrary as to which outcome is labeled success. 

% Bernoulli random variables are often denoted as \resp{1} for a success and \resp{0} for a failure. Suppose that ten trials are observed, of which 6 are successes and 4 are failures:
% \begin{center}
% 	\resp{0} \resp{1} \resp{1} \resp{1} \resp{1} \resp{0} \resp{1} \resp{1} \resp{0} \resp{0}.
% \end{center}
% The \term{sample proportion}, $\hat{p}$, is the sample mean of these observations:
% \begin{eqnarray*}
% 	\hat{p} = \frac{\text{\# of successes}}{\text{\# of trials}} = \frac{0+1+1+1+1+0+1+1+0+0}{10} = 0.6.
% \end{eqnarray*}%
% Since \resp{0} and \resp{1} are numerical outcomes, the {mean} and {standard deviation} of a Bernoulli random variable can be defined. If ${p}$ is the true probability of a success, then the mean of a Bernoulli random variable $X$ is given by
% \begin{align*}
% \mu = E[X] &= P(X=0)\times0 + P(X=1)\times1 \\
% &= (1-p)\times0 + p\times 1 = 0+p = p.
% \end{align*}
% Similarly, the variance of $X$ can be computed:
% \begin{align*}
% \sigma^2 &= {P(X=0)(0-p)^2 + P(X=1)(1-p)^2} \\
% &= {(1-p)p^2 + p(1-p)^2} = {p(1-p).}
% \end{align*}
% The standard deviation is $\sigma=\sqrt{p(1-p)}$.

% \textD{\newpage}

% \begin{onebox}{Bernoulli random variable}
% If $X$ is a random variable that takes value 1 with probability of success $p$ and 0 with probability $1-p$, then $X$ is a Bernoulli random variable with mean $p$ and standard deviation $\sqrt{p(1-p)}$.
% \end{onebox}

% Suppose $X$ represents the outcome of a single toss of a fair coin, where heads is labeled success. $X$ is a Bernoulli random variable with probability of success $p = 0.50$; this can be expressed as $X \sim \textrm{Bern}(p)$, or specifically, $X \sim \textrm{Bern}(0.50)$. It is essential to specify the probability of success when characterizing a Bernoulli random variable. For example, although the outcome of a single toss of an unfair coin can also be represented by a Bernoulli, it will have a different probability distribution since $p$ does not equal 0.50 for an unfair coin. 

% The success probability $p$ is the \term{parameter} of the distribution, and identifies a specific Bernoulli distribution out of the entire family of Bernoulli distributions where $p$ can be any value between 0 and 1 (inclusive). \marginpar[\raggedright\vspace{-12mm}

% $\textrm{Bern}(p)$\vspace{1mm}\\\footnotesize Bernoulli dist.\\with $p$ prob. of success]{\raggedright\vspace{-12mm}
	
% 	$\textrm{Bern}(p)$\vspace{1mm}\\\footnotesize Bernoulli dist.\\with $p$ prob. of success} 

% \index{distribution!Bernoulli|)}

% \begin{examplewrap}
% \begin{nexample}{Suppose that four individuals are randomly selected to participate in Milgram's experiment. What is the chance that there will be exactly one successful trial, assuming independence between trials? Suppose that the probability of success remains 0.35.}\label{oneRefuser}
	
% 	Consider a scenario in which there is one success (i.e., one person refuses to give the strongest shock). Label the individuals as $A$, $B$, $C$, and $D$:
	
% 	\begin{align*}
% 	&P(A=\text{\resp{refuse}},\text{ }B=\text{\resp{shock}},\text{ }C=\text{\resp{shock}},\text{ }D=\text{\resp{shock}}) \\
% 	&\quad =  P(A=\text{\resp{refuse}})\ P(B=\text{\resp{shock}})\ P(C=\text{\resp{shock}})\ P(D=\text{\resp{shock}}) \\
% 	&\quad =  (0.35)  (0.65)  (0.65)  (0.65) = (0.35)^1 (0.65)^3 = 0.096.
% 	\end{align*}
	
% 	However, there are three other possible scenarios: either $B$, $C$, or $D$ could have been the one to refuse. In each of these cases, the probability is also $(0.35)^1(0.65)^3$. These four scenarios exhaust all the possible ways that exactly one of these four people could refuse to administer the most severe shock, so the total probability of one success is $(4)(0.35)^1(0.65)^3 = 0.38$.
% \end{nexample}
% \end{examplewrap}


% \textD{\newpage}


% \subsection{The binomial distribution}

% The Bernoulli distribution is unrealistic in all but the simplest of settings. However, it is a useful building block for other distributions. The \termsub{binomial distribution}{distribution!binomial} describes the probability of having exactly $k$ successes in $n$ independent Bernoulli trials with probability of a success $p$. In Example~\ref{oneRefuser}, the goal was to calculate the probability of 1 success out of 4 trials, with probability of success 0.35 ($n=4$, $k=1$, $p=0.35$). 

% Like the Bernoulli distribution, the binomial is a discrete distribution, and can take on only a finite number of values. A binomial variable has values 0, 1, 2, \dots, $n$.

% A general formula for the binomial distribution can be developed from re-examining Example~\ref{oneRefuser}. There were four individuals who could have been the one to refuse, and each of these four scenarios had the same probability. Thus, the final probability can be written as:
% \begin{eqnarray}
% [\text{\# of scenarios}] \times P(\text{single scenario}.)
% \label{genBinomialFormula}
% \end{eqnarray}
% The first component of this equation is the number of ways to arrange the $k=1$ successes among the $n=4$ trials. The second component is the probability of any of the four (equally probable) scenarios.

% Consider $P($single scenario$)$ under the general case of $k$ successes and $n-k$ failures in the $n$ trials. In any such scenario, the Multiplication Rule for independent events can be applied:
% \begin{eqnarray*}
% 	p^k(1-p)^{n-k}.
% \end{eqnarray*}

% Secondly, there is a general formula for the number of ways to choose $k$ successes in $n$ trials, i.e. arrange $k$ successes and $n-k$ failures:
% \begin{eqnarray*}
% 	{n\choose k} = \frac{n!}{k!(n-k)!}.
% \end{eqnarray*}
% The quantity ${n\choose k}$ is read \term{n choose k}.\footnote{Other notation for $n$ choose $k$ includes $_nC_k$, $C_n^k$, and $C(n,k)$.} The exclamation point notation (e.g. $k!$) denotes a \term{factorial}\label{factorialDefinitionInTheBinomialSection} expression.\footnote{$0! = 1$ \label{zeroFactorial}, $1! = 1$, $2! = 2 \times 1 = 2$, $\dots$, $n! = n\times (n-1) \times \dots 2 \times 1$.}

% Using the formula, the number of ways to choose $k=1$ successes in $n=4$ trials can be computed as:
% \begin{eqnarray*}
% 	{4 \choose 1} = \frac{4!}{1!(4-1)!} =  \frac{4!}{1!3!} 
% 	= \frac{4\times3\times2\times1}{(1)(3\times2\times1)} = 4.
% \end{eqnarray*}

% Substituting $n$ choose $k$ for the number of scenarios and $p^k(1-p)^{n-k}$ for the single scenario probability in Equation~(\ref{genBinomialFormula}) yields the general binomial formula. 

% \textD{\newpage}

% \begin{onebox}{Binomial distribution}
% Suppose the probability of a single trial being a success is $p$. The probability of observing exactly $k$ successes in $n$ independent trials is given by\vspace{-1mm}
% \begin{eqnarray}
% P(X = k) = {n\choose k}p^k(1-p)^{n-k} = \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}.
% \label{binomialFormula}
% \end{eqnarray}
% Additionally, the mean, variance, and standard deviation of the number of observed successes are, respectively\vspace{-2mm}
% \begin{align}
% \mu &= np
%   &\sigma^2 &= np(1-p)
%   &\sigma &= \sqrt{np(1-p)}.
% \label{binomialStats}
% \end{align}
% A binomial random variable $X$ can be expressed as $X \sim \textrm{Bin}(n, p)$.
% \end{onebox}%
% \marginpar[\raggedright\vspace{-45mm}

% $\textrm{Bin}(n, p)$\vspace{1mm}\\\footnotesize Binomial dist.\\with $n$ trials\\\& $p$ prob. of success]{\raggedright\vspace{-45mm}
	
% 	$\textrm{Bin}(n, p)$\vspace{1mm}\\\footnotesize Binomial dist.\\with $n$ trials\\\& $p$ prob. of success} 

% \begin{onebox}{Is it binomial? Four conditions to check.}
% \label{isItBinomialTipBox}%
% (1) The trials are independent. \\
% (2) The number of trials, $n$, is fixed. \\
% (3) Each trial outcome can be classified as a \emph{success} or \emph{failure}. \\
% (4) The probability of a success, $p$, is the same for each trial.
% \end{onebox}

% \begin{examplewrap}
% \begin{nexample}{What is the probability that 3 of 8 randomly selected participants will refuse to administer the worst shock?}
	
% 	First, check the conditions for applying the binomial model. The number of trials is fixed ($n=8$) and each trial outcome can be classified as either success or failure. The sample is random, so the trials are independent, and the probability of success is the same for each trial. 
	
% 	For the outcome of interest, $k=3$ successes occur in $n=8$ trials, and the probability of a success is $p=0.35$. Thus, the probability that 3 of 8 will refuse is given by
% 	\begin{eqnarray*}
% 		P(X =3) = { 8 \choose 3}(0.35)^3(1-0.35)^{8-3}
% 		&=& \frac{8!}{3!(8-3)!}(0.35)^3(1-0.35)^{8-3} \\
% 		&=& (56)(0.35)^3(0.65)^5 \\
% 		&=& 0.28.
% 	\end{eqnarray*}
% \end{nexample}
% \end{examplewrap}

% \begin{examplewrap}
% \begin{nexample}{What is the probability that at most 3 of 8 randomly selected participants will refuse to administer the worst shock?}
	
% 	The event of at most 3 out of 8 successes can be thought of as the combined probability of 0, 1, 2, and 3 successes. Thus, the probability that at most 3 of 8 will refuse is given by:
% 	\begin{align*}
% 	P(X \leq 3) &= P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) \\
% 	&= { 8 \choose 0}(0.35)^0(1-0.35)^{8-0} + { 8 \choose 1}(0.35)^1(1-0.35)^{8-1} \\
% 	& \qquad + { 8 \choose 2}(0.35)^2(1-0.35)^{8-2} + { 8 \choose 3}(0.35)^3(1-0.35)^{8-3} \\
% 	&= (1)(0.35)^0(1-0.35)^{8} + (8)(0.35)^1(1-0.35)^{7} \\
% 	& \qquad + (28)(0.35)^2(1-0.35)^{6} + (56)(0.35)^3(1-0.35)^{5}\\
% 	&= 0.706.
% 	\end{align*}
% \end{nexample}
% \end{examplewrap}

% \begin{examplewrap}
% \begin{nexample}{If 40 individuals were randomly selected to participate in the experiment, how many individuals would be expected to refuse to administer the worst shock? What is the standard deviation of the number of people expected to refuse?}
	
% 	Both quantities can directly be computed from the formulas in Equation~(\ref{binomialStats}). The expected value (mean) is given by: $\mu=np = 40\times 0.35 = 14$. The standard deviation is: $\sigma = \sqrt{np(1-p)} = \sqrt{40\times 0.35\times 0.65} = 3.02$.
% \end{nexample}
% \end{examplewrap}

% \begin{exercisewrap}
% \begin{nexercise}
% The probability that a smoker will develop a severe lung condition in their lifetime is about 0.30. Suppose that 5 smokers are randomly selected from the population. What is the probability that (a) one will develop a severe lung condition? (b) that no more than one will develop a severe lung condition? (c) that at least one will develop a severe lung condition?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{Let $p = 0.30$; $X \sim \textrm{Bin}(5, 0.30)$. (a) $P(X=1) = {5 \choose 1}(0.30)^1(1-0.30)^{5-1} = 0.36$ (b) $P(X \leq 1) = P(X=0) + P(X=1) = {5 \choose 0}(0.30)^0(1-0.30)^{5-0} + 0.36 = 0.53$ (c) $P(X \geq 1) = 1 - P(X=0) = 1 - 0.36 = 0.83$}

% \index{distribution!binomial|)}


%_________________
\section{Normal distribution}
\label{normalDist}

\index{distribution!normal|(}

Among the many distributions seen in practice, one is by far the most common: the \termsub{normal distribution}{distribution!normal}, which has the shape of a symmetric, unimodal bell curve. Many variables are nearly normal, which makes the normal distribution useful for a variety of problems. For example, characteristics such as human height closely follow the normal distribution.


\subsection{Normal distribution model}

The normal distribution model always describes a symmetric, unimodal, bell-shaped curve. However, the curves can differ in center and spread; the model can be adjusted using mean and standard deviation. Changing the mean shifts the bell curve to the left or the right, while changing the standard deviation stretches or constricts the curve. Figure~\ref{twoSampleNormals} shows the normal distribution with mean $0$ and standard deviation $1$ in the left panel and the normal distribution with mean $19$ and standard deviation $4$ in the right panel. Figure~\ref{twoSampleNormalsStacked} shows these distributions on the same axis.

\begin{figure}[hht]
\centering
\includegraphics[width=0.85\textwidth]{ch_02a_probability_oi_biostat/figures/twoSampleNormals/twoSampleNormals}
\caption{Both curves represent the normal distribution; however, they differ in their center and spread. The normal distribution with mean 0 and standard deviation 1 is called the \term{standard normal distribution}.}
\label{twoSampleNormals}
\end{figure}

\begin{figure}[hht]
\centering
\includegraphics[width=0.6\textwidth]{ch_02a_probability_oi_biostat/figures/twoSampleNormalsStacked/twoSampleNormalsStacked}
\caption{The normal models shown in Figure~\ref{twoSampleNormals} but plotted together and on the same scale.}
\label{twoSampleNormalsStacked}
\end{figure}

For any given normal distribution with mean $\mu$ and standard deviation $\sigma$, the distribution can be written as $N(\mu, \sigma)$; $\mu$ and $\sigma$ are the parameters of the normal distribution. \marginpar[\raggedright\vspace{-5mm}

$N(\mu, \sigma)$\vspace{1mm}\\\footnotesize Normal dist.\\with mean $\mu$\\\& st. dev. $\sigma$]{\raggedright\vspace{-5mm}

$N(\mu, \sigma)$\vspace{1mm}\\\footnotesize Normal dist.\\with mean $\mu$\\\& st. dev. $\sigma$} For example, $N(0, 1)$ refers to the standard normal distribution, as shown in Figure~\ref{twoSampleNormals}. 

%CAMBIO: Removed sentence 
%Unlike the Bernoulli and binomial distributions, the normal distribution is a continuous distribution. 


%\textD{\newpage}


\subsection{Standardizing with Z-scores}

The \term{Z-score}\marginpar[\raggedright\vspace{-3mm}

$Z$\vspace{1mm}\\\footnotesize Z-score, the\\standardized\\observation]{\raggedright\vspace{-3mm}
	
	$Z$\vspace{1mm}\\\footnotesize Z-score, the\\standardized\\observation}\index{Z@$Z$} of an observation quantifies how far the observation is from the mean, in units of standard deviation(s). If $x$ is an observation from a distribution $N(\mu, \sigma)$, the Z-score is mathematically defined as:
\begin{align*}
	Z = \frac{x-\mu}{\sigma}.
\end{align*}

An observation equal to the mean has a Z-score of 0. Observations above the mean have positive Z-scores, while observations below the mean have negative Z-scores. For example, if an observation is one standard deviation above the mean, it has a Z-score of 1; if it is 1.5 standard deviations below the mean, its Z-score is -1.5. 

Z-scores can be used to identify which observations are more extreme than others, and are especially useful when comparing observations from different normal distributions. One observation $x_1$ is said to be more unusual than another observation $x_2$ if the absolute value of its Z-score is larger than the absolute value of the other observation's Z-score: $|Z_1| > |Z_2|$. In other words, the further an observation is from the mean in either direction, the more extreme it is. 

\begin{examplewrap}
\begin{nexample}{The SAT and the ACT are two standardized tests commonly used for college admissions in the United States. The distribution of test scores are both nearly normal. For the SAT, $N(1500, 300)$; for the ACT, $N(21, 5)$. While some colleges request that students submit scores from both tests, others allow students the choice of either the ACT or the SAT. Suppose that one student scores an 1800 on the SAT (Student A) and another scores a 24 on the ACT (Student B). A college admissions officer would like to compare the scores of the two students to determine which student performed better.}\label{actSAT}
		
Calculate a Z-score for each student; i.e., convert $x$ to Z.
		
Using $\mu_{SAT}=1500$, $\sigma_{SAT}=300$, and $x_{A}=1800$, find Student A's Z-score:
\begin{align*}
	Z_{A} = \frac{x_{A} - \mu_{SAT}}{\sigma_{SAT}} = \frac{1800-1500}{300} = 1.
\end{align*}

For Student B:
\begin{align*}
	Z_{B} = \frac{x_{B} - \mu_{ACT}}{\sigma_{ACT}} = \frac{24 - 21}{5} = 0.6.
\end{align*}

Student A's score is 1 standard deviation above average on the SAT, while Student B's score is 0.6 standard deviations above the mean on the ACT. As illustrated in Figure~\ref{satActNormals}, Student A's score is more extreme, indicating that Student A has scored higher with respect to other scores than Student B.
\end{nexample}
\end{examplewrap}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{ch_02a_probability_oi_biostat/figures/satActNormals/satActNormals}
\caption{Scores of Students A and B plotted on the distributions of SAT and ACT scores.}
\label{satActNormals}
\end{figure}

\begin{onebox}{The Z-score}
The Z-score of an observation quantifies how far the observation is from the mean, in units of standard deviation(s). The Z-score for an observation $x$ that follows a distribution with mean $\mu$ and standard deviation $\sigma$ can be calculated using
\begin{align*}
Z = \frac{x-\mu}{\sigma}.
\end{align*}
\end{onebox}

%\textD{\newpage}

\begin{examplewrap}
\begin{nexample}{How high would a student need to score on the ACT to have a score equivalent to Student A's score of 1800 on the SAT?} 

As shown in Example~\ref{satActNormals}, a score of 1800 on the SAT is 1 standard deviation above the mean. ACT scores are normally distributed with mean 21 and standard deviation 5. To convert a value from the standard normal curve (Z) to one on a normal distribution $N(\mu, \sigma)$:

\begin{align*}
x = \mu + Z\sigma.
\end{align*}

Thus, a student would need a score of $21 + 1(5) = 26$ on the ACT to have a score equivalent to 1800 on the SAT. 
\end{nexample}
\end{examplewrap}

\begin{exercisewrap}
\begin{nexercise}\label{nhanes_bp}%
Systolic blood pressure (SBP) for adults in the United States aged 18-39 follow an approximate normal distribution, $N(115, 17.5)$. As age increases, systolic blood pressure also tends to increase. Mean systolic blood pressure for adults 60 years of age and older is 136 mm Hg, with standard deviation 40 mm Hg. Systolic blood pressure of 140 mm Hg or higher is indicative of hypertension (high blood pressure). 
(a) How many standard deviations away from the mean is a 30-year-old with systolic blood pressure of 125 mm Hg? (b) Compare how unusual a systolic blood pressure of 140 mm Hg is for a 65-year-old, versus a 30-year-old.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a) Calculate the $Z$-score: $\frac{\overline{x} - \mu}{\sigma} = \frac{125 - 115}{17.5} = 0.571$. A 30-year-old with systolic blood pressure of 125 mm Hg is about 0.6 standard deviations above the mean. (b) For $x_1=140$ mm Hg: $Z_1 = \frac{x_1 - \mu}{\sigma} = \frac{140 - 115}{17.5} = 1.43$. For $x_2=140$ mm Hg: $Z_2 = \frac{x_2 - \mu}{\sigma} = \frac{140 - 137}{40} = 0.1$. While an SBP of 140 mm Hg is almost 1.5 standard deviations above the mean for a 30-year-old, it is only 0.1 standard deviations above the mean for a 65-year-old.}

% cite nhanes_bp, SD calculated as SE * sqrt(n) from tables


%\textD{\newpage}


\subsection{The empirical rule}\label{empiricalRule}

The empirical rule (also known as the 68-95-99.7 rule) states that for a normal distribution, almost all observations will fall within three standard deviations of the mean. Specifically, 68\% of observations are within one standard deviation of the mean, 95\% are within two SD's, and 99.7\% are within three SD's. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{ch_02a_probability_oi_biostat/figures/6895997/6895997}
	\caption{Probabilities for falling within 1, 2, and 3 standard deviations of the mean in a normal distribution.}
	\label{6895997}
\end{figure}

While it is possible for a normal random variable to take on values 4, 5, or even more standard deviations from the mean, these occurrences are extremely rare if the data are nearly normal. For example, the probability of being further than 4 standard deviations from the mean is about 1-in-30,000.

\subsection{Calculating normal probabilities}

The normal distribution is a continuous probability distribution. Recall from Section~\ref{randomVariablesSection} that the total area under the density curve is always equal to 1, and the probability that a variable has a value within a specified interval is the area under the curve over that interval. By using either statistical software or normal probability tables, the normal model can be used to identify a probability or percentile based on the corresponding Z-score (and vice versa). 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{ch_02a_probability_oi_biostat/figures/normalTails/normalTails}
	\caption{The area to the left of $Z$ represents the percentile of the observation.}
	\label{normalTails}
\end{figure}

%\textD{\newpage}

A \term{normal probability table} is given in Appendix~\vref{normalProbabilityTable} and abbreviated in Figure~\ref{zTableShort}. This table can be used to identify the \term{percentile} corresponding to any particular Z-score; for instance, the percentile of $Z=0.43$ is shown in row $0.4$ and column $0.03$ in Figure~\ref{zTableShort}: 0.6664, or the $66.64^{th}$ percentile. First, find the proper row in the normal probability table up through the first decimal, and then determine the column representing the second decimal value. The intersection of this row and column is the percentile of the observation. This value also represents the probability that the standard normal variable Z takes on a value of 0.43 or less; i.e. $P(Z \leq 0.43) = 0.6664$.

The table can also be used to find the Z-score associated with a percentile. For example, to identify Z for the $80^{th}$ percentile, look for the value closest to 0.8000 in the middle portion of the table: 0.7995. The Z-score for the $80^{th}$ percentile is given by combining the row and column Z values: 0.84.

\begin{figure}
	\centering
	\begin{tabular}{c | rrrrr | rrrrr |}
		\cline{2-11}
		&&&& \multicolumn{4}{c}{Second decimal place of $Z$} &&& \\
		\cline{2-11}
		$Z$ & 0.00 & 0.01 & 0.02 & \highlightT{0.03} & \highlightO{0.04} & 0.05 & 0.06 & 0.07 & 0.08 & 0.09 \\
		\hline
		\hline
		0.0 & \scriptsize{0.5000} & \scriptsize{0.5040} & \scriptsize{0.5080} & \scriptsize{0.5120} & \scriptsize{0.5160} & \scriptsize{0.5199} & \scriptsize{0.5239} & \scriptsize{0.5279} & \scriptsize{0.5319} & \scriptsize{0.5359} \\
		0.1 & \scriptsize{0.5398} & \scriptsize{0.5438} & \scriptsize{0.5478} & \scriptsize{0.5517} & \scriptsize{0.5557} & \scriptsize{0.5596} & \scriptsize{0.5636} & \scriptsize{0.5675} & \scriptsize{0.5714} & \scriptsize{0.5753} \\
		0.2 & \scriptsize{0.5793} & \scriptsize{0.5832} & \scriptsize{0.5871} & \scriptsize{0.5910} & \scriptsize{0.5948} & \scriptsize{0.5987} & \scriptsize{0.6026} & \scriptsize{0.6064} & \scriptsize{0.6103} & \scriptsize{0.6141} \\
		%  May comment out 0.0-0.2 to make extra space. Then insert the following line:
		%  $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ \\
		0.3 & \scriptsize{0.6179} & \scriptsize{0.6217} & \scriptsize{0.6255} & \scriptsize{0.6293} & \scriptsize{0.6331} & \scriptsize{0.6368} & \scriptsize{0.6406} & \scriptsize{0.6443} & \scriptsize{0.6480} & \scriptsize{0.6517} \\
		\highlightT{0.4} & \scriptsize{0.6554} & \scriptsize{0.6591} & \scriptsize{0.6628} & \highlightT{\scriptsize{0.6664}} & \scriptsize{0.6700} & \scriptsize{0.6736} & \scriptsize{0.6772} & \scriptsize{0.6808} & \scriptsize{0.6844} & \scriptsize{0.6879} \\
		\hline
		0.5 & \scriptsize{0.6915} & \scriptsize{0.6950} & \scriptsize{0.6985} & \scriptsize{0.7019} & \scriptsize{0.7054} & \scriptsize{0.7088} & \scriptsize{0.7123} & \scriptsize{0.7157} & \scriptsize{0.7190} & \scriptsize{0.7224} \\
		0.6 & \scriptsize{0.7257} & \scriptsize{0.7291} & \scriptsize{0.7324} & \scriptsize{0.7357} & \scriptsize{0.7389} & \scriptsize{0.7422} & \scriptsize{0.7454} & \scriptsize{0.7486} & \scriptsize{0.7517} & \scriptsize{0.7549} \\
		0.7 & \scriptsize{0.7580} & \scriptsize{0.7611} & \scriptsize{0.7642} & \scriptsize{0.7673} & \scriptsize{0.7704} & \scriptsize{0.7734} & \scriptsize{0.7764} & \scriptsize{0.7794} & \scriptsize{0.7823} & \scriptsize{0.7852} \\
		\highlightO{0.8} & \scriptsize{0.7881} & \scriptsize{0.7910} & \scriptsize{0.7939} & \scriptsize{0.7967} & \highlightO{\scriptsize{0.7995}} & \scriptsize{0.8023} & \scriptsize{0.8051} & \scriptsize{0.8078} & \scriptsize{0.8106} & \scriptsize{0.8133} \\
		0.9 & \scriptsize{0.8159} & \scriptsize{0.8186} & \scriptsize{0.8212} & \scriptsize{0.8238} & \scriptsize{0.8264} & \scriptsize{0.8289} & \scriptsize{0.8315} & \scriptsize{0.8340} & \scriptsize{0.8365} & \scriptsize{0.8389} \\
		\hline
		\hline
		1.0 & \scriptsize{0.8413} & \scriptsize{0.8438} & \scriptsize{0.8461} & \scriptsize{0.8485} & \scriptsize{0.8508} & \scriptsize{0.8531} & \scriptsize{0.8554} & \scriptsize{0.8577} & \scriptsize{0.8599} & \scriptsize{0.8621} \\
		1.1 & \scriptsize{0.8643} & \scriptsize{0.8665} & \scriptsize{0.8686} & \scriptsize{0.8708} & \scriptsize{0.8729} & \scriptsize{0.8749} & \scriptsize{0.8770} & \scriptsize{0.8790} & \scriptsize{0.8810} & \scriptsize{0.8830} \\
		$\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ &   $\vdots$ \\
		\hline
	\end{tabular}
	\caption{A section of the normal probability table. The percentile for a normal random variable with $Z=0.43$ has been \highlightT{highlighted}, and the percentile closest to 0.8000 has also been \highlightO{highlighted}.}
	\label{zTableShort}
\end{figure}

\begin{examplewrap}
\begin{nexample}{Student A from Example~\ref{actSAT} earned a score of 1800 on the SAT, which corresponds to $Z=1$. What percentile is this score associated with?}
In this context, the \term{percentile} is the percentage of people who earned a lower SAT score than Student A. From the normal table, $Z$ of 1.00 is 0.8413. Thus, the student is in the $84^{th}$ percentile of test takers. This area is shaded in Figure~\ref{satBelow1800}.
\end{nexample}
\end{examplewrap}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{ch_02a_probability_oi_biostat/figures/satBelow1800/satBelow1800}
	\caption{The normal model for SAT scores, with shaded area representing scores below 1800.}
	\label{satBelow1800}
\end{figure}
		
\begin{exercisewrap}
\begin{nexercise}
Determine the proportion of SAT test takers who scored better than Student A on the SAT.\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{If 84\% had lower scores than Student A, the number of people who had better scores must be 16\%.}


%\textD{\newpage}


\subsection{Normal probability examples}

There are two main types of problems that involve the normal distribution: calculating probabilities from a given value (whether $X$ or $Z$), or identifying the observation that corresponds to a particular probability. 

\begin{examplewrap}
\begin{nexample}{Cumulative SAT scores are well-approximated by a normal model, $N(1500, 300)$. What is the probability that a randomly selected test taker scores at least 1630 on the SAT?}\label{satAbove1630Exam}%	
For any normal probability problem, it can be helpful to start out by drawing the normal curve and shading the area of interest.

\begin{center}
\includegraphics[width=0.45\textwidth]{ch_02a_probability_oi_biostat/figures/satAbove1630/satAbove1630}
\end{center}
To find the shaded area under the curve, convert 1630 to a Z-score:
\begin{align*}
Z = \frac{x - \mu}{\sigma} = \frac{1630 - 1500}{300} = \frac{130}{300} = 0.43.
\end{align*}
Look up the percentile of $Z=0.43$ in the normal probability table shown in Figure~\ref{zTableShort} or in Appendix~\vref{normalProbabilityTable}: 0.6664. However, note that the percentile describes those who had a Z-score \emph{lower} than 0.43, or in other words, the area below 0.43. To find the area \emph{above} $Z=0.43$, subtract the area of the lower tail from the total area under the curve, 1:
\begin{center}
\includegraphics[width=0.5\textwidth]{ch_02a_probability_oi_biostat/figures/subtractingArea/subtractingArea}
\end{center}
The probability that a student scores at least 1630 on the SAT is 0.3336.
\end{nexample}
\end{examplewrap}

\begin{onebox}{Discrete versus continuous probabilities}
\label{discreteVsContDistr}%
Recall that the probability of a continuous random variable equaling some exact value is always 0. As a result, for a continuous random variable $X$, $P(X \leq x) = P(X < x)$ and $P(X \geq x) = P(X > x)$. It is valid to state that $P(X \geq x) = 1 - P(X \leq x) = 1 - P(X < x)$.\\
		
This is \textit{not} the case for discrete random variables. For example, for a discrete random variable $Y$, $P(Y \geq 2) = 1 - P(Y < 2) = 1 - P(Y \leq 1)$. It would be incorrect to claim that $P(Y \geq 2) = 1 - P(Y \leq 2)$.
\end{onebox}

\begin{exercisewrap}
\begin{nexercise}
What is the probability of a student scoring at most 1630 on the SAT?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{This probability was calculated as part of Example~\ref{satAbove1630Exam}: 0.6664. A picture for this exercise is represented by the shaded area below ``0.6664'' in Example~\ref{satAbove1630Exam}.}

\begin{exercisewrap}
\begin{nexercise}
Systolic blood pressure for adults 60 years of age and older in the United States is approximately normally distributed: $N(136, 40)$. What is the probability of an adult in this age group having systolic blood pressure of 140 mm Hg or greater?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{The Z-score for this observation was calculated in Exercise~\ref{nhanes_bp} as 0.1. From the table, this corresponds to 0.54.}

%%\textD{\newpage}

\begin{examplewrap}
\begin{nexample}{The height of adult males in the United States between the ages of 20 and 62 is nearly normal, with mean 70 inches and standard deviation 3.3 inches.\footnotemark{} What is the probability that a random adult male is between 5'9'' and 6'2''?}
	These heights correspond to 69 inches and 74 inches. First, draw the figure. The area of interest is an interval, rather than a tail area.
	\begin{center}
		\includegraphics[width=0.43\textwidth]{ch_02a_probability_oi_biostat/figures/between59And62/between59And62}
	\end{center}
	To find the middle area, find the area to the left of 74; from that area, subtract the area to the left of 69.
	
	First, convert to Z-scores:
	
	\begin{align*}
	Z_{74} = \dfrac{x-\mu}{\sigma} = \dfrac{74-70}{3.3} = 1.21, \qquad Z_{62} = \dfrac{x-\mu}{\sigma} = \dfrac{69-70}{3.3} = -0.30.
	\end{align*}
	
	From the normal probability table, the areas are respectively, $0.8868$ and $0.3821$. The middle area is $0.8868 - 0.3821 = 0.5048$. The probability of being between heights 5'9'' and 6'2'' is 0.5048.
	\begin{center}
		\includegraphics[width=0.65\textwidth]{ch_02a_probability_oi_biostat/figures/subtracting2Areas/subtracting2Areas}
	\end{center}
\end{nexample}
\end{examplewrap}
\footnotetext{As based on a sample of 100 men, from the USDA Food Commodity Intake Database.}

\begin{exercisewrap}
\begin{nexercise}
What percentage of adults in the United States ages 60 and older have blood pressure between 145 and 130 mm Hg?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{First calculate Z-scores, then find the percent below 145 mm Hg and below 130 mm Hg: $Z_{145} = 0.23 \to 0.5890$, $Z_{130} = -0.15 \to 0.4404$ (area above). Final answer: $0.5890 - 0.4404 = 0.1486$.}

%\textD{\newpage}

\begin{examplewrap}
\begin{nexample}{How tall is a man with height in the 40$^{th}$ percentile?}\label{normalExam40Perc}
First, draw a picture. The lower tail probability is 0.40, so the shaded area must start before the mean. \vspace{-1mm}
\begin{center}
\includegraphics[width=0.35\textwidth]{ch_02a_probability_oi_biostat/figures/height40Perc/height40Perc}\vspace{-1mm}
\end{center}

Determine the Z-score associated with the $40^{th}$ percentile. Because the percentile is below 50\%, $Z$ will be negative. Look for the probability inside the negative part of table that is closest to 0.40: 0.40 falls in row $-0.2$ and between columns $0.05$ and $0.06$. Since it falls closer to $0.05$, choose $Z=-0.25$.

Convert the Z-score to $X$, where $X \sim N(70, 3.3)$. 
\begin{align*}
X = \mu + \sigma Z = 70 + (-0.25)(3.3) = 69.18.
\end{align*}
A man with height in the 40$^{th}$ percentile is 69.18 inches tall, or about 5' 9''. 
\end{nexample}
\end{examplewrap}

\begin{exercisewrap}
\begin{nexercise}
(a) What is the $95^{th}$ percentile for SAT scores? (b) What is the $97.5^{th}$ percentile of the male heights?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{(a) Look for 0.95 in the probability portion (middle part) of the normal probability table: row 1.6 and (about) column 0.05, i.e. $Z_{95}=1.65$. Knowing $Z_{95}=1.65$, $\mu = 1500$, and $\sigma = 300$, convert Z to $x$: $1500 + (1.65)(300) = 1995$. (b) Similarly, find $Z_{97.5} = 1.96$, and convert to $x$: $x_{97.5} = 76.5$ inches.}


% CAMBIO: removed subsection
% \textD{\newpage}


% \subsection{Normal approximation to the binomial distribution}
	
% \index{distribution!binomial!normal approximation|(}

% The normal distribution can be used to approximate other distributions, such as the binomial distribution. The binomial formula is cumbersome when sample size is large, particularly when calculating probabilities for a large number of observations. Under certain conditions, the normal distribution can be used to approximate binomial probabilities. This method was widely used when calculating binomial probabilities by hand was the only option. Nowadays, modern statistical software is capable of calculating exact binomial probabilities even for very large $n$. The normal approximation to the binomial is discussed here since it is an important result that will be revisited in later chapters.

% Consider the binomial model when probability of success is $p=0.10$. Figure~\ref{fourBinomialModelsShowingApproxToNormal} shows four hollow histograms for simulated samples from the binomial distribution using four different sample sizes: $n=10, 30, 100, 300$. As the sample size increases from $n=10$ to $n=300$, the distribution is transformed from a blocky and skewed distribution into one resembling the normal curve.
	
% \begin{figure}[h]
% 		\centering
% 		\includegraphics[width=0.83\textwidth]{ch_distributions_oi_biostat/figures/fourBinomialModelsShowingApproxToNormal/fourBinomialModelsShowingApproxToNormal}
% 		\caption{Hollow histograms of samples from the binomial model when $p=0.10$. The sample sizes for the four plots are $n=10$, 30, 100, and 300, respectively.}
% 		\label{fourBinomialModelsShowingApproxToNormal}
% \end{figure}
	
% \begin{onebox}{Normal approximation of the binomial distribution}
% The binomial distribution with probability of success $p$ is nearly normal when the sample size $n$ is sufficiently large such that $np$ and $n(1-p)$ are both at least 10. The approximate normal distribution has parameters corresponding to the mean and standard deviation of the binomial distribution:\vspace{-1.5mm}
% \begin{align*}
% \mu &= np
% &&\sigma= \sqrt{np(1-p)}
% \end{align*}
% \end{onebox}

% \begin{examplewrap}
% \begin{nexample}{Approximately 20\% of the US population smokes cigarettes. A local government commissioned a survey of 400 randomly selected individuals to investigate whether their community might have a lower smoker rate than 20\%. The survey found that 59 of the 400 participants smoke cigarettes. If the true proportion of smokers in the community is 20\%, what is the probability of observing 59 or fewer smokers in a sample of 400 people?}\label{approxBinomialForN400P20SmokerExample}
		
% The desired probability is equivalent to the sum of the individual probabilities of observing $k=0$, 1, ..., 58, or 59 smokers in a sample of $n=400$: $P(X \leq 59)$. Confirm that the normal approximation is valid: $np=400\times 0.20=80$, $n(1-p)=400\times 0.8=320$. To use the normal approximation, calculate the mean and standard deviation from the binomial model:
% \begin{align*}
% 		\mu &= np = 80
% 		&\sigma &= \sqrt{np(1-p)} = 8.
% \end{align*}
		
% Convert 59 to a Z-score: $Z = \dfrac{59-80}{8} = -2.63$. Use the normal probability table to identify the left tail area, which is 0.0043. 
		
% This estimate is very close to the answer derived from the exact binomial calculation:
% \[P(k=0\text{ or }k=1\text{ or }\cdots\text{ or } k=59) = P(k=0) + P(k=1) + \cdots + P(k=59) = 0.0041. \]
% \end{nexample}
% \end{examplewrap}
	
% However, even when the conditions for using the approximation are met, the normal approximation to the binomial tends to perform poorly when estimating the probability of a small range of counts. Suppose the normal approximation is used to compute the probability of observing 69, 70, or 71 smokers in 400 when $p=0.20$. In this setting, the exact binomial and normal approximation result in notably different answers: the approximation gives 0.0476, while the binomial returns 0.0703.
	
% The cause of this discrepancy is illustrated in Figure~\ref{normApproxToBinomFail}, which shows the areas representing the binomial probability (outlined) and normal approximation (shaded). Notice that the width of the area under the normal distribution is 0.5 units too slim on both sides of the interval.
	
% \begin{figure}[h]
% 		\centering
% 		\includegraphics[width=0.77\textwidth]{ch_distributions_oi_biostat/figures/normApproxToBinomFail/normApproxToBinomFail}
% 		\caption{A normal curve with the area between 69 and 71 shaded. The outlined area represents the exact binomial probability.}
% 		\label{normApproxToBinomFail}
% \end{figure}
	
% The normal approximation can be improved if the cutoff values for the range of observations is modified slightly: the lower value should be reduced by 0.5 and the upper value increased by 0.5. The normal approximation with continuity correction gives 0.0687 for the probability of observing 69, 70, or 71 smokers in 400 when $p = 0.20$, which is closer to the exact binomial result of 0.0703. 

% This adjustment method is known as a continuity correction, which allows for increased accuracy when a continuous distribution is used to approximate a discrete one. The modification is typically not necessary when computing a tail area, since the total interval in that case tends to be quite wide.
	
% \index{distribution!binomial!normal approximation|)}


 %\subsection{Evaluating the normal approximation}
 %\label{assessingNormal}

% The normal model can also be used to approximate data distributions. While using a normal model can be convenient, it is important to remember that normality is always an approximation. Testing the appropriateness of the normal assumption is a key step in many data analyses.

 %\index{normal probability plot|(}

 %\begin{figure}[h]
 %	\centering
 %	\includegraphics[width=0.8\textwidth]{ch_02_probability_oi_biostat/figures/fcidMHeights/fcidMHeights}
 %	\caption{A sample of 100 male heights. Since the observations are rounded to the nearest whole inch, the points in the normal probability plot appear to jump in increments.}
 %	\label{fcidMHeights}
 %\end{figure}

 %Example~\ref{normalExam40Perc} suggests the distribution of heights of US males is well approximated by the normal model. There are two visual methods used to assess the assumption of normality. The first is a simple histogram with the best fitting normal curve overlaid on the plot, as shown in the left panel of Figure~\ref{fcidMHeights}. The sample mean $\bar{x}$ and standard deviation $s$ are used as the parameters of the best fitting normal curve. The closer this curve fits the histogram, the more reasonable the normal model assumption. More commonly, a \term{normal probability plot} is used, such as the one shown in the right panel of Figure~\ref{fcidMHeights}.\footnote{Also called a \term{quantile-quantile plot}, or Q-Q plot.} If the points fall on or near the line, the data closely follow the normal model.

 %\textD{\newpage}

 %\begin{examplewrap}
 %\begin{nexample}{Three datasets were simulated from a normal distribution, with sample sizes $n = 40$, $n = 100$, and $n = 400$; the histograms and normal probability plots of the datasets are shown in Figure~\ref{normalExamples}. What happens as sample size increases?}\label{normalExamplesExample}%
 %As sample size increases, the data more closely follows the normal distribution; the histograms become more smooth, and the points on the Q-Q plots show fewer deviations from the line.

 %It is important to remember that when evaluating normality in a small dataset, apparent deviations from normality may simply be due to small sample size. Remember that all three of these simulated datasets are drawn from a normal distribution.

 %When assessing the normal approximation in real data, it will be rare to observe a Q-Q plot as clean as the one shown for $n = 400$. Typically, the normal approximation is reasonable even if there are some small observed departures from normality in the tails, such as in the plot for $n = 100$.
 %\end{nexample}
 %\end{examplewrap}

 %\begin{figure}[h]
 %\centering
 %\includegraphics[width=\textwidth]{ch_distributions_oi_biostat/figures/normalExamples/normalExamples}
 %\caption{Histograms and normal probability plots for three simulated normal data sets; $n=40$ (left), $n=100$ (middle), $n=400$ (right).}
 %\label{normalExamples}
 %\end{figure}

% \textD{\newpage}

% \begin{examplewrap}
% \begin{nexample}{Would it be reasonable to use the normal distribution to accurately calculate percentiles of heights of NBA players? Consider all 435 NBA players from the 2008-9 season presented in Figure~\ref{nbaNormal}.\footnotemark{}}
% The histogram in the left panel is slightly left skewed, and the points in the normal probability plot do not closely follow a straight line, particularly in the upper quantiles. The normal model is not an accurate approximation of NBA player heights.
% \end{nexample}
% \end{examplewrap}
% \footnotetext{These data were collected from \oiRedirect{textbook-nba_com}{www.nba.com}.}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.80\textwidth]{ch_distributions_oi_biostat/figures/nbaNormal/nbaNormal}
% 	\caption{Histogram and normal probability plot for the NBA heights from the 2008-9 season.}
% 	\label{nbaNormal}
% \end{figure}		
		
% \begin{examplewrap}
% \begin{nexample}{Consider the poker winnings of an individual over 50 days. A histogram and normal probability plot of these data are shown in Figure~\ref{pokerNormal} Evaluate whether a normal approximation is appropriate.}
% The data are very strongly right skewed\index{skew!example: very strong} in the histogram, which corresponds to the very strong deviations on the upper right component of the normal probability plot. These data show very strong deviations from the normal model; the normal approximation should not be applied to these data.
% \end{nexample}
% \end{examplewrap}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.85\textwidth]{ch_distributions_oi_biostat/figures/pokerNormal/pokerNormal}
% 	\caption{A histogram of poker data with the best fitting normal plot and a normal probability plot.}
% 	\label{pokerNormal}
% \end{figure}	

% \begin{exercisewrap}
% \begin{nexercise}\label{normalQuantileExercise}%
% Determine which data sets represented in Figure~\ref{normalQuantileExer} plausibly come from a nearly normal distribution.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{Answers may vary. The top-left plot shows some deviations in the smallest values in the dataset; specifically, the left tail shows some large outliers. The top-right and bottom-left plots do not show any obvious or extreme deviations from the lines for their respective sample sizes, so a normal model would be reasonable. The bottom-right plot has a consistent curvature that suggests it is not from the normal distribution. From examining the vertical coordinates of the observations, most of the data are between -20 and 0, then there are about five observations scattered between 0 and 70; this distribution has strong right skew.}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.80\textwidth]{ch_distributions_oi_biostat/figures/normalQuantileExer/normalQuantileExer}
% 	\caption{Four normal probability plots for Guided Practice~\ref{normalQuantileExercise}.}
% 	\label{normalQuantileExer}
% \end{figure}

% \textD{\newpage}

% When observations spike downwards on the left side of a normal probability plot, this indicates that the data have more outliers in the left tail expected under a normal distribution. When observations spike upwards on the right side, the data have more outliers in the right tail than expected under the normal distribution.

% \begin{exercisewrap}
% \begin{nexercise}\label{normalQuantileExerciseAdditional}%
% Figure~\ref{normalQuantileExerAdditional} shows normal probability plots for two distributions that are skewed. One distribution is skewed to the low end (left skewed) and the other to the high end (right skewed). Which is which?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{Examine where the points fall along the vertical axis. In the first plot, most points are near the low end with fewer observations scattered along the high end; this describes a distribution that is skewed to the high end. The second plot shows the opposite features, and this distribution is skewed to the low end.}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.80\textwidth]{ch_distributions_oi_biostat/figures/normalQuantileExer/normalQuantileExerAdditional}
% \caption{Normal probability plots for Guided Practice~\ref{normalQuantileExerciseAdditional}.}
% \label{normalQuantileExerAdditional}
% \end{figure}


% \index{normal probability plot|)}
% \index{distribution!normal|)}


% CAMBIO: Section removed


%_________________
% \section{Poisson distribution}
% \label{poisson}

% \index{distribution!Poisson|(}

% The \termsub{Poisson distribution}{distribution!Poisson} is a discrete distribution used to calculate probabilities for the number of occurrences of a rare event.  In technical terms, it is used as a model for count data. For example, historical records of hospitalizations in New York City indicate that among a population of approximately 8 million people, 4.4 people are hospitalized each day for an acute myocardial infarction (AMI), on average.  A histogram of showing the distribution of the number of AMIs per day on 365 days for NYC is shown in Figure~\ref{amiIncidencesOver100Days}.\footnote{These data are simulated. In practice, it would be important to check for an association between successive days.}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.7\textwidth]{ch_distributions_oi_biostat/figures/amiIncidencesOver100Days/amiIncidencesOver100Days}
% 	\caption{A histogram of the number of people hospitalized for an AMI on 365 days for NYC, as simulated from a Poisson distribution with mean 4.4.}
% 	\label{amiIncidencesOver100Days}
% \end{figure}


% \begin{onebox}{Poisson distribution}
% The Poisson distribution is a probability model for the number of events that occur in a population.  The probability that exactly $k$ events occur is given by 
% \begin{align*}
% P(X = k) = \frac{e^{-\lambda}(\lambda)^{k}}{k!},
% \end{align*}
% where $k$ may take a value 0, 1, 2, \dots The mean and standard deviation of this distribution are $\lambda$ and $\sqrt{\lambda}$, respectively.
% A Poisson random variable $X$ can be expressed as $X \sim \textrm{Pois}(\lambda)$.
% \end{onebox}%
% \marginpar[\raggedright\vspace{-35mm}

% $\textrm{Pois}(\lambda)$\vspace{1mm}\\\footnotesize Poisson dist.\\with rate $\lambda$]{\raggedright\vspace{-35mm}
	
% 	$\textrm{Pois}(\lambda)$\vspace{1mm}\\\footnotesize Poisson dist.\\with rate $\lambda$} 

% When events accumulate over time in such a way that the probability an event occurs in an interval is proportional to the length of an interval and that the number of events in non-overlapping intervals are independent, the parameter $\lambda$ \marginpar[\raggedright\vspace{-5mm}

% $\lambda$\vspace{0mm}\\\footnotesize Rate for the\\Poisson dist.]{\raggedright\vspace{-5mm}
	
% 	$\lambda$\vspace{0mm}\\\footnotesize Rate for the\\Poisson dist.}\index{Greek!lambda@lambda ($\lambda$)} (the Greek letter \textit{lambda}) represents the average number of events per unit time; i.e., the rate per unit time.

% In this setting, the number of events in $t$ units of time has probability
% \[P(X = k) = \frac{e^{-\lambda t}(\lambda t)^{k}}{k!}, \]
% where $k$ takes on values 0, 1, 2, \dots.  When used this way, the mean and standard deviation  are $\lambda t$ and $\sqrt{\lambda t}$, respectively. The rate parameter $\lambda$ represents the expected number of events per unit time, while the quantity $\lambda t$ represents the expected number events over a time period of $t$ units.

% The histogram in Figure~\ref{amiIncidencesOver100Days} approximates a Poisson distribution with rate equal to 4.4 events per day, for a population of 8 million. 

% \textD{\newpage}

% \begin{examplewrap}
% \begin{nexample}{In New York City, what is the probability that 2 individuals are hospitalized for AMI in seven days, if the rate is known to be 4.4 deaths per day?}
% From the given information, $\lambda = 4.4$, $k = 2$, and $t = 7$. 
% \begin{align*}
% P(X = k) =& \frac{e^{-\lambda t}(\lambda t)^{k}}{k!} \\
% P(X = 2) =& \frac{e^{-4.4 \times 7}(4.4 \times 7)^{2}}{2!} = 1.99 \times 10^{-11}.
% \end{align*}
% \end{nexample}
% \end{examplewrap}

% \begin{exercisewrap}
% \begin{nexercise}
% In New York City, what is the probability that (a) at most 2 individuals are hospitalized for AMI in seven days, (b) at least 3 individuals are hospitalized for AMI in seven days?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{(a) $P(X \leq 2) = P(X=0) + P(X=1) + P(X=2) = \frac{e^{-4.4 \times 7}(4.4 \times 7)^{0}}{0!} + \frac{e^{-4.4 \times 7}(4.4 \times 7)^{1}}{1!} + \frac{e^{-4.4 \times 7}(4.4 \times 7)^{2}}{2!} = 2.12 \times 10^{-11}$ (b) $P(X \geq 3) = 1 - P(X < 3) = 1 - P(X \leq 2) = 1 - 2.12 \times 10^{-11} \approx 1 $}

% A rigorous set of conditions for the Poisson distribution is not discussed here. Generally, the Poisson distribution is used to calculate probabilities for rare events that accumulate over time, such as the occurrence of a disease in a population.

% \begin{examplewrap}
% \begin{nexample}{For children ages 0 - 14, the incidence rate of acute lymphocytic leukemia (ALL) was approximately 30 diagnosed cases per million children per year in 2010. Approximately 20\% of the US population of 319,055,000 are in this age range. What is the expected number of cases of ALL in the US over five years?}

% The incidence rate for one year can be expressed as $30/1,000,000 = 0.00003$; for five years, the rate is $(5)(0.00003) = 0.00015$. The number of children age 0-14 in the population is $(0.20)(319,055,000) \approx 63,811,000$. 
% \begin{align*}
% \lambda &= \text{(relevant population size)(rate per child)} \\
% &= 63,811,000 \times 0.00015 \\
% &= 9,571.5
% \end{align*}
	
% The expected number of cases over five years is 9,571.5 cases.
% \end{nexample}
% \end{examplewrap}

% \index{distribution!Poisson|)}


% %_________________
% \section{Distributions related to Bernoulli trials}
% \label{distRelatedToBernoulli}

% The binomial distribution is not the only distribution that can be built from a series of repeated Bernoulli trials. This section discusses the geometric, negative binomial, and hypergeometric distributions.

% \subsection{Geometric distribution}
% \label{geomDist}

% \index{distribution!geometric|(}

% The geometric distribution describes the waiting time until one success for a series of independent Bernoulli random variables, in which the probability of success $p$ remains constant.

% \begin{examplewrap}
% \begin{nexample}{Recall that in the Milgram shock experiments, the probability of a person refusing to give the most severe shock is $p = 0.35$. Suppose that participants are tested one at a time until one person refuses; i.e., until the first occurrence of a successful trial. What are the chances that the first occurrence happens with the first trial? The second trial? The third?}\label{waitForShocker}%
% The probability that the first trial is successful is simply $p = 0.35$. 

% If the second trial is the first successful one, then the first one must have been unsuccessful. Thus, the probability is given by $(0.65)(0.35) = 0.228$.

% Similarly, the probability that the first success is the third trial: $(0.65)(0.65)(0.35) = 0.148$.

% This can be stated generally. If the first success is on the $n^{th}$ trial, then there are $n-1$ failures and finally 1 success, which corresponds to the probability $(0.65)^{n-1}(0.35)$.
% \end{nexample}
% \end{examplewrap}

% The geometric distribution from Example~\ref{waitForShocker} is shown in Figure~\ref{geometricDist35}. In general, the probabilities for a geometric distribution decrease \term{exponentially}.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.8\textwidth]{ch_distributions_oi_biostat/figures/geometricDist35/geometricDist35}
% \caption{The geometric distribution when the probability of success is \mbox{$p=0.35$}.}
% \label{geometricDist35}
% \end{figure}

% \begin{onebox}{Geometric Distribution}\index{distribution!geometric|textbf}
% If the probability of a success in one trial is $p$ and the probability of a failure is $1-p$, then the probability of finding the first success in the $k^{th}$ trial is given by
% \begin{eqnarray*}
% P(X = k) = (1-p)^{k-1}p.
% \end{eqnarray*}
% The mean (i.e. expected value), variance, and standard deviation of this wait time are given by
% \begin{align*}
% \mu &= \frac{1}{p}
% 	&\sigma^2&=\frac{1-p}{p^2}
% 	&\sigma &= \sqrt{\frac{1-p}{p^2}}
% \label{geomFormulas}
% \end{align*}
% A geometric random variable $X$ can be expressed as $X \sim \textrm{Geom}(p)$.
% \end{onebox}

% \marginpar[\raggedright\vspace{-45mm}

% $\textrm{Geom}(p)$\vspace{1mm}\\\footnotesize Geometric dist.\\with $p$ prob. of success]{\raggedright\vspace{-45mm}
	
% 	$\textrm{Geom}(p)$\vspace{1mm}\\\footnotesize Geometric dist.\\with $p$ prob. of success} 

% \begin{exercisewrap}
% \begin{nexercise}
% If individuals were examined until one did not administer the most severe shock, how many might need to be tested before the first success?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{About $1/p = 1/0.35 = 2.86$ individuals.}

% \begin{examplewrap}
% \begin{nexample}{What is the probability of the first success occurring within the first 4 people?}\label{marglimFirstSuccessIn4}%
% This is the probability it is the first ($k=1$), second ($k=2$), third ($k=3$), or fourth ($k=4$) trial that is the first success, which represent four disjoint outcomes. Compute the probability of each case and add the separate results:
% \begin{eqnarray*}
% &&P(X=1, 2, 3,\text{ or }4) \\
% 	&& \quad = P(X=1)+P(X=2)+P(X=3)+P(X=4) \\
% 	&& \quad = (0.65)^{1-1}(0.35) + (0.65)^{2-1}(0.35) + (0.65)^{3-1}(0.35) + (0.65)^{4-1}(0.35) \\
% 	&& \quad = 0.82.
% \end{eqnarray*}

% Alternatively, find the complement of P(X = 0), since the described event is the complement of no success in 4 trials: $1 - (0.65)^{4}(0.35)^{0} = 0.82$.

% There is a 0.82 probability that the first success occurs within 4 trials.
% \end{nexample}
% \end{examplewrap}

% Note that there are differing conventions for defining the geometric distribution; while this text uses the definition that the distribution describes the total number of trials \textit{including} the success, others define the distribution as the number of trials required before the success is obtained. In \textsf{R}, the latter definition is used. 

% \index{distribution!geometric|)}


% \textD{\newpage}


% \subsection{Negative binomial distribution}
% \label{negativeBinomial}

% \index{distribution!negative binomial|(}

% The geometric distribution describes the probability of observing the first success on the $k^{th}$ trial. The \termsub{negative binomial distribution}{distribution!negative binomial} is more general: it describes the probability of observing the $r^{th}$ success on the $k^{th}$ trial.

% Suppose a research assistant needs to successfully extract RNA from four plant samples before leaving the lab for the day. Yesterday, it took 6 attempts to attain the fourth successful extraction. The last extraction must have been a success; that leaves three successful extractions and two unsuccessful ones that make up the first five attempts. There are ten possible sequences, which are shown in \ref{successFailureOrdersForRNAExtractions}. 

% \begin{figure}[ht]
% 	\newcommand{\succObs}[1]{{\color{oiB}$\stackrel{#1}{S}$}}
% 	\centering
% 	\begin{tabular}{c|c ccc cl | r}
% 		\multicolumn{8}{c}{\hspace{10mm}Extraction Attempt} \\
% 		& & 1 & 2 & 3 & 4 & \multicolumn{2}{l}{5\hfill6} \\
% 		\hline
% 		1&& $F$ & $F$ & \succObs{1} & \succObs{2} & \succObs{3} & \succObs{4} \\
% 		2&& $F$ & \succObs{1} & $F$ & \succObs{2} & \succObs{3} & \succObs{4} \\
% 		3&& $F$ & \succObs{1} & \succObs{2} & $F$ & \succObs{3} & \succObs{4} \\
% 		4&& $F$ & \succObs{1} & \succObs{2} & \succObs{3} & $F$ & \succObs{4} \\
% 		5&& \succObs{1} & $F$ & $F$ & \succObs{2} & \succObs{3} & \succObs{4} \\
% 		6&& \succObs{1} & $F$ & \succObs{2} & $F$ & \succObs{3} & \succObs{4} \\
% 		7&& \succObs{1} & $F$ & \succObs{2} & \succObs{3} & $F$ & \succObs{4} \\
% 		8&& \succObs{1} & \succObs{2} & $F$ & $F$ & \succObs{3} & \succObs{4} \\
% 		9&& \succObs{1} & \succObs{2} & $F$ & \succObs{3} & $F$ & \succObs{4} \\
% 		10&& \succObs{1} & \succObs{2} & \succObs{3} & $F$ & $F$ & \succObs{4} \\
% 	\end{tabular}
% 	\caption{The ten possible sequences when the fourth successful extraction is on the sixth attempt.}
% 	\label{successFailureOrdersForRNAExtractions}
% \end{figure}

% \begin{exercisewrap}
% \begin{nexercise}\label{probOfEachSeqOfSixTriesToGetFourSuccesses}%
% Each sequence in Figure~\ref{successFailureOrdersForRNAExtractions} has exactly two failures and four successes with the last attempt always being a success. If the probability of a success is $p=0.8$, find the probability of the first sequence.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{The first sequence: $0.2\times0.2\times0.8\times0.8\times0.8\times0.8 = 0.0164$.}

% If the probability of a successful extraction is $p=0.8$, what is the probability that it takes exactly six attempts to reach the fourth successful extraction? As expressed by \ref{probOfEachSeqOfSixTriesToGetFourSuccesses}, there are 10 different ways that this event can occur. The probability of the first sequence was identified in Guided Practice~\ref{probOfEachSeqOfSixTriesToGetFourSuccesses} as 0.0164, and each of the other sequences have the same probability. Thus, the total probability is $(10)(0.0164) = 0.164$.

% \textD{\newpage}

% A general formula for computing a negative binomial probability can be generated using similar logic as for binomial probability. The probability is comprised of two pieces: the probability of a single sequence of events, and then the number of possible sequences. The probability of observing $r$ successes out of $k$ attempts can be expressed as $(1-p)^{k-r} p^{r}$. Next, identify the number of possible sequences. In the above example, 10 sequences were identified by fixing the last observation as a success and looking for ways to arrange the other observations. In other words, the goal is to arrange $r-1$ successes in $k-1$ trials. This can be expressed as: \[{k-1 \choose r-1} = \frac{(k-1)!}{(r-1)! \left((k-1) - (r-1)\right)!} = \frac{(k-1)!}{(r-1)! \left(k - r\right)!}.\]

% \begin{onebox}{Negative binomial distribution}
% The negative binomial distribution describes the probability of observing the $r^{th}$ success on the $k^{th}$ trial, for independent trials:
% \begin{eqnarray}
% P(X = k) = {k-1 \choose r-1} p^{r}(1-p)^{k-r},
% \label{negativeBinomialEquation}
% \end{eqnarray}
% where $p$ is the probability an individual trial is a success.

% The mean and variance are given by\vspace{-2.5mm}
% \begin{align*}
% \mu &= \frac{r}{p}
% &\sigma^2&=\frac{r(1-p)}{p^2}
% \end{align*}
% A negative binomial random variable $X$ can be expressed as $X \sim \textrm{NB}(r, p)$.
% \end{onebox}

% \marginpar[\raggedright\vspace{-45mm}

% $\textrm{NB}(r, p)$\vspace{1mm}\\\footnotesize Neg. Bin. dist.\\with $r$ successes\\\& $p$ prob. of success]{\raggedright\vspace{-45mm}
	
% 	$\textrm{NB}(r, p)$\vspace{1mm}\\\footnotesize Neg. Bin. dist.\\with $k$ successes\\\& $p$ prob. of success} 

% \begin{onebox}{Is it negative binomial? Four conditions to check.}
% (1) The trials are independent. \\
% (2) Each trial outcome can be classified as a success or failure. \\
% (3) The probability of a success ($p$) is the same for each trial. \\
% (4) The last trial must be a success.
% \end{onebox}


% \begin{examplewrap}
% \begin{nexample}{Calculate the probability of a fourth successful extraction on the fifth attempt.}
% The probability of a single success is $p=0.8$, the number of successes is $r=4$, and the number of necessary attempts under this scenario is $k=5$.
% \begin{align*}
% {k-1 \choose r-1}p^r(1-p)^{k-r}\ 
% 	=\ \frac{4!}{3!1!} (0.8)^4 (0.2)\ 
% 	=\ 4 \times 0.08192\ 
% 	=\ 0.328.
% \end{align*}
% \end{nexample}
% \end{examplewrap}

% \textD{\newpage}

% \begin{exercisewrap}
% \begin{nexercise}
% Assume that each extraction attempt is independent. What is the probability that the fourth success occurs within 5 attempts?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{If the fourth success ($r=4$) is within five attempts, it either took four or five tries ($k=4$ or $k=5$):
% \begin{align*}
% & P(k=4\text{ OR }k=5) = P(k=4) + P(k=5) \\
% &\quad = {4-1 \choose 4-1} 0.8^4 + {5-1 \choose 4-1} (0.8)^4(1-0.8) = 1\times 0.41 + 4\times 0.082 = 0.41 + 0.33 = 0.74.
% \end{align*}}

% \begin{onebox}{Binomial versus negative binomial}
% The binomial distribution is used when considering the number of successes for a fixed number of trials. For negative binomial problems, there is a fixed number of successes and the goal is to identify the number of trials necessary for a certain number of successes (note that the last observation must be a success).
% \end{onebox}

% \begin{exercisewrap}
% \begin{nexercise}
% On 70\% of days, a hospital admits at least one heart attack patient. On 30\% of the days, no heart attack patients are admitted. Identify each case below as a binomial or negative binomial case, and compute the probability. (a) What is the probability the hospital will admit a heart attack patient on exactly three days this week? (b) What is the probability the second day with a heart attack patient will be the fourth day of the week? (c) What is the probability the fifth day of next month will be the first day with a heart attack patient?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{In each part, $p=0.7$. (a) The number of days is fixed, so this is binomial. The parameters are $k=3$ and $n=7$: 0.097. (b) The last "success" (admitting a patient) is fixed to the last day, so apply the negative binomial distribution. The parameters are $r=2$, $k=4$: 0.132. (c) This problem is negative binomial with $r=1$ and $k=5$: 0.006. Note that the negative binomial case when $r=1$ is the same as using the geometric distribution.}

% In \textsf{R}, the negative binomial distribution is defined as the number of failures that occur before a target number of successes is reached; i.e., $k - r$. In this text, the distribution is defined in terms of the total number of trials required to observe $r$ successes, where the last trial is necessarily a success. 

% \index{distribution!negative binomial|)}


% \textD{\newpage}


% \subsection{Hypergeometric distribution}
% \label{hypergeometric}

% \index{distribution!hypergeometric|(}

% Suppose that a large number of deer live in a forest. Researchers are interested in using the capture-recapture method to estimate total population size. A number of deer are captured in an initial sample and marked, then released; at a later time, another sample of deer are captured, and the number of marked and unmarked deer are recorded.\footnote{It is assumed that enough time has passed so that the marked deer redistribute themselves in the population, and that marked and unmarked deer have equal probability of being captured in the second sample.} An estimate of the total population can be calculated based on the assumption that the proportion of marked deer in the second sample should equal the proportion of marked deer in the entire population. For example, if 50 deer were initially captured and marked, and then 5 out of 40 deer (12.5\%) in a second sample are found to be marked, then the population estimate is 400 deer, since 50 out of 400 is 12.5\%.

% The capture-recapture method sets up an interesting scenario that requires a new probability distribution. Let $N$ represent the total number of deer in the forest, $m$ the number of marked deer captured in the original sample, and $n$ the number of deer in the second sample. What are the probabilities of obtaining $0, 1, ... , m$ marked deer in the second sample, if $N$ and $m$ are known?

% It is helpful to think in terms of a series of Bernoulli trials, where each capture in the second sample represents a trial; consider the trial a success if a marked deer is captured, and a failure if an unmarked deer is captured. If the deer were sampled \textit{with replacement}, such that one deer was sampled, checked if it were marked versus unmarked, then released before another deer was sampled, then the probability of obtaining some number of marked deer in the second sample would be binomially distributed with probability of success $m/N$ (out of $n$ trials). The trials are independent, and the probability of success remains constant across trials.

% However, in capture-recapture, the goal is to collect a representative sample such that the proportion of marked deer in the sample can be used to estimate the total population\textemdash the sampling is done \textit{without replacement}. Once a trial occurs and a deer is sampled, it is not returned to the population before the next trial. The probability of success is not constant from trial to trial; i.e., these trials are dependent. For example, if a marked deer has just been sampled, then the probability of sampling a marked deer in the next trial decreases, since there is one fewer marked deer available. 

% Suppose that out of 9 deer, 4 are marked. What is the probability of observing 1 marked deer in a sample of size 3, if the deer are sampled without replacement? First, consider the total number of ways to draw 3 deer from the population; As shown in Figure~\ref{hGeomSchematic}, samples may consist of 3, 2, 1, or 0 marked deer. There are ${4 \choose 3}$ ways to obtain a sample consisting of 3 marked deer out of the 4 total marked deer. By independence, there are ${4 \choose 2} {5 \choose 1}$ ways to obtain a sample consisting of exactly 2 marked deer and 1 unmarked deer. In total, there are 84 possible combinations; this quantity is equivalent to ${9 \choose 3}$. Only ${4 \choose 1} {5 \choose 2} = 40$ of those combinations represent the desired event of exactly 1 marked deer. Thus, the probability of observing 1 marked deer in a sample of size 3, under sampling without replacement, equals $40/84 = 0.476$.

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=0.880\textwidth]
% 	{ch_distributions_oi_biostat/figures/hGeomSchematic/hGeomSchematic.png}
% 	\caption{Possible samples of marked and unmarked deer in a sample $n = 3$, where $m = 4$ and $N - m = 5$. Striped circles represent marked deer, and empty circles represent unmarked deer.}
% 	\label{hGeomSchematic}
% \end{figure}

% \textD{\newpage}

% \begin{exercisewrap}
% \begin{nexercise}
% Suppose that out of 9 deer, 4 are marked. What is the probability of observing 1 marked deer in a sample of size 3, if the deer are sampled with replacement?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{Let $X$ represent the number of marked deer in the sample of size 3. If the deer are sampled with replacement, $X \sim \textrm{Bin}(3, 4/9)$, and $P(X = 1) = { 3 \choose 1 } (4/9)^1 (5/9)^2 = 0.412$.}

% \begin{onebox}{Hypergeometric distribution}
% The hypergeometric distribution describes the probability of observing $k$ successes in a sample of size $n$, from a population of size $N$, where there are $m$ successes, and individuals are sampled without replacement:
% \begin{align*}
% P(X = k) = \dfrac{{m \choose k} {N - m \choose n-k}}{{N \choose n}}.
% \label{hypergeometricEquation}
% \end{align*}

% Let $p = m/N$, the probability of success. The mean and variance are given by\vspace{-2.5mm}
% \begin{align*}
% \mu &= np
% &\sigma^2&=np(1-p)\frac{N-n}{N-1}
% \end{align*}
% A hypergeometric random variable $X$ can be written as $X \sim \textrm{HGeom}(m, N-m, n)$.
% \end{onebox}

% \begin{onebox}{Is it hypergeometric? Three conditions to check.}
% (1) The trials are dependent. \\
% (2) Each trial outcome can be classified as a success or failure. \\
% (3) The probability of a success is different for each trial.
% \end{onebox}

% \begin{exercisewrap}
% \begin{nexercise}
% A small clinic would like to draw a random sample of 10 individuals from their patient list of 120, of which 30 patients are smokers. (a) What is the probability of 6 individuals in the sample being smokers? (b) What is the probability that at least 2 individuals in the sample smoke?\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{(a) Let $X$ represent the number of smokers in the sample. $P(X = 6) = \dfrac{{30 \choose 6}{90 \choose 4}}{{120 \choose 10}} = 0.013$. (b) $P(X \geq 2) = 1 - P(X \leq 1) = 1 - P(X = 0) - P(X = 1) = 1 - \dfrac{{30 \choose 0}{90 \choose 10}}{{120 \choose 10}} - \dfrac{{30 \choose 1}{90 \choose 9}}{{120 \choose 10}} = 0.768$. }

% \index{distribution!hypergeometric|)}


% %____________
% \section{Distributions for pairs of random variables}
% \label{correlatedRVs}

% \index{joint distribution for random variables|(}
% \index{marginal distribution for random variables|(}
% \index{conditional distribution for random variables|(}

% Example~\ref{sdHealthCostsPartners} calculated the variability in health care costs for an employee and her partner relying on the assumption that the number of health episodes between the two are not related. It could be reasonable to assume that the health status of one person gives no information about the other's health, given that the two are not physically related and were not previously living together. However, associations between random variables can be subtle. For example, couples are often attracted to each other because of common interests or lifestyles, which suggests that health status may actually be related.

% The relationship between a pair of discrete random variables is a feature of the \term{joint distribution} of the pair. In this example the joint distribution of annual costs is a table of all possible combinations of costs for the employee and her partner, using the probabilities and costs from the last 10 years (these costs were previously calculated in Example~\ref{example:healthCareCosts}
% and Guided Practice~\ref{healthCareCostsPartner}). Entries in the table are probabilities of pairs of annual costs. For example, the entry 0.25 in the second row and second column of Figure~\ref{table:healthExpensesJointDistribution} indicates that in approximately 25\% of the last 10 years, the employee paid \$1,008 in costs while her partner paid \$988.
% \begin{figure}[h]
% 	\centering
% 	\begin{tabular}{crr}
% 		\hline
% 		&   \multicolumn{2}{c}{\textbf{Partner costs, $Y$}} \\
% 		\textbf{Employee costs, $X$} & \textbf{\$968} & \textbf{\$988} \\ 
% 		\hline
% 		\textbf{\$968} & 0.18 & 0.12 \\ 
% 		\textbf{\$1,008} & 0.15 & 0.25 \\ 
% 		\textbf{\$1,028}  & 0.04 & 0.16 \\ 
% 		\textbf{\$1,108}  & 0.03 & 0.07 \\ 
% 		\hline
% 	\end{tabular}
% 	\caption{Joint distribution of health care costs.} 
% 	\label{table:healthExpensesJointDistribution}
% \end{figure}

% More generally, the definition of a joint distribution for a pair of random variables $X$ and $Y$ uses the notion of joint probabilities discussed in Section~\ref{marginalAndJointProbabilities}.

% \begin{onebox}{Joint distribution}
% The \term{joint distribution} $p_{X, Y}(x, y)$ for a pair of random variables $(X, Y)$ is the collection of probabilities
% \begin{align*}
% p(x_i,y_j) = P(X=x_i \text{ and } Y = y_j)
% \end{align*}
% for all pairs of values $(x_i,y_j)$ that the random variables $X$ and $Y$ take on.
% \end{onebox}

% \textD{\newpage}

% Joint distributions are often displayed in tabular form as in Figure~\ref{table:healthExpensesJointDistribution}.  If $X$ and $Y$ have $k_1$ and $k_2$ possible values respectively, there will be $(k_1)(k_2)$ possible $(x,y)$ pairs. This is unlike pairs of values $(x,y)$ observed in a dataset, where each observed value of $x$ is usually paired with only one value of $y$. A joint distribution is often best displayed as a table of probabilities, with $(k_1)(k_2)$ entries.  Figure~\ref{table:generalJointDistribution} shows the general form of the table for the joint distribution of two discrete distributions.

% \begin{figure}[h]
% 	\centering
% 	\begin{tabular}{ccccc}
% 		\hline
% 		& \multicolumn{4}{c}{\textbf{Values of $Y$}} \\
% 		\hline
% 		\textbf{Values of $X$} & $y_1$ & $y_2$ & $\cdots$ & $y_{k_2}$ \\
% 		$x_1$ &   $p(x_1, y_1)$ &   $p(x_1, y_2)$  & $\cdots$  & $p(x_1, y_{k_2})$  \\
% 		$x_2$ &  $p(x_2, y_1)$   &  $p(x_2, y_2)$  & $\cdots$ &   $p(x_2, y_{k_2})$ \\
% 		$\vdots$ &   $\cdots$  &  $\cdots$  &  $\cdots$ &   $\cdots$ \\
% 		$x_{k_1}$ & $p(x_{k_1}, y_1)$ &  $p(x_{k_1}, y_2)$  &  $\cdots$ & $p(x_{k_1}, y_{k_2})$\vspace{1.5mm} \\
% 		\hline
% 	\end{tabular}
% 	\caption{Table for a joint distribution. Entries are probabilities for pairs ($x_i, y_j$). These probabilities can be written as $p(x_i, y_j)$ or more specifically, $p_{X, Y}(x_i, y_j)$.}
% 	\label{table:generalJointDistribution}
% \end{figure}

% When two variables $X$ and $Y$ have a joint distribution, the \term{marginal distribution} of $X$ is the collection of probabilities for $X$ when $Y$ is ignored.\footnote{The marginal distribution of $X$ can be written as $p_X(x)$, and a specific value in the marginal distribution written as $p_X(x_i)$.}  If $X$ represents employee costs and $Y$ represents partner costs, the event $(X = \$968)$ consists of the two  disjoint events $(X = \$968, Y = \$968)$ and $(X = \$968, Y = \$988)$, so $P(X = \$968) = 0.18 + 0.12 = 0.30$, the sum of the first row of the table. The row sums are the values of the marginal distribution of $X$, while the column sums are the values of the marginal distributions of $Y$. The marginal distributions of $X$ and $Y$ are shown in Figure~\ref{healthExpensesJointMarginalDistribution}, along with the joint distribution of $X$ and $Y$. The term marginal distribution is apt in this setting---the marginal probabilities appear in the table margins.

% \begin{figure}[h]
% 	\centering
% 	\begin{tabular}{c|rr | c}
% 		&   \multicolumn{2}{c}{\textbf{Partner Costs, $Y$}} & \\
% 		\hline
% 		\textbf{Employee costs, $X$} & \textbf{\$968} & \textbf{\$988} &  \textbf{Marg. Dist., $X$} \\ 
% 		\hline
% 		\textbf{\$968} & 0.18 & 0.12 & 0.30\\ 
% 		\textbf{\$1,008} & 0.15 & 0.25 & 0.40 \\ 
% 		\textbf{\$1,028}  & 0.04 & 0.16 & 0.20\\ 
% 		\textbf{\$1,108}  & 0.03 & 0.07  & 0.10\\ 
% 		\hline
% 		\textbf{Marg. Dist., $Y$} & 0.40 & 0.60 & 1.00 \\
% 	\end{tabular}
% 	\caption{Joint and marginal distributions of health care costs} 
% 	\label{healthExpensesJointMarginalDistribution}
% \end{figure}

% For a pair of random variables $X$ and $Y$, the \term{conditional distribution} of $Y$ given a value $x$ of the variable $X$ is the probability distribution of $Y$ when its values are restricted to the value $x$ for $X$. Just as marginal and joint probabilities are used to calculate conditional probabilities, joint and marginal distributions can be used to obtain conditional distributions. If information is observed about the value of one of the correlated random variables, such as $X$, then this information can be used to obtain an updated distribution for $Y$; unlike the marginal distribution of $Y$, the conditional distribution of $Y$ given $X$ accounts for information from $X$.

% \textD{\newpage}

% \begin{onebox}{Conditional distribution}
% The \term{conditional distribution} $p_{Y|X}(y|x)$ for a pair of random variables $(X, Y)$ is the collection of probabilities
% \begin{align*}
% P(Y = y_j| X = x_i) = \dfrac{P(Y = y_j \text{ and } X = x_i)}{P(X = x_j)}
% \end{align*}
% for all pairs of values $(x_i,y_j)$ that the random variables $X$ and $Y$ take on.
% \end{onebox}

% \begin{examplewrap}
% \begin{nexample}{If it is known that the employee's annual health care cost is \$968, what is the conditional distribution of the partner's annual health care cost?}\label{conditionalHealthCareDistribution}%
% Note that there is a different conditional distribution of $Y$ for every possible value of $X$; this problem specifically asks for the conditional distribution of $Y$ given that $X = \$968$.
	
% 	\[p_{Y|X}(\$968|\$968) = P(Y = \$968 | X = \$968) = \dfrac{P(Y = \$968 \textrm{ and } X = \$968)}{P(X = \$968)} = \dfrac{0.18}{0.30} = 0.60\]
	
% 	\[p_{Y|X}(\$988|\$968) = P(Y = \$988 | X = \$968) = \dfrac{P(Y = \$988 \textrm{ and } X = \$968)}{P(X = \$968)} = \dfrac{0.12}{0.30} = 0.40\]
	
%   With the knowledge that the employee's annual health care cost is \$968, there is a probability of 0.60 that the partner's cost is \$968 and 0.40 that the partner's cost is \$988.
% \end{nexample}
% \end{examplewrap}

% \begin{exercisewrap}
% \begin{nexercise}
% Consider two random variables, $X$ and $Y$, with the joint distribution shown in Figure~\ref{table:jointDistGuidedPractice}. \\
% (a)~Compute the marginal distributions of $X$ and $Y$. \\
% (b)~Identify the joint probability $p_{X,Y}(1, 2)$. \\
% (c)~What is the value of $p_{X, Y}(2, 1)$? \\
% (d)~Compute the conditional distribution of $X$ given that $Y = 2$.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{(a)~ The marginal distribution of $X$: $p_X(1) = 0.60$, $p_X(4) = 0.40$. The marginal distribution of $Y$: $p_Y(1) = 0.50$, $p_Y(2) = 0.50$ (b)~$p_{X,Y}(1, 2) = P(X = 1, Y = 2) = 0.40$ (c)~Since $X$ cannot take on value 2, $p_{X, Y}(2, 1) = 0$. (d)~ The conditional distribution of $X$ given that $Y = 2$: $p_{X|Y}(1|2) = \frac{p_{X, Y}(1, 2)}{p_Y(2)}  = \frac{0.40}{0.50} = 0.80$, $p_{X|Y}(4|2) = \frac{p_{X, Y}(4, 2)}{p_Y(2)}  = \frac{0.10}{0.50} = 0.20$.}

% \begin{figure}[h]
% 	\centering
% 	\small
% 	\begin{tabular}{r|rr}
% 		\hline
% 		& $Y = 1$ & $Y = 2$ \\ 
% 		\hline
% 		$X = 1$  & 0.20 & 0.40 \\ 
% 		$X = 4$ & 0.30 & 0.10 \\ 
% 		\hline
% 	\end{tabular}
% 	\caption{Joint distribution of $X$ and $Y$} 
% 	\label{table:jointDistGuidedPractice}
% \end{figure}

% The variance calculation in Example~\ref{sdHealthCostsPartners} relied on the assumption that the patterns of health care expenses for the two partners were unrelated. In Example~\ref{conditionalHealthCareDistribution}, 0.40 is the conditional probability that the partner's health care costs will be \$988, given that the employee's cost is \$968.  The marginal probability that the partner's health care cost is \$988 is 0.60, which is different from 0.40.  The patterns of health care costs are related in that knowing the value of the employee's costs changes the probabilities associated with partner's costs.  The marginal and conditional distributions of the partner's costs are not the same.

% The notion of independence of two events discussed in Chapter~\ref{probability} can be applied to the setting of random variables. Recall that two events $A$ and $B$ are independent if the conditional probability $P(A|B)$ equals the marginal probability $P(A)$ or equivalently, if the product of the marginal probabilities $P(A)$ and $P(B)$ equals the joint probability $P(A \text{ and }B)$.

% A pair $(X,Y)$ of random variables are called \term{independent random variables} if the conditional distribution for $Y$, given any value of $X$, is the same as the marginal distribution of~$Y$. Additionally, if all joint probabilities $P(X = x_i, Y = y_j)$ that comprise the joint distribution of $X$ and $Y$ can be computed from the product of the marginal probabilities, $P(X = x_i)P(Y = y_j)$, $X$ and $Y$ are independent.

% \begin{onebox}{Independent Random Variables}
% Two random variables $X$ and $Y$ are independent if the probabilities
% \begin{align*}
% P(Y = y_j| X = x_i) = P(Y = y_j)
% \end{align*}
% for all pairs of values $(x_i,y_j)$. \\
% Equivalently, $X$ and $Y$ are independent if the probabilities
% \begin{align*}
% P(Y = y_j \text{ and } X = x_i) = P(Y = y_j)P(X = x_i)
% \end{align*}
% for all pairs of values $(x_i,y_j)$.
% \end{onebox}

% \begin{examplewrap}
% \begin{nexample}{Demonstrate that the employee's health care costs and the partner's health care costs are not independent random variables.}
	
% 	As shown in Example~\ref{conditionalHealthCareDistribution}, the conditional distribution of the partner's annual health care cost given that the employee's annual cost is \$968 is $P(Y = \$968 | X = \$ 968) = 0.60$, $P(Y = \$988 | X = \$ 968) = 0.40$. However, the marginal distribution of the partner's annual health care cost is $P(Y = \$968) = 0.40$, $P(Y = \$968) = 0.60$. Thus, $X$ and $Y$ are not independent.
	
% 	This can also be demonstrated from examining the joint distribution, as shown in Figure~\ref{healthExpensesJointMarginalDistribution}. The probability that the employee's cost and partner's cost are both \$968 is 0.18. The marginal probabilities $P(X = \$968)$ and $P(Y = \$968)$, respectively, are 0.30 and 0.40. Since $(0.40)(0.30) \neq 0.18$, $X$ and $Y$ are dependent random variables.
	
% 	Note that demonstrating $P(Y = y_j| X = x_i) = P(Y = y_j)$ or $P(Y = y_j \text{ and } X = x_i) = P(Y = y_j)P(X = x_i)$ does not hold for any one $(x_i, y_j)$ pair is sufficient to prove that $X$ and $Y$ are not independent, since independence requires these conditions to hold over \textit{all} pairs of values $(x_i, y_j)$.
% \end{nexample}
% \end{examplewrap}

% \begin{exercisewrap}
% \begin{nexercise}
% Based on Figure~\ref{table:jointDistGuidedPractice}, check whether $X$ and $Y$ are independent.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{$X$ and $Y$ are not independent. One way to demonstrate this is to compare $p_X(1)$ with $p_{X|Y}(1|2)$. If $X$ were independent of $Y$, then conditioning on $Y = 2$ should not provide any information about $X$, and $p_X(1)$ should equal $p_{X|Y}(1|2)$. However, $p_X(1) = 0.60$ and $p_{X|Y}(1|2) = 0.80$. Thus, $X$ and $Y$ are not independent.}

% Two random variables that are not independent are called \term{correlated random variables}.  The correlation between two random variables is a measure of the strength of the relationship between them, just as it was for pairs of data points explored in Section~\ref{subsectionCorrelation}.  There are many examples of correlated random variables, such as height and weight in a population of individuals, or the gestational age and birth weight of newborns.  

% When two random variables are positively correlated, they tend to increase or decrease together. If one of the variables increases while the other decreases (or vice versa) they are negatively correlated.  Correlation is easy to identify in a scatterplot, but is more difficult to identify in a table of a joint distribution.  Fortunately, there is a formula to calculate correlation for a joint distribution specified  in a table.

%  Correlation between random variables is similar to correlation between pairs of observations in a dataset, with some important differences.  Calculating a correlation $r$ in a dataset was introduced in Section~\ref{scatterPlots} and uses the formula:
% \begin{align}
% r &=  \frac{1}{n-1}\sum^{n}_{i=1}
% \left(\frac{x_{i}-\overline{x}}
% {s_{x}}\right)\left(\frac{y_{i}-\overline{y}}{s_{y}}\right).
% \end{align} 
% The correlation coefficient $r$ is an average of products, with each term in the product measuring the distance between $x$ and its mean $\overline{x}$ and the distance between $y$ and its mean $\overline{y}$, after the distances have been scaled by respective standard deviations.

% The compact formula for the correlation between two random variables $X$ and $Y$  uses the same idea:
% \begin{equation}
% \rho_{X,Y} = E\left(\frac{X - \mu_x}{\sigma_X}\right)\left(\frac{Y - \mu_Y}{\sigma_Y} \right),
% \label{eq:generalFormulaCorrelation}
% \end{equation}
% where $\rho_{X,Y}$ is the correlation between the two variables, and $\mu_X, \mu_Y$, $\sigma_X, \sigma_Y$ are the respective means and standard deviations for $X$ and $Y$. Just as with the mean of a random variable, the expectation in the formula for correlation is a weighted sum of products, with each term weighted by the probability of values for the pair $(X,Y)$.  Equation~\ref{eq:generalFormulaCorrelation} is useful for understanding the analogy between correlation of random variables and correlation of observations in a dataset, but it cannot be used to calculate $\rho_{X,Y}$ without the probability weights.  The weights come from the \term{joint distribution} of the pair of variables~$(X,Y)$. 

% Equation~\ref{eq:specificFormulaCorrelation} is an expansion of Equation~\ref{eq:generalFormulaCorrelation}. The double summation adds up terms over all combinations of the indices $i$ and $j$.

% \begin{equation}
% \rho_{X,Y} = \sum_{i} \sum_{j} p(i,j)\frac{(x_i - \mu_X)}{\textrm{sd}(X)}\frac{(y_j - \mu_Y)}{\textrm{sd}(Y)}.
% \label{eq:specificFormulaCorrelation}
% \end{equation}


% \begin{examplewrap}
% \begin{nexample}{Compute the correlation between annual health care costs for the employee and her partner.} 
% As calculated previously, $E(X) = \$1010$, $\textrm{Var}(X) = 1556$, $E(Y) = \$980$, and $\textrm{Var}(Y)= 96$. Thus, $SD(X) = \$39.45$ and $SD(Y) = \$9.80$.
% 	\begin{align*}
% 	\rho_{X,Y} &= p(x_1,y_1) \frac{(x_1 - \mu_X)}{\textrm{sd}(X)}\frac{(y_1 - \mu_Y)}{\textrm{sd}(Y)} 
% 	+ p(x_1,y_2)\frac{(x_1 - \mu_X)}{\textrm{sd}(X)}\frac{(y_2 - \mu_Y)}{\textrm{sd}(Y)}  \\
% 	&\phantom{{}=1} + \cdots + p(x_4,y_1)\frac{(x_4 - \mu_X)}{\textrm{sd}(X)}\frac{(y_1 - \mu_Y)}{\textrm{sd}(Y)} + p(x_4,y_2)\frac{(x_4 - \mu_X)}{\textrm{sd}(X)}\frac{(y_2 - \mu_Y)}{\textrm{sd}(Y)} \\
% 	&= (0.18) \frac{(968 - 1010)}{39.45}\frac{(968 - 980)}{9.8} 
% 	+ (0.12)\frac{(968 - 1010)}{39.45}\frac{(988 - 980)}{9.8}  \\
% 	&\phantom{{}=1} + \cdots + (0.03)\frac{(1108 - 1010)}{39.45}\frac{(968 - 980)}{9.8} + (0.07)\frac{(1108 - 1010)}{39.45}\frac{(988 - 980)}{9.8} \\
% 	&= 0.22.
% 	\end{align*} 
% 	The correlation between annual health care costs for these two individuals is positive. It is reasonable to expect that there might be a positive correlation in health care costs for two individuals in a relationship; for example, if one person contracts the flu, then it is likely the other person will also contract the flu, and both may need to see a doctor.
% \end{nexample}
% \end{examplewrap}

% \begin{exercisewrap}
% \begin{nexercise}
% Based on Figure~\ref{table:jointDistGuidedPractice}, compute the correlation between $X$ and $Y$. For your convenience, the following values are provided: $E(X) = 2.2$, $\text{Var}(X) = 2.16$, $E(Y) = 1.5$, $\text{Var}(Y) = 0.25$.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{The correlation between $X$ and $Y$ is $\rho_{X, Y} = (0.20)\frac{(1 - 2.2)}{\sqrt{2.16}}\frac{(1 - 1.5)}{\sqrt{0.25}} + \dots + (0.10)\frac{(4 - 2.2)}{\sqrt{2.16}}\frac{(2 - 1.5)}{\sqrt{0.25}} = -0.0208.$}

% When two random variables $X$ and $Y$ are correlated:
% \begin{equation} 
% \text{Variance}(X + Y) = \text{Variance}(X) + \text{Variance}(Y) + 
% 2 \sigma_X \sigma_Y\text{Correlation}(X,Y) 
% \label{eq:generalVarianceSumRVs}
% \end{equation}
% \begin{equation}
% \text{Variance}(X - Y) = \text{Variance}(X) + \text{Variance}(Y) - 
% 2 \sigma_X \sigma_Y\text{Correlation}(X,Y). 
% \label{eq:generalVarianceDiffRVs}
% \end{equation}

% When random variables are positively correlated the variance of the sum or the difference of two variables will be larger than the sum of the two variances. When they are negatively correlated the variance of the sum or difference  will be smaller than the sum of the two variances. 

% The standard deviation for the sum or difference will always be the square root of the variance.

% \begin{examplewrap}
% \begin{nexample}{Calculate the standard deviation of the sum of the health care costs for the couple.}
% 	This calculation uses Equation~\ref{eq:generalVarianceSumRVs} to calculate the variance of the sum.  The standard deviation will be the square root of the variance.
	
% 	\begin{align*} 
% 	\text{Var}(X + Y) &= \text{Var}(X) + \text{Var}(Y) + 
% 	2 \sigma_X \sigma_Y \rho_{X,Y} \\
% 	&= (1556 + 96) + (2)(39.45)(9.80)(0.22) \\
% 	& = 1822.10.
% 	\end{align*}
	
%     The standard deviation is $\sqrt{1822.10} = \$42.69$.  Because the health care costs are correlated, the standard deviation of the total cost is larger than the value calculated in Example~\ref{sdHealthCostsPartners} under the assumption that the annual costs were independent.
% \end{nexample}
% \end{examplewrap}

% \begin{exercisewrap}
% \begin{nexercise}
% Compute the standard deviation of $X - Y$ for the pair of random variables shown in Figure~\ref{table:jointDistGuidedPractice}.\footnotemark{}
% \end{nexercise}
% \end{exercisewrap}
% \footnotetext{$\text{Var}(X - Y) = \text{Var}(X) + \text{Var}(Y) - 2\sigma_X \sigma_Y \rho_{X, Y} = 2.16 + 0.25 - 2(\sqrt{2.16})(\sqrt{0.25})(-0.0208) = 2.44$. Thus, $SD(X - Y) = \sqrt{2.44} = 1.56.$}

% \textD{\newpage}

% \begin{examplewrap}
% \begin{nexample}{The Association of American Medical Colleges (AAMC) introduced a new version of the Medical College Admission Test (MCAT) in the spring of 2015. Data from the scores were recently released by AAMC.\footnotemark{} The test consists of 4 components: chemical and physical foundations of biological systems; critical analysis and reasoning skills; biological and biochemical foundations of living systems; psychological, social and biological foundations of behavior. The overall score is the sum of the individual component scores. The grading for each of the four components is scaled so that the mean score is 125.  The means and standard deviations for the four components and the total scores for the population taking the exam in May 2015 exam are shown in Figure~\ref{table:mcatScoreDistribution}.

% Show that the standard deviation in the table for the total score does not agree with that obtained under the assumption of independence.}
% 	The variance of each component of the score is the square of each standard deviation.  Under the assumption of independence, the variance of the total score would be 
% 	\begin{align*}
% 	\textrm{Var(Total Score)} &= 3.0^2 + 3.0^2 + 3.0^2 + 3.1^2 \\
% 	&= 36.61,
% 	\end{align*}
% 	so the standard deviation is 6.05, which is less than 10.6. 
	
% 	Since the observed standard deviation is larger than that calculated under independence, this suggests the component scores are positively correlated.
	
% 	It would not be reasonable to expect that the component scores are independent. Think about a student taking the MCAT exam: someone who scores well on one component of the exam is likely to score well on the other parts.
% \end{nexample}
% \end{examplewrap}
% \footnotetext{\url{https://www.aamc.org/students/download/434504/data/percentilenewmcat.pdf}}

% \begin{figure}[h]
% 	\centering
% 	\begin{tabular}{lrr}
% 		\hline
% 		\textbf{Component} & \textbf{Mean} & \textbf{Standard Deviation}\\
% 		\hline
% 		Chem. Phys. Found. &   125 &   3.0\\
% 		Crit. Analysis &   125 &    3.0 \\
% 		Living  Systems &  125 &     3.0\\
% 		Found. Behavior&   125 &     3.1\\
% 		\textbf{Total Score} &    500 &     10.6\\
% 		\hline
% 	\end{tabular}
% 	\caption{Means and Standard Deviations for MCAT Scores}
% 	\label{table:mcatScoreDistribution}
% \end{figure}	

% \index{joint distribution for random variables|)}
% \index{marginal distribution for random variables|)}
% \index{conditional distribution for random variables|)}


% %___________
% \section{Notes}
% \label{sectionDistOfRVNotes}

% Thinking in terms of random variables and distributions of probabilities makes it easier to describe all possible outcomes of an experiment or process of interest, versus only considering probabilities on the scale of individual outcomes or sets of outcomes. Several of the fundamental concepts of probability can naturally be extended to probability distributions. For example, the process of obtaining a conditional distribution is analogous to the one for calculating a conditional probability.

% Many processes can be modeled using a specific named distribution. The statistical techniques discussed in later chapters, such as hypothesis testing and regression, are often based on particular distributional assumptions. In particular, many methods rely on the assumption that data are normally distributed. 

% The discussion of random variables and their distribution provided in this chapter only represents an introduction to the topic. In this text, properties of random variables such as expected value or correlation are presented in the context of discrete random variables; these concepts are also applicable to continuous random variables. A course in probability theory will cover additional named distributions as well as more advanced methods for working with distributions.

% Lab 1 introduces the general notion of a random variable and its distribution using a simulation, then discusses the binomial distribution.  Lab 2 discusses the normal distribution and working with normal probabilities, as well as the Poisson distribution. Lab 3 covers the geometric, negative binomial, and hypergeometric distributions.  All three labs include practice problems that illustrate the use of \textsf{R} functions for probability distributions and introduce additional features of the \textsf{R} programming language. Lab 4 discusses distributions for pairs of random variables and some \textsf{R} functions useful for matrix calculations.


